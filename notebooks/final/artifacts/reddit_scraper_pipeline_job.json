{
  "pipelineSpec": {
    "components": {
      "comp-custom-batch-prediction": {
        "executorLabel": "exec-custom-batch-prediction",
        "inputDefinitions": {
          "parameters": {
            "gcs_input_file": {
              "type": "STRING"
            },
            "gcs_output_dir": {
              "type": "STRING"
            },
            "job_display_name": {
              "type": "STRING"
            },
            "location": {
              "type": "STRING"
            },
            "model_resource_name": {
              "type": "STRING"
            },
            "project": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "parameters": {
            "Output": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-firestore": {
        "executorLabel": "exec-firestore",
        "inputDefinitions": {
          "parameters": {
            "batch_prediction_uri": {
              "type": "STRING"
            },
            "collection_name": {
              "type": "STRING"
            },
            "csv_input_file": {
              "type": "STRING"
            },
            "gcs_bucket_name": {
              "type": "STRING"
            },
            "gcs_prefix_name": {
              "type": "STRING"
            },
            "percentage": {
              "type": "DOUBLE"
            },
            "project_id": {
              "type": "STRING"
            },
            "subreddit_name": {
              "type": "STRING"
            },
            "threshold": {
              "type": "DOUBLE"
            }
          }
        },
        "outputDefinitions": {
          "parameters": {
            "stored": {
              "type": "INT"
            },
            "unusable": {
              "type": "INT"
            },
            "usable": {
              "type": "INT"
            }
          }
        }
      },
      "comp-reddit": {
        "executorLabel": "exec-reddit",
        "inputDefinitions": {
          "parameters": {
            "gcs_bucket_name": {
              "type": "STRING"
            },
            "gcs_prefix_name": {
              "type": "STRING"
            },
            "limit": {
              "type": "INT"
            },
            "project_id": {
              "type": "STRING"
            },
            "secret_name": {
              "type": "STRING"
            },
            "subreddit_name": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "parameters": {
            "Output": {
              "type": "STRING"
            }
          }
        }
      },
      "comp-storage": {
        "executorLabel": "exec-storage",
        "inputDefinitions": {
          "parameters": {
            "collection_name": {
              "type": "STRING"
            },
            "csv_input_file": {
              "type": "STRING"
            },
            "gcs_bucket_name": {
              "type": "STRING"
            },
            "gcs_prefix_name": {
              "type": "STRING"
            },
            "location_name": {
              "type": "STRING"
            },
            "project_id": {
              "type": "STRING"
            }
          }
        },
        "outputDefinitions": {
          "parameters": {
            "batch_predict_file_uri": {
              "type": "STRING"
            },
            "posts_csv_file": {
              "type": "STRING"
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-custom-batch-prediction": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "custom_batch_prediction"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'google-cloud-aiplatform' 'kfp==1.8.12' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef custom_batch_prediction(\n    project: str,\n    location: str,\n    model_resource_name: str,\n    job_display_name: str,\n    gcs_input_file: str,\n    gcs_output_dir: str\n) -> str:\n    from google.cloud import aiplatform\n\n    aiplatform.init(project=project, location=location)\n\n    model=aiplatform.Model(model_resource_name)\n\n    batch_prediction_job = model.batch_predict(\n        job_display_name=job_display_name,\n        gcs_source=gcs_input_file,\n        gcs_destination_prefix=gcs_output_dir,\n        sync=True,\n    )\n\n    batch_prediction_job.wait()\n\n    return batch_prediction_job.output_info.gcs_output_directory\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-firestore": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "firestore"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'Pillow' 'google-cloud-firestore' 'google-cloud-storage' 'numpy' 'pandas' 'jsonlines' 'kfp==1.8.12' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef firestore(\n    subreddit_name: str,\n    collection_name: str,\n    gcs_bucket_name: str,\n    gcs_prefix_name: str,\n    csv_input_file: str,\n    batch_prediction_uri: str,\n    project_id: str,\n    threshold: float,\n    percentage: float\n) -> NamedTuple(\n    \"outputs\",\n    [\n        (\"usable\", int),\n        (\"unusable\", int),\n        (\"stored\", int),\n    ]\n):\n\n    from datetime import datetime\n    import hashlib\n    from io import BytesIO\n    import json\n    import jsonlines\n    import math\n    import pandas as pd\n    from PIL import Image\n    import re\n    import requests\n    import shutil\n\n    from google.cloud import firestore\n    from google.cloud import storage\n\n    def process_batch_predict_output(bp_jsonl, threshold, percentage):\n        \"\"\"Parses a set of batch predictions for highest confidence inputs.\n\n        Arguments:\n            bp_jsonl (str): the batch prediction results, as a string\n            threshold (float): the lowest confidence value to accept from 0.0 to 1.0\n            percentage (float): the top percentage (by quality) of predictions to check\n\n        Returns:\n            list of GCS URIs\n        \"\"\"\n        predictions = bp_jsonl.decode(\"utf-8\").split(\"\\n\")\n        reader = jsonlines.Reader(predictions)\n\n        images_uris = {\n            \"usable\": [],\n            \"unusable\": [],\n        }\n\n        for obj in reader.iter(type=dict, skip_invalid=True):\n            confidences = obj[\"prediction\"][\"confidences\"]\n            image_gcs_uri = obj[\"instance\"][\"content\"]\n            # NOTE: we check the predictions above the percentage of\n            # to ensure they are above the acceptable threshold. If\n            # the image is above the acceptable threshold, we keep\n            # the image for training data.\n            top_n_images = int(len(confidences) * percentage)\n            marginal_result = confidences[top_n_images]\n            if marginal_result > threshold:\n                images_uris[\"usable\"].append(image_gcs_uri)\n            else:\n                images_uris[\"unusable\"].append(image_gcs_uri)\n\n        return images_uris\n\n    def get_image_width_and_height(img_bytes):\n        \"\"\"Open the image and get the image's height and width in pixels.\n\n        Arguments:\n            img_bytes (str):\n\n        Returns:\n            Tuple of width, height\n        \"\"\"\n        w = h = 0\n\n        f = BytesIO(img_bytes)\n        with Image.open(f) as img:\n            w, h = img.size\n\n        return (int(w), int(h))\n\n    def compute_vtt_data(width, height, columns, rows):\n        \"\"\"Calculate the VTT values for the image.\n\n        Arguments:\n            width (int):\n            height (int):\n            columns (int):\n            rows (int): \n        Returns:\n            Dict.\n        \"\"\"\n\n        return {\n            \"cellsOffsetX\": 0, # Assumes no offset\n            \"cellsOffsetY\": 0, # Assumes no offset\n            \"imageWidth\": int(width),\n            \"imageHeight\": int(height),\n            \"cellWidth\": int(width / columns),\n            \"cellHeight\": int(height / rows)\n        }\n\n    def convert_image_to_hash(content):\n        \"\"\"Convert image data to hash value (str).\n\n        Arguments:\n            content (byte array): the image\n\n        Return:\n            The image hash value as a string.\n        \"\"\"\n        sha1 = hashlib.sha1()\n        jpg_hash = sha1.update(content)\n        jpg_hash = sha1.hexdigest()\n\n        return jpg_hash    \n\n    def compute_bboxes(*, width=0, height=0, columns=0, rows=0, cell_width=0, cell_height=0):\n        \"\"\"Determines bounding boxes for image object detection.\n\n        Arguments:\n            width (int): width of the image\n            height (int): height of the image\n            columns (int): number of columns in the grid\n            rows (int): number of rows in the grid\n            cell_width (int):\n            cell_height (int):\n\n        Returns:\n            List of dict.\n        \"\"\"\n        bboxes = []\n        BORDER = 1 # 1px border around the outside of the cell\n        LABEL = \"cell\"\n\n        curr_x = cell_width\n        while curr_x < width:\n            curr_y = cell_height\n            while curr_y < height:\n                x_min = (curr_x - BORDER) / width\n                y_min = (curr_y - BORDER) / height\n                x_max = (curr_x + cell_width + BORDER) / width\n                y_max = (curr_y + cell_height + BORDER) / height\n                bboxes.append({\n                    \"xMin\": x_min,\n                    \"xMax\": x_max,\n                    \"yMin\": y_min,\n                    \"yMax\": y_max,\n                    \"displayName\": LABEL\n                })\n                curr_y = curr_y + cell_height\n            curr_x = curr_x + cell_width\n\n        return bboxes\n\n    def store_metadata_fs(*, project_id, series, collection_name, uid):\n        \"\"\"Upserts image metadata into a Firestore collection.\n\n        Arguments:\n            project_id (str): the Google Cloud project to store these in\n            series (pd.Series): a Pandas series with the image's metadata\n            collection_name (str): the Firestore collection to store the data in\n        \"\"\"\n        client = firestore.Client(project=project_id)\n\n        series_dict = series.to_dict()\n\n        # clean up the data a little bit before upserting\n        vtt = series[\"VTT\"]\n        if vtt is not \"\":\n            vtt = json.loads(vtt)\n            series_dict[\"VTT\"] = vtt\n\n        bboxes = series[\"BBoxes\"]\n        if bboxes is not \"\":\n            bboxes = json.loads(bboxes)[\"bboxes\"]\n            series_dict[\"BBoxes\"] = bboxes\n\n        # upsert the dict directly into Firestore!\n        client.collection(collection_name).document(uid).set(series_dict)\n\n    # BEGIN MAIN\n    storage_client = storage.Client(project=project_id)\n    bucket = storage_client.bucket(gcs_bucket_name)\n\n    firestore_client = firestore.Client(project=project_id)\n    collection_ref = firestore_client.collection(collection_name)\n\n    # Determine how many of the scraped data is usable for training a model\n    bp_path = batch_prediction_uri.replace(f\"gs://{gcs_bucket_name}/\", \"\")\n\n    blobs = bucket.list_blobs(prefix=bp_path)\n    for b in blobs:\n        if b.name.find(\".jsonl\") > -1:\n            bp_jsonl_blob = b\n            break\n\n    bp_jsonl = bp_jsonl_blob.download_as_string()\n\n    image_uris = process_batch_predict_output(bp_jsonl, threshold, percentage)\n\n    # Open up the complete list of scraped images as a DataFrame\n    blob = bucket.blob(csv_input_file)\n    csv_bytes = blob.download_as_string()\n    csv_buffer = BytesIO(csv_bytes)\n\n    jpg_df = pd.read_csv(csv_buffer)\n    shards_df = pd.DataFrame()\n\n    # Iterate over JPG URIs, download them in batches, convert to sha values\n    for i, row in jpg_df.iterrows():\n        jpg_uri = row[\"URI\"]\n        filename = row[\"Filename\"]\n\n        if jpg_uri in image_uris[\"unusable\"]:\n            continue\n\n        jpg_uri = jpg_uri.replace(f\"gs://{gcs_bucket_name}/\", \"\")\n        jpg_blob = bucket.blob(jpg_uri)\n        jpg_bytes = jpg_blob.download_as_bytes()\n\n        w, h = get_image_width_and_height(jpg_bytes)\n\n        jpg_df.at[i, \"Width\"] = w\n        jpg_df.at[i, \"Height\"] = h\n\n        # Get columns & rows for original image, based upon the name.\n        paths = filename.split(\".\")\n        dims = paths[-2]\n        cols, rows = dims.split(\"x\")\n\n        cols = int(cols)\n        rows = int(rows)\n\n        jpg_df.at[i, \"Columns\"] = cols\n        jpg_df.at[i, \"Rows\"] = rows\n\n        # Compute the vtt data for the image\n        vtt = compute_vtt_data(width=w, height=h, columns=cols, rows=rows)\n\n        # Note: pandas has issues storing a dict in a cell\n        jpg_df.at[i, \"VTT\"] = json.dumps(vtt)  \n        bboxes = compute_bboxes(width=w,\n                                height=h,\n                                columns=cols,\n                                rows=rows,\n                                cell_width=vtt[\"cellWidth\"],\n                                cell_height=vtt[\"cellHeight\"])\n        if len(bboxes) is 0:\n            print(f\"Error: {filename}\") \n\n        jpg_df.at[i, \"BBoxes\"] = json.dumps({ \"bboxes\": bboxes })\n\n        if (cols * rows) > 500:\n            jpg_df.at[i, \"NeedsSharding\"] = True\n        else:\n            jpg_df.at[i, \"NeedsSharding\"] = False\n\n    complete_df = pd.concat([jpg_df, shards_df])\n    complete_df.set_index(\"UID\", inplace=True)\n    #complete_df.fillna(\"\", inplace=True)\n    complete_df.head(10)\n\n    for uid, row in complete_df.iterrows():\n        store_metadata_fs(project_id=project_id, series=row,\n                          collection_name=collection_name, uid=uid)\n\n    return (len(image_uris[\"usable\"]), len(image_uris[\"unusable\"]), len(complete_df.index))\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-reddit": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "reddit"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'praw' 'google-cloud-secret-manager' 'google-cloud-storage' 'numpy' 'pandas' 'spacy' 'kfp==1.8.12' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef reddit(\n    secret_name: str,\n    subreddit_name: str,\n    gcs_bucket_name: str,\n    gcs_prefix_name: str,\n    project_id: str,\n    limit: int,\n) -> str:\n    from datetime import datetime\n    import numpy as np\n    import pandas as pd\n    import praw\n    import re\n\n    from google.cloud import storage\n\n    def get_reddit_credentials(project_id):\n        \"\"\"Gets the Reddit API key out of Secrets Manager\n\n        Arguments:\n            project_id (str): the current project ID\n\n        Returns:\n            JSON object (dict)\n        \"\"\"\n        from google.cloud import secretmanager\n        import json\n\n        client = secretmanager.SecretManagerServiceClient()\n\n        secret_resource_name = f\"projects/{project_id}/secrets/{secret_name}/versions/1\"\n        response = client.access_secret_version(request={\"name\": secret_resource_name})\n        payload = response.payload.data.decode(\"UTF-8\")\n\n        return json.loads(payload)\n\n    def get_reddit_posts(reddit_credentials, subreddit_name, limit):\n        \"\"\"Gets posts from a subreddit.\n\n        Arguments:\n            reddit_credentials (dict): a dictionary with client_id, secret, and user_agent\n            subreddit_name (str): the name of the subreddit to scrape posts from\n            limit (int): the maximum number of posts to grab\n\n        Returns:\n            List of Reddit API objects\n        \"\"\"\n        import praw\n\n        reddit = praw.Reddit(client_id=reddit_credentials[\"client_id\"], \n                     client_secret=reddit_credentials[\"secret\"],\n                     user_agent=reddit_credentials[\"user_agent\"])\n\n        return reddit.subreddit(subreddit_name).hot(limit=limit)\n\n    def convert_posts_to_dataframe(posts, columns):\n        \"\"\"Converts a sequence of Reddit API post objects into a pandas.DataFrame.\n\n        Arguments:\n            posts (list(praw.Post)): the posts from Reddit\n            columns (list(str)): the column headings for the Dataframe\n\n        Returns:\n            A pandas.Dataframe\n        \"\"\"\n        import numpy as np\n        import pandas as pd\n\n        filtered_posts = [[s.title, s.selftext, s.id, s.url] for s in posts]\n        filtered_posts = np.array(filtered_posts)\n        reddit_posts_df = pd.DataFrame(filtered_posts,\n                                   columns=columns)\n\n        return reddit_posts_df\n\n    COLUMNS = ['Title', 'Post', 'ID', 'URL']\n\n    # Get the data from Reddit\n    credentials = get_reddit_credentials(project_id=project_id)\n    posts = get_reddit_posts(reddit_credentials=credentials, subreddit_name=subreddit_name,\n                             limit=limit)\n\n    reddit_posts_df = convert_posts_to_dataframe(posts=posts, columns=COLUMNS)\n\n    # Remove all of the posts that don't meet our criteria\n    import re\n    jpg_df = reddit_posts_df[(reddit_posts_df[\"URL\"].str.contains(\"jpg\")) &\n                             (reddit_posts_df[\"Title\"].str.contains(pat = \"\\d+x\\d\"))]\n\n    # Save the dataframe as CSV in Storage\n    csv_str = jpg_df.to_csv()\n\n    storage_client = storage.Client(project=project_id)\n    bucket = storage_client.bucket(gcs_bucket_name)\n\n    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n    csv_file_uri = f\"{gcs_prefix_name}/_reddit-scraped-{subreddit_name}-{timestamp}.csv\"\n\n    file_blob = bucket.blob(csv_file_uri)\n    file_blob.upload_from_string(csv_str)\n\n    return csv_file_uri\n\n"
            ],
            "image": "python:3.7"
          }
        },
        "exec-storage": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "storage"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'spacy' 'google-cloud-firestore' 'google-cloud-storage' 'pandas' 'jsonlines' 'kfp==1.8.12' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef storage(\n    project_id: str,\n    location_name: str,\n    gcs_bucket_name: str,\n    gcs_prefix_name: str,\n    collection_name: str,\n    csv_input_file: str,\n) -> NamedTuple(\n    \"outputs\",\n    [\n        (\"batch_predict_file_uri\", str),\n        (\"posts_csv_file\", str),\n    ]\n):\n\n    from google.cloud import firestore\n    from google.cloud import storage\n\n    import base64\n    from datetime import datetime\n    from io import BytesIO\n    import jsonlines\n    import pandas as pd\n\n    import spacy\n    spacy.cli.download(\"en_core_web_sm\")\n    spacy.prefer_gpu()\n    nlp = spacy.load(\"en_core_web_sm\")\n\n    def make_nice_filename(name):\n        \"\"\"Converts Reddit post title into a meaningful(ish) filename.\n\n        Arguments:\n            name (str): title of the post\n\n        Returns:\n            String. Format is `<adj.>-<nouns>.<cols>x<rows>.jpg`\n        \"\"\"\n        import re\n\n        dims = re.findall(\"\\d+x\\d+\", name)\n        if len(dims) is 0:\n            return \"\"\n\n        dims = dims[0].split(\"x\")\n        if len(dims) is not 2:\n            return \"\"\n\n        tokens = get_tokens(name)\n        new_name = name.lower()[:30]\n\n        if len(tokens) > 0:\n            tokens = tokens[:6] # Arbitrarily keep new names to six words or less\n            new_name = \"_\".join(tokens)\n\n        return f\"{new_name}.{dims[0]}x{dims[1]}.jpg\"\n\n    def get_tokens(title):\n        \"\"\"Analyzes a post for nouns, proper nouns, and adjectives.\n\n        Arguments:\n            title (str): title of the post\n\n        Returns:\n            List of string. Words to use in a filename.    \n        \"\"\"\n        import spacy\n\n        POS = [\"PROPN\", \"NOUN\", \"ADJ\"]\n        words = []\n\n        tokens = nlp(title)\n        for t in tokens:\n            pos = t.pos_\n\n            if pos in POS:\n                words.append(t.text.lower())\n\n        return words\n    def convert_image_to_hash(content):\n        \"\"\"Convert image data to hash value (str).\n\n        Arguments:\n            content (byte array): the image\n\n        Return:\n            The image hash value as a string.\n        \"\"\"\n        import hashlib\n\n        sha1 = hashlib.sha1()\n        jpg_hash = sha1.update(content)\n        jpg_hash = sha1.hexdigest()\n\n        return jpg_hash\n\n    def download_image(url):\n        \"\"\"Download an image from the internet to local file system.\n\n        Arguments:\n            url (str): the image to download\n\n        Returns:\n            Bool. Indicates whether downloading the image was successful.\n        \"\"\"\n        import requests\n\n        r = requests.get(url, stream=True)\n        if r.status_code == 200:\n            r.raw.decode_content = True\n\n            hsh = convert_image_to_hash(r.content)\n            return (r.content, hsh)\n\n    # Begin pipeline\n    storage_client = storage.Client(project=project_id)\n    bucket = storage_client.bucket(gcs_bucket_name)\n\n    firestore_client = firestore.Client(project=project_id)\n    collection_ref = firestore_client.collection(collection_name)\n\n    blob = bucket.blob(csv_input_file)\n    csv_bytes = blob.download_as_string()\n    csv_buffer = BytesIO(csv_bytes)\n\n    jpg_df = pd.read_csv(csv_buffer)\n    batch_prediction_inputs = []\n\n    for i, row, in jpg_df.iterrows():\n        url = row[\"URL\"]\n        title = row[\"Title\"]\n\n        content, hsh = download_image(url)\n\n        # Check whether we already have this image\n        doc_ref = collection_ref.document(hsh)\n        doc_ref = doc_ref.get()\n        if doc_ref.exists:\n            continue\n\n        file_name = make_nice_filename(title)\n\n        img_gcs_uri = f\"gs://{gcs_bucket_name}/{gcs_prefix_name}/{file_name}\"\n        blob_name = f\"{gcs_prefix_name}/{file_name}\"\n\n        file_blob = bucket.blob(blob_name)\n        image_buffer = BytesIO(content)\n\n        # Get image grid metadata\n        file_blob.upload_from_file(image_buffer)\n        jpg_df.at[i, \"URI\"] = img_gcs_uri\n        jpg_df.at[i, \"UID\"] = hsh\n        jpg_df.at[i, \"Filename\"] = file_name\n\n        batch_prediction_inputs.append({\n            \"content\": img_gcs_uri,\n            \"mimeType\": \"image/jpeg\",\n        })\n\n    # Save the dataframe as CSV in Storage (again)\n    csv_str = jpg_df.to_csv()\n    file_blob = bucket.blob(csv_input_file)\n    file_blob.upload_from_string(csv_str)\n\n    # Create batch prediction input file\n    bpi = BytesIO()\n    writer = jsonlines.Writer(bpi)\n    writer.write_all(batch_prediction_inputs)\n    writer.close()\n\n    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n    bpi_gcs_path = f\"{gcs_prefix_name}/_batch_prediction_input_{timestamp}.jsonl\"\n    batch_prediction_input_file = f\"gs://{gcs_bucket_name}/{bpi_gcs_path}\"\n\n    bpi_str = str(bpi.getvalue(), encoding=\"UTF8\")\n    bpi_blob = bucket.blob(bpi_gcs_path)\n    bpi_blob.upload_from_string(bpi_str)\n\n    return (batch_prediction_input_file, csv_input_file)\n\n"
            ],
            "image": "python:3.7"
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "reddit-scraper-pipeline"
    },
    "root": {
      "dag": {
        "tasks": {
          "custom-batch-prediction": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-custom-batch-prediction"
            },
            "dependentTasks": [
              "storage"
            ],
            "inputs": {
              "parameters": {
                "gcs_input_file": {
                  "taskOutputParameter": {
                    "outputParameterKey": "batch_predict_file_uri",
                    "producerTask": "storage"
                  }
                },
                "gcs_output_dir": {
                  "componentInputParameter": "gcs_bp_output"
                },
                "job_display_name": {
                  "runtimeValue": {
                    "constantValue": {
                      "stringValue": "test-custom-bp"
                    }
                  }
                },
                "location": {
                  "componentInputParameter": "location"
                },
                "model_resource_name": {
                  "componentInputParameter": "model_name"
                },
                "project": {
                  "componentInputParameter": "project_id"
                }
              }
            },
            "taskInfo": {
              "name": "custom-batch-prediction"
            }
          },
          "firestore": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-firestore"
            },
            "dependentTasks": [
              "custom-batch-prediction",
              "storage"
            ],
            "inputs": {
              "parameters": {
                "batch_prediction_uri": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "custom-batch-prediction"
                  }
                },
                "collection_name": {
                  "componentInputParameter": "collection_name"
                },
                "csv_input_file": {
                  "taskOutputParameter": {
                    "outputParameterKey": "posts_csv_file",
                    "producerTask": "storage"
                  }
                },
                "gcs_bucket_name": {
                  "componentInputParameter": "gcs_bucket"
                },
                "gcs_prefix_name": {
                  "componentInputParameter": "gcs_prefix"
                },
                "percentage": {
                  "componentInputParameter": "percentage"
                },
                "project_id": {
                  "componentInputParameter": "project_id"
                },
                "subreddit_name": {
                  "componentInputParameter": "subreddit_name"
                },
                "threshold": {
                  "componentInputParameter": "threshold"
                }
              }
            },
            "taskInfo": {
              "name": "firestore"
            }
          },
          "reddit": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-reddit"
            },
            "inputs": {
              "parameters": {
                "gcs_bucket_name": {
                  "componentInputParameter": "gcs_bucket"
                },
                "gcs_prefix_name": {
                  "componentInputParameter": "gcs_prefix"
                },
                "limit": {
                  "componentInputParameter": "limit"
                },
                "project_id": {
                  "componentInputParameter": "project_id"
                },
                "secret_name": {
                  "componentInputParameter": "secret_name"
                },
                "subreddit_name": {
                  "componentInputParameter": "subreddit_name"
                }
              }
            },
            "taskInfo": {
              "name": "reddit"
            }
          },
          "storage": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-storage"
            },
            "dependentTasks": [
              "reddit"
            ],
            "inputs": {
              "parameters": {
                "collection_name": {
                  "componentInputParameter": "collection_name"
                },
                "csv_input_file": {
                  "taskOutputParameter": {
                    "outputParameterKey": "Output",
                    "producerTask": "reddit"
                  }
                },
                "gcs_bucket_name": {
                  "componentInputParameter": "gcs_bucket"
                },
                "gcs_prefix_name": {
                  "componentInputParameter": "gcs_prefix"
                },
                "location_name": {
                  "componentInputParameter": "location"
                },
                "project_id": {
                  "componentInputParameter": "project_id"
                }
              }
            },
            "taskInfo": {
              "name": "storage"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "collection_name": {
            "type": "STRING"
          },
          "gcs_bp_output": {
            "type": "STRING"
          },
          "gcs_bucket": {
            "type": "STRING"
          },
          "gcs_prefix": {
            "type": "STRING"
          },
          "limit": {
            "type": "INT"
          },
          "location": {
            "type": "STRING"
          },
          "model_name": {
            "type": "STRING"
          },
          "percentage": {
            "type": "DOUBLE"
          },
          "project_id": {
            "type": "STRING"
          },
          "secret_name": {
            "type": "STRING"
          },
          "subreddit_name": {
            "type": "STRING"
          },
          "threshold": {
            "type": "DOUBLE"
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.12"
  },
  "runtimeConfig": {
    "gcsOutputDirectory": "gs://fantasy-maps/pipeline_root",
    "parameters": {
      "collection_name": {
        "stringValue": "FantasyMapsTest2"
      },
      "gcs_bp_output": {
        "stringValue": "gs://fantasy-maps/ScrapedData"
      },
      "gcs_bucket": {
        "stringValue": "fantasy-maps"
      },
      "gcs_prefix": {
        "stringValue": "ScrapedData"
      },
      "limit": {
        "intValue": "300"
      },
      "location": {
        "stringValue": "us-central1"
      },
      "model_name": {
        "stringValue": "projects/fantasymaps-334622/locations/us-central1/models/4304645197347684352"
      },
      "percentage": {
        "doubleValue": 0.1
      },
      "project_id": {
        "stringValue": "fantasymaps-334622"
      },
      "secret_name": {
        "stringValue": "reddit-api-key"
      },
      "subreddit_name": {
        "stringValue": "battlemaps"
      },
      "threshold": {
        "doubleValue": 0.3
      }
    }
  }
}