{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "551c8bcb",
   "metadata": {},
   "source": [
    "# Extract training data from Reddit\n",
    "\n",
    "This notebook uses Cloud Secret Manager to import an API key into a Vertex AI pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f67b63f",
   "metadata": {},
   "source": [
    "## Install all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bafcfe57",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-secret-manager in /opt/conda/lib/python3.7/site-packages (2.10.0)\n",
      "Requirement already satisfied: google-cloud-aiplatform in /home/jupyter/.local/lib/python3.7/site-packages (1.11.0)\n",
      "Requirement already satisfied: kfp in /home/jupyter/.local/lib/python3.7/site-packages (1.8.12)\n",
      "Requirement already satisfied: google-cloud-pipeline-components in /opt/conda/lib/python3.7/site-packages (1.0.2)\n",
      "Requirement already satisfied: praw in /home/jupyter/.local/lib/python3.7/site-packages (7.5.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-secret-manager) (0.12.3)\n",
      "Requirement already satisfied: proto-plus>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-secret-manager) (1.19.8)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-secret-manager) (1.31.5)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (21.3)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.43.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.30.1)\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (0.11.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.3.2 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (0.4.0)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (0.8.9)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (0.13)\n",
      "Requirement already satisfied: jsonschema<4,>=3.0.1 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied: fire<1,>=0.3.1 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions<4,>=3.7.4 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (3.10.0.2)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.14 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (0.1.14)\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (18.20.0)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (1.7.1)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (3.19.1)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp) (8.0.3)\n",
      "Requirement already satisfied: strip-hints<1,>=0.1.8 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (0.1.10)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.8.2)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (1.2.13)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (1.12.8)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (5.4.1)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (2.0.0)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (3.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp) (1.35.0)\n",
      "Requirement already satisfied: google-cloud-notebooks>=0.4.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-pipeline-components) (1.2.1)\n",
      "Requirement already satisfied: update-checker>=0.18 in /home/jupyter/.local/lib/python3.7/site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /opt/conda/lib/python3.7/site-packages (from praw) (1.2.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in /home/jupyter/.local/lib/python3.7/site-packages (from praw) (2.3.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<2,>=0.9->kfp) (1.16.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9,>=7.1.2->kfp) (4.8.2)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp) (1.13.3)\n",
      "Requirement already satisfied: termcolor in /home/jupyter/.local/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp) (1.1.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-secret-manager) (59.4.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-secret-manager) (2021.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-secret-manager) (2.26.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-secret-manager) (1.53.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-secret-manager) (1.42.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.20.2)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp) (0.1.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp) (4.2.4)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.2.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp) (0.18.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2021.10.8)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.26.7)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-aiplatform) (3.0.6)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp) (0.37.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-secret-manager) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-secret-manager) (3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click<9,>=7.1.2->kfp) (3.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp) (3.1.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.21)\n"
     ]
    }
   ],
   "source": [
    "! pip install google-cloud-secret-manager google-cloud-aiplatform kfp google-cloud-pipeline-components praw --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b07d7e2",
   "metadata": {},
   "source": [
    "### Set project information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36f9b419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  fantasymaps-334622\n"
     ]
    }
   ],
   "source": [
    "# Get your GCP project id from gcloud\n",
    "shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "PROJECT_ID=shell_output[0]\n",
    "print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103288f3",
   "metadata": {},
   "source": [
    "### Set IAM permissions on your service account\n",
    "\n",
    "`secretmanager.versions.access`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d9b80e",
   "metadata": {},
   "source": [
    "## Store your API key in Cloud Secret Manager\n",
    "\n",
    "Although you can [create a new secret in Cloud Secret Manager programmatically](https://cloud.google.com/secret-manager/docs/creating-and-accessing-secrets#create), in this notebook you must create it using the Cloud Console.\n",
    "\n",
    "To create a new secret in the Cloud Console, do the following:\n",
    "\n",
    "  1. Open the [Cloud Console](https://console.cloud.google.com/security/secret-manager).\n",
    "  1. Click **Create secret**.\n",
    "  1. In the **Create secret** page, do the following:\n",
    "     \n",
    "     + Give your secret a memorable name. This notebook uses the Reddit API, so the name of the secret\n",
    "       is 'reddit-api-key'.\n",
    "     + Upload the credentials file. In this example, the `client_id`, `secret`, and `user_agent` credentials\n",
    "       provided by Reddit are stored as JSON in a single file.\n",
    "  \n",
    "  1. Click **Create secret** at the bottom of the page.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc8e2cf",
   "metadata": {},
   "source": [
    "## Access the key programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import secretmanager\n",
    "import json\n",
    "\n",
    "client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "secret_resource_name = f\"projects/{PROJECT_ID}/secrets/reddit-api-key/versions/1\"\n",
    "\n",
    "response = client.access_secret_version(request={\"name\": secret_resource_name})\n",
    "\n",
    "payload = response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "reddit_key_json = json.loads(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8babfd8",
   "metadata": {},
   "source": [
    "### Construct a request to the Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f329f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(client_id=reddit_key_json[\"client_id\"], \n",
    "                     client_secret=reddit_key_json[\"secret\"],\n",
    "                     user_agent=reddit_key_json[\"user_agent\"])\n",
    "print(f'Reddit is in read-only mode: {reddit.read_only}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a182cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "nan_value = float(\"NaN\")\n",
    "sciatica_sub = \"sciatica\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97953808",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = reddit.subreddit(sciatica_sub).hot(limit=100)\n",
    "filtered_posts = [[s.title, s.selftext, s.id] for s in posts]\n",
    "\n",
    "filtered_posts = np.array(filtered_posts)\n",
    "reddit_posts_df = pd.DataFrame(filtered_posts,\n",
    "                               columns=['Title', 'Posts', 'ID'])\n",
    "\n",
    "# Drop all the rows with empty values\n",
    "reddit_posts_df.replace(\"\", nan_value, inplace=True)\n",
    "reddit_posts_df = reddit_posts_df[reddit_posts_df.Posts != nan_value]\n",
    "\n",
    "\n",
    "# Print \n",
    "reddit_posts_df.head(10)\n",
    "\n",
    "print(reddit_posts_df.iloc[8]['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d6cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from google.cloud import secretmanager\n",
    "import json\n",
    "\n",
    "def get_google_cloud_credentials():\n",
    "    from google import auth\n",
    "    creds, project = auth.default()\n",
    "\n",
    "    LocalCredentials = NamedTuple(\"LocalCredentials\",\n",
    "    [\n",
    "        (\"creds\", str),\n",
    "        (\"project\", str),\n",
    "    ])\n",
    "    return LocalCredentials(creds, project)\n",
    "\n",
    "local_creds = get_google_cloud_credentials()\n",
    "\n",
    "client = secretmanager.SecretManagerServiceClient(credentials=local_creds.creds)\n",
    "\n",
    "secret_resource_name = f\"projects/{local_creds.project}/secrets/reddit-api-key/versions/1\"\n",
    "response = client.access_secret_version(request={\"name\": secret_resource_name})\n",
    "payload = response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "print(json.loads(payload))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a82b0-b51f-44e4-a4a8-2878e863ac6b",
   "metadata": {},
   "source": [
    "## Troubleshoot Firestore component code\n",
    "\n",
    "```json\n",
    "{\n",
    "    'imageWidth': 4620, \n",
    "    'imageHeight': 2940, \n",
    "    'cellOffsetX': 0, \n",
    "    'cellOffsetY': 0, \n",
    "    'cellWidth': 140, \n",
    "    'cellHeight': 140, \n",
    "    'path': 'Abandoned Mine Entrance [33x21] - $5 Rewards/Gridded/G_AbandonedMineEntrance_Crystal.jpg'\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "471e7bf5-5970-41ed-80c0-02c1aacabeb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'imageWidth': 1610, 'imageHeight': 4060, 'cellOffsetX': 0, 'cellOffsetY': 0, 'cellWidth': 70.0, 'cellHeight': 70.0}\n",
      "Set data: {'filename': 'feywild_waterfalls_+_50%_disco.jpg', 'gcsURI': 'gs://fantasy-maps/ScrapedData/feywild_waterfalls_+_50%_disco.jpg', 'source': 'ScrapedData', 'vttData': {'imageWidth': 1610, 'imageHeight': 4060, 'cellOffsetX': 0, 'cellOffsetY': 0, 'cellWidth': 70.0, 'cellHeight': 70.0}, 'userId': 'None'}\n",
      "None\n",
      "Set data: {'filename': 'neon_alley_-_an_experiment_usi.jpg', 'gcsURI': 'gs://fantasy-maps/ScrapedData/neon_alley_-_an_experiment_usi.jpg', 'source': 'ScrapedData', 'vttData': None, 'userId': 'None'}\n",
      "{'imageWidth': 3360, 'imageHeight': 3360, 'cellOffsetX': 0, 'cellOffsetY': 0, 'cellWidth': 70.0, 'cellHeight': 70.0}\n",
      "Set data: {'filename': 'a_small_castle_in_a_forest_[48.jpg', 'gcsURI': 'gs://fantasy-maps/ScrapedData/a_small_castle_in_a_forest_[48.jpg', 'source': 'ScrapedData', 'vttData': {'imageWidth': 3360, 'imageHeight': 3360, 'cellOffsetX': 0, 'cellOffsetY': 0, 'cellWidth': 70.0, 'cellHeight': 70.0}, 'userId': 'None'}\n",
      "{'imageWidth': 4480, 'imageHeight': 6440, 'cellOffsetX': 0, 'cellOffsetY': 0, 'cellWidth': 140.0, 'cellHeight': 140.0}\n",
      "Set data: {'filename': \"[oc]_desert_oasis_-_airship's_.jpg\", 'gcsURI': \"gs://fantasy-maps/ScrapedData/[oc]_desert_oasis_-_airship's_.jpg\", 'source': 'ScrapedData', 'vttData': {'imageWidth': 4480, 'imageHeight': 6440, 'cellOffsetX': 0, 'cellOffsetY': 0, 'cellWidth': 140.0, 'cellHeight': 140.0}, 'userId': 'None'}\n",
      "None\n",
      "Set data: {'filename': 'river_city_map.jpg', 'gcsURI': 'gs://fantasy-maps/ScrapedData/river_city_map.jpg', 'source': 'ScrapedData', 'vttData': None, 'userId': 'None'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6262/4051675703.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# overwrite and download the image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mdoc_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollection_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjpg_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/firestore_v1/document.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, field_paths, transaction, retry, timeout)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         response_iter = self._client._firestore_api.batch_get_documents(\n\u001b[0;32m--> 402\u001b[0;31m             \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rpc_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         )\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/firestore_v1/services/firestore/client.py\u001b[0m in \u001b[0;36mbatch_get_documents\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;31m# Done; return the response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0msleep_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                 \u001b[0mon_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m             )\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, deadline, on_error)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mprefetch_first\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_prefetch_first_result_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             return _StreamingResponseIterator(\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefetch_first_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefetch_first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             )\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, wrapped, prefetch_first_result)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprefetch_first_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stored_first_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# It is possible the wrapped method isn't an iterable (a grpc.Call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    815\u001b[0m                          self._state.code is not None))\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m             \u001b[0m_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_response_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/grpc/_common.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(wait_fn, wait_complete_fn, timeout, spin_cb)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwait_complete_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0m_wait_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAXIMUM_WAIT_TIMEOUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspin_cb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/grpc/_common.py\u001b[0m in \u001b[0;36m_wait_once\u001b[0;34m(wait_fn, timeout, spin_cb)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wait_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspin_cb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mwait_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspin_cb\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mspin_cb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "project_id = PROJECT_ID\n",
    "collection_name = \"FantasyMapsTest\"\n",
    "gcs_bucket_name = \"fantasy-maps\"\n",
    "gcs_prefix_name = \"ScrapedData\"\n",
    "csv_input_file = \"ScrapedData/reddit-scraped-20220404223231.csv\"\n",
    "\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "from io import BytesIO\n",
    "import json\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import regex as re\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "from google.cloud import firestore\n",
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client(project=project_id)\n",
    "bucket = storage_client.bucket(gcs_bucket_name)\n",
    "\n",
    "firestore_client = firestore.Client(project=project_id)\n",
    "collection_ref = firestore_client.collection(collection_name)\n",
    "\n",
    "blob = bucket.blob(csv_input_file)\n",
    "csv_bytes = blob.download_as_string()\n",
    "csv_buffer = BytesIO(csv_bytes)\n",
    "\n",
    "jpg_df = pd.read_csv(csv_buffer)\n",
    "\n",
    "hashes = [None] * len(jpg_df.index)\n",
    "jpg_df.insert(1, \"HashId\", hashes, True)\n",
    "jpg_df.insert(6, \"GcsURI\", hashes, True)\n",
    "\n",
    "# Concatenate string of batch prediction inputs\n",
    "bp_inputs = \"\"\n",
    "\n",
    "def make_nice_filename(name):\n",
    "    regex = \"[\\s|\\(|\\\"|\\)]\"\n",
    "    new_name = re.sub(regex, \"_\", name)\n",
    "    new_name = new_name.lower()[:30]\n",
    "    new_name = new_name.replace(\"__\", \"_\")\n",
    "    return f\"{new_name}.jpg\"\n",
    "\n",
    "\n",
    "def create_vtt_json(content, title):\n",
    "    img = Image.open(BytesIO(content))\n",
    "    w, h = img.size\n",
    "    \n",
    "    dims = re.findall(\"\\d+x\\d+\", title)\n",
    "    if len(dims) is 0:\n",
    "        return None\n",
    "    \n",
    "    dims = dims[0].split(\"x\")\n",
    "    \n",
    "    if len(dims) is not 2:\n",
    "        return None\n",
    "    \n",
    "    rows = int(dims[0])\n",
    "    cols = int(dims[1])\n",
    "    \n",
    "    cell_w = w / rows\n",
    "    cell_h = h / cols\n",
    "    if cell_w != cell_h:\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"imageWidth\": w,\n",
    "        \"imageHeight\": h,\n",
    "        \"cellOffsetX\": 0,\n",
    "        'cellOffsetY': 0, \n",
    "        'cellWidth': cell_w, \n",
    "        'cellHeight': cell_h, \n",
    "    }\n",
    "    \n",
    "\n",
    "# Iterate over JPG URIs, download them in batches, convert to sha values\n",
    "for i, r in jpg_df.iterrows():\n",
    "    jpg_url = r[\"URL\"]\n",
    "    title = r[\"Title\"]\n",
    "    \n",
    "    req = requests.get(jpg_url, stream=True)\n",
    "    if req.status_code == 200:\n",
    "        req.raw.decode_content = True\n",
    "        sha1 = hashlib.sha1()\n",
    "        jpg_hash = sha1.update(req.content)\n",
    "        jpg_hash = sha1.hexdigest()\n",
    "        \n",
    "        jpg_df[\"HashId\"][i] = jpg_hash\n",
    "        #print(f\"Index {i}, hash {jpg_hash}\")\n",
    "        hashes.append(jpg_hash)\n",
    "        \n",
    "        # Try to fetch each document from Firestore. If it does not exist,\n",
    "        # overwrite and download the image.\n",
    "        doc_ref = collection_ref.document(jpg_hash)\n",
    "        doc = doc_ref.get()\n",
    "        if not doc.exists:\n",
    "            \n",
    "            file_name = make_nice_filename(title)\n",
    "            img_gcs_uri = f\"gs://{gcs_bucket_name}/{gcs_prefix_name}/{file_name}\"\n",
    "            blob_name = f\"{gcs_prefix_name}/{file_name}\"\n",
    "            \n",
    "            file_blob = bucket.blob(blob_name)\n",
    "            image_buffer = BytesIO(req.content)\n",
    "            \n",
    "            # Get image grid metadata\n",
    "            img_data = create_vtt_json(req.content, title)\n",
    "            print(img_data)\n",
    "            \n",
    "            file_blob.upload_from_file(BytesIO(req.content))\n",
    "            \n",
    "            data = {\n",
    "                u\"filename\": file_name,\n",
    "                u\"gcsURI\": img_gcs_uri,\n",
    "                u\"source\": gcs_prefix_name,\n",
    "                u\"vttData\": img_data,\n",
    "                u\"userId\": \"None\",\n",
    "            }\n",
    "            doc_ref.set(data)\n",
    "            print(f\"Set data: {data}\")\n",
    "            bp_inputs += json.dumps({ \"content\": img_gcs_uri, \"mimeType\": \"image/jpeg\"})\n",
    "            bp_inputs += \"\\n\"\n",
    "\n",
    "# No fresh JPGs in this scraping; return empty string\n",
    "if bp_inputs is \"\":\n",
    "    # return \"\"\n",
    "    print(\"no inputs\")\n",
    "            \n",
    "print(f\"First ten: {jpg_df.head(10)}\")\n",
    "\n",
    "# Save the batch_predict file\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\") \n",
    "batch_predict_file_uri = f\"gs://{gcs_bucket_name}/{gcs_prefix_name}/bp_input_{timestamp}.jsonl\"\n",
    "\n",
    "bp_blob_name = f\"{gcs_prefix_name}/bp_input_{timestamp}.jsonl\"\n",
    "bp_blob = bucket.blob(bp_blob_name)\n",
    "\n",
    "bp_blob.upload_from_string(bp_inputs)\n",
    "\n",
    "print(batch_predict_file_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add52e89-ed90-4d32-98ab-74ecd2e37c71",
   "metadata": {},
   "source": [
    "## Troubleshoot BP output to FS component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c10d944a-c03b-4c60-9065-91f44fd5c7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonlines in /home/jupyter/.local/lib/python3.7/site-packages (3.0.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from jsonlines) (21.2.0)\n",
      "Requirement already satisfied: typing-extensions in /home/jupyter/.local/lib/python3.7/site-packages (from jsonlines) (3.10.0.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6854bd8e-a6b3-492c-9153-d35aa2aca448",
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_resource = \"projects/733537716875/locations/us-central1/batchPredictionJobs/3414681244372303872\"\n",
    "collection_name = \"FantasyMapsTest\"\n",
    "gcs_bucket_name = \"fantasy-maps\"\n",
    "project = \"fantasymaps-334622\"\n",
    "location = \"us-central1\"\n",
    "minimum_confidence = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5a104a15-07ef-4df7-bd72-747034539628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_bp_output_to_firestore(\n",
    "    bp_resource=bp_resource,\n",
    "    collection_name=collection_name,\n",
    "    gcs_bucket_name=gcs_bucket_name,\n",
    "    project=project,\n",
    "    location=location,\n",
    "    minimum_confidence=minimum_confidence):\n",
    "    \n",
    "    import json\n",
    "    \n",
    "    from google.cloud import aiplatform as aip\n",
    "    from google.cloud import firestore as fs\n",
    "    \n",
    "    aip.init(project=project, location=location)\n",
    "    \n",
    "    bp_job = aip.BatchPredictionJob(\n",
    "        batch_prediction_job_name=bp_resource)\n",
    "    \n",
    "    output_info = bp_job.output_info\n",
    "    \n",
    "    # Get the predictions out of GCS\n",
    "    predictions = []\n",
    "    for out in bp_job.iter_outputs():\n",
    "        out_str = out.download_as_string()\n",
    "        p = out_str.decode(\"utf-8\")\n",
    "        \n",
    "        ps = p.split(\"\\n\")\n",
    "        predictions.extend(ps)\n",
    "\n",
    "    if len(predictions) is 0:\n",
    "        return\n",
    "    \n",
    "    fs_client = fs.Client()\n",
    "    collection_ref = fs_client.collection(collection_name)\n",
    "    \n",
    "    docs = []\n",
    "    prediction_data = dict()\n",
    "    \n",
    "    # Query Firestore for all documents relevant to these predictions\n",
    "    for p in predictions:\n",
    "        try:\n",
    "            data = json.loads(p)\n",
    "            instance = data[\"instance\"][\"content\"]\n",
    "            prediction_data[instance] = data\n",
    "            docs_ref = collection_ref.where(\"gcsURI\", \"==\", instance).stream()\n",
    "            \n",
    "            docs_tmp = [doc for doc in docs_ref]\n",
    "            docs.extend(docs_tmp)\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(p)\n",
    "    \n",
    "    print(f\"Images processed: {len(docs)}\")\n",
    "        \n",
    "    # Update all of the Firestore documents with the predictions\n",
    "    for d in docs:\n",
    "        doc_dict = d.to_dict()\n",
    "        gcsURI = doc_dict[\"gcsURI\"]\n",
    "        doc_predictions = prediction_data[gcsURI]\n",
    "        \n",
    "        # Iterate over bboxes and labels to create\n",
    "        # training-ready data\n",
    "        bboxes = doc_predictions[\"prediction\"][\"bboxes\"]\n",
    "        labels = doc_predictions[\"prediction\"][\"displayNames\"]\n",
    "        confidences = doc_predictions[\"prediction\"][\"confidences\"]\n",
    "        \n",
    "        training_data = []\n",
    "        \n",
    "        for i, e in enumerate(bboxes):\n",
    "            confidence = confidences[i]\n",
    "            if confidence >= minimum_confidence:\n",
    "                training_data.append({\n",
    "                    \"displayName\": labels[i],\n",
    "                    \"xMin\": e[0],\n",
    "                    \"xMax\": e[1],\n",
    "                    \"yMin\": e[2],\n",
    "                    \"yMax\": e[3],\n",
    "                })\n",
    "        \n",
    "        # If training_data is empty for this image, skip\n",
    "        if len(training_data) is 0:\n",
    "            continue\n",
    "            \n",
    "        d.reference.set({\"predictedBBoxes\": training_data}, merge=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9d8ba0a0-df90-49d4-b49a-e8c39911cb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "65\n",
      "gs://fantasy-maps/ScrapedData/the_cat_sanctury_needs_defendi.jpg\n",
      "gs://fantasy-maps/ScrapedData/fae_village_-_mirage_[30x50]_[.jpg\n",
      "gs://fantasy-maps/ScrapedData/snowy_field.jpg\n",
      "gs://fantasy-maps/ScrapedData/the_citadel_of_ash_[54x88].jpg\n",
      "gs://fantasy-maps/ScrapedData/desert_ruins_[battlemap][2304x.jpg\n",
      "gs://fantasy-maps/ScrapedData/the_rift.jpg\n",
      "gs://fantasy-maps/ScrapedData/winter_battle.jpg\n",
      "gs://fantasy-maps/ScrapedData/[1960x2940][28x42]_into_the_mi.jpg\n",
      "gs://fantasy-maps/ScrapedData/{the_road_to_bellshire}_-enjoy.jpg\n",
      "gs://fantasy-maps/ScrapedData/old_school_battle_map,_stock_d.jpg\n",
      "gs://fantasy-maps/ScrapedData/sith_temple.jpg\n",
      "gs://fantasy-maps/ScrapedData/[oc]_desert_monorail_heist_bat.jpg\n",
      "gs://fantasy-maps/ScrapedData/3rd_time_lucky...._hi_red_spac.jpg\n",
      "gs://fantasy-maps/ScrapedData/the_broken_bridge_[51x62]_[cav.jpg\n",
      "gs://fantasy-maps/ScrapedData/defend_the_wall_!.jpg\n",
      "gs://fantasy-maps/ScrapedData/hillside_expedition_store_[13x.jpg\n",
      "gs://fantasy-maps/ScrapedData/the_forgotten_temple_of_nine_e.jpg\n",
      "gs://fantasy-maps/ScrapedData/the_warrens_caves_-_66x66_ma.jpg\n",
      "gs://fantasy-maps/ScrapedData/desert_road.jpg\n",
      "gs://fantasy-maps/ScrapedData/help_needed!.jpg\n",
      "gs://fantasy-maps/ScrapedData/farm_2.jpg\n",
      "gs://fantasy-maps/ScrapedData/snowy_rocky_terrain.jpg\n",
      "gs://fantasy-maps/ScrapedData/ancient_portal_[battlemap][49x.jpg\n",
      "gs://fantasy-maps/ScrapedData/fantasy_world_map.jpg\n",
      "gs://fantasy-maps/ScrapedData/fantasy_world_map.jpg\n",
      "gs://fantasy-maps/ScrapedData/40_x_28_spaceship_battlemap_2.jpg\n",
      "gs://fantasy-maps/ScrapedData/an_initial_draft_of_the_map_fo.jpg\n",
      "gs://fantasy-maps/ScrapedData/water_cave_30x40_.jpg\n",
      "gs://fantasy-maps/ScrapedData/[26x40]_forge_factory.jpg\n",
      "gs://fantasy-maps/ScrapedData/[oc]_dmhelper_vtt_brand_new_ve.jpg\n",
      "gs://fantasy-maps/ScrapedData/beaverfolk_lodge_40x40_.jpg\n",
      "gs://fantasy-maps/ScrapedData/floating_sanctuary_boss_arena!.jpg\n",
      "gs://fantasy-maps/ScrapedData/old_desert_temple_battle_map_3.jpg\n",
      "gs://fantasy-maps/ScrapedData/cyberquest_mk2_using_droid_car.jpg\n",
      "gs://fantasy-maps/ScrapedData/could_i_interest_you_in_this_e.jpg\n",
      "gs://fantasy-maps/ScrapedData/ancient_ravine_[40x40]_[battle.jpg\n",
      "gs://fantasy-maps/ScrapedData/[25x35]_dragon_lair_[dragon][b.jpg\n",
      "gs://fantasy-maps/ScrapedData/free_adventurer's_guide_to_the.jpg\n",
      "gs://fantasy-maps/ScrapedData/free_adventurer's_guide_to_the.jpg\n",
      "gs://fantasy-maps/ScrapedData/free_adventurer's_guide_to_the.jpg\n",
      "gs://fantasy-maps/ScrapedData/free_adventurer's_guide_to_the.jpg\n",
      "gs://fantasy-maps/ScrapedData/first_fantasy_map_for_a_dark_f.jpg\n",
      "gs://fantasy-maps/ScrapedData/an_icy_crater_island!.jpg\n",
      "gs://fantasy-maps/ScrapedData/gnome_burrow.jpg\n",
      "gs://fantasy-maps/ScrapedData/a_camp_at_night_[4k]_[gridless.jpg\n",
      "gs://fantasy-maps/ScrapedData/sauhagin_attack_25_x25.jpg\n",
      "gs://fantasy-maps/ScrapedData/dwarven_tunneler_[30x60]_-_one.jpg\n",
      "gs://fantasy-maps/ScrapedData/grab_a_pina_colada_and_save_th.jpg\n",
      "gs://fantasy-maps/ScrapedData/emperor's_throne_room.jpg\n",
      "gs://fantasy-maps/ScrapedData/arcology_apartment_-_cyberpunk.jpg\n",
      "gs://fantasy-maps/ScrapedData/curse_of_the_crimson_throne_pa.jpg\n",
      "gs://fantasy-maps/ScrapedData/the_blossoming_pool_[4k]_[grid.jpg\n",
      "gs://fantasy-maps/ScrapedData/underwater_octopus_temple_with.jpg\n",
      "gs://fantasy-maps/ScrapedData/heroquest.jpg\n",
      "gs://fantasy-maps/ScrapedData/dwarven_undercity_104x64_140p.jpg\n",
      "gs://fantasy-maps/ScrapedData/[battlemap]_the_sewer_ends_whe.jpg\n",
      "gs://fantasy-maps/ScrapedData/made_a_dragon_skull_for_my_mod.jpg\n",
      "gs://fantasy-maps/ScrapedData/castle_core_room_battle_map_30.jpg\n",
      "gs://fantasy-maps/ScrapedData/cyberquest?_non_serious_board_.jpg\n",
      "gs://fantasy-maps/ScrapedData/subterranean_waterfall_[48x48].jpg\n",
      "gs://fantasy-maps/ScrapedData/riverbank_outpost_attacked_by_.jpg\n",
      "gs://fantasy-maps/ScrapedData/[oc]_nord_tochy_description_i.jpg\n",
      "gs://fantasy-maps/ScrapedData/underhive_/_wastelands_highly_.jpg\n",
      "gs://fantasy-maps/ScrapedData/[40x40]_jungle_encounter_-_a_b.jpg\n",
      "gs://fantasy-maps/ScrapedData/winter_port_30x40_.jpg\n"
     ]
    }
   ],
   "source": [
    "save_bp_output_to_firestore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9383ecef",
   "metadata": {},
   "source": [
    "## Create a custom Reddit pipelines component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "99aef58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component)\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fdc68927",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"praw\",\n",
    "                                \"google-cloud-secret-manager\",\n",
    "                                \"google-cloud-storage\",\n",
    "                                \"numpy\",\n",
    "                                \"pandas\"],\n",
    "           output_component_file=\"reddit.yaml\")\n",
    "def reddit(\n",
    "    secret_name: str,\n",
    "    subreddit_name: str,\n",
    "    gcs_bucket_name: str,\n",
    "    gcs_prefix_name: str,\n",
    "    project_id: str,\n",
    ") -> str:\n",
    "    from datetime import datetime\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import praw\n",
    "    import regex as re\n",
    "    \n",
    "    from google.cloud import storage\n",
    "\n",
    "    def get_reddit_credentials(project_id):\n",
    "        from google.cloud import secretmanager\n",
    "        import json\n",
    "\n",
    "        client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "        secret_resource_name = f\"projects/{project_id}/secrets/{secret_name}/versions/1\"\n",
    "        response = client.access_secret_version(request={\"name\": secret_resource_name})\n",
    "        payload = response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "        return json.loads(payload)\n",
    "    \n",
    "    def get_reddit_posts(reddit_credentials):\n",
    "        import praw\n",
    "\n",
    "        reddit = praw.Reddit(client_id=reddit_credentials[\"client_id\"], \n",
    "                     client_secret=reddit_credentials[\"secret\"],\n",
    "                     user_agent=reddit_credentials[\"user_agent\"])\n",
    "        print(f\"Reddit is in read-only mode: {reddit.read_only}\")\n",
    "        return reddit.subreddit(subreddit_name).hot(limit=100)\n",
    "    \n",
    "    nan_value = float(\"NaN\")\n",
    "    \n",
    "    print(f\"Project ID is: {project_id}\")\n",
    "    \n",
    "    # Get the data from Reddit\n",
    "    credentials = get_reddit_credentials(project_id)\n",
    "    posts = get_reddit_posts(credentials)\n",
    "    \n",
    "    posts = filter(lambda p: len(re.findall(\"\\d+x\\d+\", p.title)) > 0, posts)\n",
    "    \n",
    "    # Filter the posts the data that we want and store as DataFrame\n",
    "    filtered_posts = [[s.title, s.selftext, s.id, s.url] for s in posts]\n",
    "\n",
    "    filtered_posts = np.array(filtered_posts)\n",
    "    reddit_posts_df = pd.DataFrame(filtered_posts,\n",
    "                               columns=['Title', 'Post', 'ID', 'URL'])\n",
    "\n",
    "    reddit_posts_df.replace(\"\", nan_value, inplace=True)\n",
    "    reddit_posts_df = reddit_posts_df[reddit_posts_df[\"Post\"] != nan_value]\n",
    "    \n",
    "    jpg_df = reddit_posts_df.loc[reddit_posts_df[\"URL\"].str.contains(\"jpg\")]\n",
    "    jpg_df.head(10)\n",
    "    \n",
    "    # Save the dataframe as CSV in Storage\n",
    "    csv_str = jpg_df.to_csv()\n",
    "    \n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(gcs_bucket_name)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    csv_file_uri = f\"{gcs_prefix_name}/reddit-scraped-{subreddit_name}-{timestamp}.csv\"\n",
    "    \n",
    "    file_blob = bucket.blob(csv_file_uri)\n",
    "    file_blob.upload_from_string(csv_str)\n",
    "    \n",
    "    return csv_file_uri\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52768187-c334-451e-819d-e0d7fd1f5161",
   "metadata": {},
   "source": [
    "## Create the Firestore component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "50ff240e-a81e-4504-8803-4ecbf2f8962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "@component(packages_to_install=[\"google-cloud-firestore\",\n",
    "                                \"google-cloud-storage\",\n",
    "                                \"numpy\",\n",
    "                                \"pandas\",\n",
    "                                \"Pillow\"],\n",
    "           output_component_file=\"firestore.yaml\")\n",
    "def firestore(\n",
    "    subreddit_name: str,\n",
    "    collection_name: str,\n",
    "    gcs_bucket_name: str,\n",
    "    gcs_prefix_name: str,\n",
    "    csv_input_file: str,\n",
    "    project_id: str,\n",
    ") -> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"batch_predict_file_uri\", str),\n",
    "        (\"bp_inputs_count\", int),\n",
    "    ]\n",
    "):\n",
    "    \n",
    "    from datetime import datetime\n",
    "    import hashlib\n",
    "    from io import BytesIO\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from PIL import Image\n",
    "    import re\n",
    "    import requests\n",
    "    import shutil\n",
    "\n",
    "    from google.cloud import firestore\n",
    "    from google.cloud import storage\n",
    "\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(gcs_bucket_name)\n",
    "\n",
    "    firestore_client = firestore.Client(project=project_id)\n",
    "    collection_ref = firestore_client.collection(collection_name)\n",
    "\n",
    "    blob = bucket.blob(csv_input_file)\n",
    "    csv_bytes = blob.download_as_string()\n",
    "    csv_buffer = BytesIO(csv_bytes)\n",
    "\n",
    "    jpg_df = pd.read_csv(csv_buffer)\n",
    "\n",
    "    hashes = [None] * len(jpg_df.index)\n",
    "    jpg_df.insert(1, \"HashId\", hashes, True)\n",
    "    jpg_df.insert(6, \"GcsURI\", hashes, True)\n",
    "\n",
    "    # Concatenate string of batch prediction inputs\n",
    "    bp_inputs = \"\"\n",
    "    bp_inputs_count = 0\n",
    "\n",
    "    def make_nice_filename(name):\n",
    "        regex = \"[\\s|\\(|\\\"|\\)]\"\n",
    "        new_name = re.sub(regex, \"_\", name)\n",
    "        new_name = new_name.lower()[:30]\n",
    "        new_name = new_name.replace(\"__\", \"_\")\n",
    "        return f\"{new_name}.jpg\"\n",
    "\n",
    "    # Iterate over JPG URIs, download them in batches, convert to sha values\n",
    "    for i, r in jpg_df.iterrows():\n",
    "        jpg_url = r[\"URL\"]\n",
    "        title = r[\"Title\"]\n",
    "\n",
    "        req = requests.get(jpg_url, stream=True)\n",
    "        if req.status_code == 200:\n",
    "            req.raw.decode_content = True\n",
    "            sha1 = hashlib.sha1()\n",
    "            jpg_hash = sha1.update(req.content)\n",
    "            jpg_hash = sha1.hexdigest()\n",
    "\n",
    "            jpg_df[\"HashId\"][i] = jpg_hash\n",
    "            #print(f\"Index {i}, hash {jpg_hash}\")\n",
    "            hashes.append(jpg_hash)\n",
    "\n",
    "            # Try to fetch each document from Firestore. If it does not exist,\n",
    "            # overwrite and download the image.\n",
    "            doc_ref = collection_ref.document(jpg_hash)\n",
    "            doc = doc_ref.get()\n",
    "            if not doc.exists:\n",
    "\n",
    "                file_name = make_nice_filename(title)\n",
    "                img_gcs_uri = f\"gs://{gcs_bucket_name}/{gcs_prefix_name}/{file_name}\"\n",
    "                blob_name = f\"{gcs_prefix_name}/{file_name}\"\n",
    "\n",
    "                file_blob = bucket.blob(blob_name)\n",
    "                image_buffer = BytesIO(req.content)\n",
    "                \n",
    "                # Get the image size and width\n",
    "                img = Image.open(image_buffer)\n",
    "                width, height = img.size\n",
    "                \n",
    "                file_blob.upload_from_file(image_buffer)\n",
    "\n",
    "                data = {\n",
    "                    u\"filename\": file_name,\n",
    "                    u\"gcsURI\": img_gcs_uri,\n",
    "                    u\"source\": gcs_prefix_name,\n",
    "                    u\"userId\": \"None\",\n",
    "                }\n",
    "                doc_ref.set(data)\n",
    "                print(f\"Set data: {data}\")\n",
    "                bp_inputs += json.dumps({ \"content\": img_gcs_uri, \"mimeType\": \"image/jpeg\"})\n",
    "                bp_inputs += \"\\n\"\n",
    "                bp_inputs_count += 1\n",
    "\n",
    "    print(f\"bp_inputs_count={bp_inputs_count}\")\n",
    "\n",
    "    # Save the batch_predict file\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\") \n",
    "\n",
    "    bp_blob_name = f\"{gcs_prefix_name}/bp_input_{subreddit_name}_{timestamp}.jsonl\"\n",
    "    batch_predict_file_uri = f\"gs://{gcs_bucket_name}/{bp_blob_name}\"\n",
    "    bp_blob = bucket.blob(bp_blob_name)\n",
    "\n",
    "    bp_blob.upload_from_string(bp_inputs)\n",
    "    \n",
    "    return (batch_predict_file_uri, bp_inputs_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832945c7-b5da-446c-818c-1c45ff0f11ec",
   "metadata": {},
   "source": [
    "## Create a (better) batch prediction component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "7cd638d8-095a-4e32-91bd-e95cfc46896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "@component(packages_to_install=[\"google-cloud-aiplatform\"],\n",
    "           output_component_file=\"batch_prediction.yaml\")\n",
    "def batch_prediction(\n",
    "    gcs_bucket_name: str,\n",
    "    gcs_prefix_name: str,\n",
    "    input_file_1: str,\n",
    "    input_file_2: str,\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    model_id: str,\n",
    ") -> str:\n",
    "    \n",
    "    from google.cloud import aiplatform as aip\n",
    "    from datetime import datetime\n",
    "\n",
    "    csv_input_files = [input_file_1, input_file_2]\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    aip.init(project=project_id, location=location)\n",
    "    \n",
    "    model_resource_name = f\"projects/{project_id}/locations/{location}/models/{model_id}\"\n",
    "    model = aip.Model(model_resource_name)\n",
    "    \n",
    "    batch_prediction_job = model.batch_predict(\n",
    "        job_display_name=f\"reddit-scraping-batch-predict-{timestamp}\",\n",
    "        gcs_source=csv_input_files,\n",
    "        gcs_destination_prefix=f\"gs://{gcs_bucket_name}/{gcs_prefix_name}\",\n",
    "        sync=True\n",
    "    )\n",
    "    \n",
    "    batch_prediction_job.wait()\n",
    "\n",
    "    print(batch_prediction_job.display_name)\n",
    "    print(batch_prediction_job.resource_name)\n",
    "    print(batch_prediction_job.state)\n",
    "    return str(batch_prediction_job.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcb311b-c0c0-4f71-82ae-ef4f26dc84b6",
   "metadata": {},
   "source": [
    "## Create a prediction-to-Firestore component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4e183223-0aed-4658-997d-3dc2a0597af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-aiplatform\", \"google-cloud-firestore\"],\n",
    "           output_component_file=\"prediction_to_firestore.yaml\")\n",
    "def save_bp_output_to_firestore(\n",
    "    bp_resource: str,\n",
    "    collection_name: str,\n",
    "    gcs_bucket_name: str,\n",
    "    project: str,\n",
    "    location: str,\n",
    "    minimum_confidence: float):\n",
    "    \n",
    "    import json\n",
    "    \n",
    "    from google.cloud import aiplatform as aip\n",
    "    from google.cloud import firestore as fs\n",
    "    \n",
    "    aip.init(project=project, location=location)\n",
    "    \n",
    "    bp_job = aip.BatchPredictionJob(\n",
    "        batch_prediction_job_name=bp_resource)\n",
    "    \n",
    "    output_info = bp_job.output_info\n",
    "    \n",
    "    # Get the predictions out of GCS\n",
    "    predictions = []\n",
    "    for out in bp_job.iter_outputs():\n",
    "        out_str = out.download_as_string()\n",
    "        p = out_str.decode(\"utf-8\")\n",
    "        \n",
    "        ps = p.split(\"\\n\")\n",
    "        predictions.extend(ps)\n",
    "\n",
    "    if len(predictions) is 0:\n",
    "        return\n",
    "    \n",
    "    fs_client = fs.Client(project=project)\n",
    "    collection_ref = fs_client.collection(collection_name)\n",
    "    \n",
    "    docs = []\n",
    "    prediction_data = dict()\n",
    "    \n",
    "    # Query Firestore for all documents relevant to these predictions\n",
    "    for p in predictions:\n",
    "        try:\n",
    "            data = json.loads(p)\n",
    "            instance = data[\"instance\"][\"content\"]\n",
    "            prediction_data[instance] = data\n",
    "            docs_ref = collection_ref.where(\"gcsURI\", \"==\", instance).stream()\n",
    "            \n",
    "            docs_tmp = [doc for doc in docs_ref]\n",
    "            docs.extend(docs_tmp)\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(p)\n",
    "    \n",
    "    print(f\"Images processed: {len(docs)}\")\n",
    "        \n",
    "    # Update all of the Firestore documents with the predictions\n",
    "    for d in docs:\n",
    "        doc_dict = d.to_dict()\n",
    "        gcsURI = doc_dict[\"gcsURI\"]\n",
    "        doc_predictions = prediction_data[gcsURI]\n",
    "        \n",
    "        # Iterate over bboxes and labels to create\n",
    "        # training-ready data\n",
    "        bboxes = doc_predictions[\"prediction\"][\"bboxes\"]\n",
    "        labels = doc_predictions[\"prediction\"][\"displayNames\"]\n",
    "        confidences = doc_predictions[\"prediction\"][\"confidences\"]\n",
    "        \n",
    "        training_data = []\n",
    "        \n",
    "        for i, e in enumerate(bboxes):\n",
    "            confidence = confidences[i]\n",
    "            if confidence >= minimum_confidence:\n",
    "                training_data.append({\n",
    "                    \"displayName\": labels[i],\n",
    "                    \"xMin\": e[0],\n",
    "                    \"xMax\": e[1],\n",
    "                    \"yMin\": e[2],\n",
    "                    \"yMax\": e[3],\n",
    "                })\n",
    "        \n",
    "        # If training_data is empty for this image, skip\n",
    "        if len(training_data) is 0:\n",
    "            continue\n",
    "            \n",
    "        d.reference.set({\"predictedBBoxes\": training_data}, merge=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb86e8e",
   "metadata": {},
   "source": [
    "## Build a simple pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "db0963a0-3d63-4f59-93a8-a41d224f3868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fantasymaps-334622\n"
     ]
    }
   ],
   "source": [
    "GCS_BUCKET = \"fantasy-maps\"\n",
    "GCS_PREFIX = \"ScrapedData\"\n",
    "MODEL_ID = \"7292897899317297152\"\n",
    "LOCATION = \"us-central1\"\n",
    "COLLECTION_NAME = \"FantasyMaps\"\n",
    "print(PROJECT_ID)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6cd70b-ec08-47f0-b428-8a9fae90db60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear out the test collection before continuing\n",
    "\n",
    "# NOTE: DOES NOT WORK :S\n",
    "if COLLECTION_NAME.find(\"Test\") > -1:\n",
    "    \n",
    "    print(f\"Deleting {COLLECTION_NAME} ...\")\n",
    "    from google.cloud import firestore\n",
    "    \n",
    "    client = firestore.Client(project=PROJECT_ID)\n",
    "    coll_ref = client.collection(COLLECTION_NAME)\n",
    "    client.recursive_delete(coll_ref)\n",
    "    \n",
    "    #client.collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a33687d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"reddit-scraper-pipeline\",\n",
    "    description=\"Gets data from a subreddit\",\n",
    "    pipeline_root=f\"gs://{GCS_BUCKET}/pipeline_root\",\n",
    ")\n",
    "def reddit_pipeline(\n",
    "    collection_name: str = COLLECTION_NAME,\n",
    "    secret_name: str = \"reddit-api-key\",\n",
    "    subreddit_name_1: str = \"battlemaps\",\n",
    "    subreddit_name_2: str = \"FantasyMaps\",\n",
    "    gcs_bucket: str = GCS_BUCKET,\n",
    "    gcs_prefix: str = GCS_PREFIX,\n",
    "    project_id: str = PROJECT_ID,\n",
    "    location: str = LOCATION,\n",
    "    model_id: str = MODEL_ID,\n",
    "):\n",
    "    \n",
    "    # First stream of Reddit scraping\n",
    "    reddit_op_1 = reddit(\n",
    "        secret_name=secret_name,\n",
    "        subreddit_name=subreddit_name_1,\n",
    "        gcs_bucket_name=gcs_bucket,\n",
    "        gcs_prefix_name=gcs_prefix,\n",
    "        project_id=project_id\n",
    "    )\n",
    "    \n",
    "    reddit_csv_file_1 = reddit_op_1.output\n",
    "    \n",
    "    firestore_op_1 = firestore(\n",
    "        subreddit_name=subreddit_name_1,\n",
    "        collection_name=collection_name,\n",
    "        gcs_bucket_name=gcs_bucket,\n",
    "        gcs_prefix_name=gcs_prefix,\n",
    "        csv_input_file=reddit_csv_file_1,\n",
    "        project_id=project_id,\n",
    "    )\n",
    "    \n",
    "    # Second stream of Reddit scraping\n",
    "    reddit_op_2 = reddit(\n",
    "        secret_name=secret_name,\n",
    "        subreddit_name=subreddit_name_2,\n",
    "        gcs_bucket_name=gcs_bucket,\n",
    "        gcs_prefix_name=gcs_prefix,\n",
    "        project_id=project_id\n",
    "    )\n",
    "\n",
    "    reddit_csv_file_2 = reddit_op_2.output\n",
    "    \n",
    "    firestore_op_2 = firestore(\n",
    "        subreddit_name=subreddit_name_2,\n",
    "        collection_name=collection_name,\n",
    "        gcs_bucket_name=gcs_bucket,\n",
    "        gcs_prefix_name=gcs_prefix,\n",
    "        csv_input_file=reddit_csv_file_2,\n",
    "        project_id=project_id,\n",
    "    )\n",
    "  \n",
    "    #inputs_count = firestore_op_1.outputs[\"bp_inputs_count\"] + firestore_op_2.outputs[\"bp_inputs_count\"] \n",
    "\n",
    "    with dsl.Condition((firestore_op_1.outputs[\"bp_inputs_count\"] > 0)\n",
    "                      or (firestore_op_2.outputs[\"bp_inputs_count\"] > 0), name=\"hasBPInputs\"):\n",
    "        batch_prediction_op = batch_prediction(\n",
    "            gcs_bucket_name=gcs_bucket,\n",
    "            gcs_prefix_name=gcs_prefix,\n",
    "            input_file_1=firestore_op_1.outputs[\"batch_predict_file_uri\"],\n",
    "            input_file_2=firestore_op_2.outputs[\"batch_predict_file_uri\"],\n",
    "            project_id=project_id,\n",
    "            location=location,\n",
    "            model_id=model_id)\n",
    "\n",
    "        # Set minimum_confidence as global--should be configurable\n",
    "        min_confidence = 0.15\n",
    "\n",
    "        # Update Firestore with BP results\n",
    "        save_bp_output_to_firestore(\n",
    "            bp_resource=batch_prediction_op.output,\n",
    "            collection_name=collection_name,\n",
    "            gcs_bucket_name=gcs_bucket,\n",
    "            project=project_id,\n",
    "            location=location,\n",
    "            minimum_confidence=min_confidence\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "1f32db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=reddit_pipeline, package_path=\"reddit_scraper_pipeline_job.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ff782324",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=LOCATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "cf97982c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/reddit-scraper-pipeline-20220420190135?project=fantasymaps-334622\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    job_spec_path=\"reddit_scraper_pipeline_job.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95653db8-73dd-48f5-9b8f-93ceec38981a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
