{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/telpirion/FantasyMaps/blob/main/notebooks/final/1_firestore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "67741dbe-06db-4f5c-baf9-b75416d69539",
      "metadata": {
        "tags": [],
        "id": "67741dbe-06db-4f5c-baf9-b75416d69539"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8489ffb-e3a6-4cb6-883a-547afd8eac82",
      "metadata": {
        "id": "c8489ffb-e3a6-4cb6-883a-547afd8eac82"
      },
      "source": [
        "# Storing training data in Firestore\n",
        "\n",
        "{TODO: Update the links below.}\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/notebook_template.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fffca17d-1158-4ce0-ad90-f8b869358df5",
      "metadata": {
        "id": "fffca17d-1158-4ce0-ad90-f8b869358df5"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to collect data from social media (e.g. Reddit), preprocess that data, and then store that data in a Firestore collection. In this scenario, you collect fictional, hand-drawn maps that are used for virtual role-playing games. (Later, you convert these maps into training data for a Vertex AI notebook.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c569a85-e173-4c11-9988-7712fb0ac26f",
      "metadata": {
        "tags": [],
        "id": "2c569a85-e173-4c11-9988-7712fb0ac26f"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to create a dataset in Cloud Storage and Firestore from a third-party API.\n",
        "\n",
        "This tutorial uses the following Google Cloud resources:\n",
        "\n",
        "+ Cloud Storage bucket\n",
        "+ Firestore collection\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "1. Collecting images from Reddit\n",
        "1. Storing the images in Cloud Storage\n",
        "1. Inferring training data from the images\n",
        "1. Storing that training data in a Firestore collection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "597098cf-1b89-4466-92d1-6c9434594677",
      "metadata": {
        "id": "597098cf-1b89-4466-92d1-6c9434594677"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "In this tutorial, you collect data from subreddit posts on Reddit. The individual posts are processed and their metadata stored in Firestore. Any image data the posts contain are extracted and stored in a Storage bucket."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2d14d80-853b-40cf-883a-9d762c082a08",
      "metadata": {
        "id": "d2d14d80-853b-40cf-883a-9d762c082a08"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Firestore\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Firestore pricing](https://cloud.google.com/firestore/pricing),\n",
        "and [Cloud Storage pricing](https://cloud.google.com/storage/pricing),\n",
        "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0aad14a-3e5d-4a5f-a56e-9a91a8c4192a",
      "metadata": {
        "id": "c0aad14a-3e5d-4a5f-a56e-9a91a8c4192a"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the following packages required to execute this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "237e0e69-b281-4683-bc2b-570e0ffeb713",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "237e0e69-b281-4683-bc2b-570e0ffeb713",
        "outputId": "b862dcef-2344-41b1-c5af-4fe8c7131c2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "google-cloud-firestore\n",
        "google-cloud-storage\n",
        "praw\n",
        "pandas\n",
        "numpy\n",
        "spacy\n",
        "pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5e29f548-6bce-4b71-9a6c-244fe2ebd28f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e29f548-6bce-4b71-9a6c-244fe2ebd28f",
        "outputId": "467bd635-7412-4a3c-a4d5-4193fd21924a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m961.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.2/326.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.1.4, but you have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install --upgrade -qr requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "235a373b-c3ca-41e5-9a48-e7ac13c2ff5f",
      "metadata": {
        "id": "235a373b-c3ca-41e5-9a48-e7ac13c2ff5f"
      },
      "source": [
        "We will also use a simple natural language parsing library to analyze posts. For this use case, we'll use the open source library [spaCy](https://spacy.io). spaCy requires that a language model be downloaded before it can be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3ec6dea5-e36d-495f-b765-3cf6949136b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ec6dea5-e36d-495f-b765-3cf6949136b9",
        "outputId": "fd7bbda8-4c6f-4481-a469-6d956b67b101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install \"git+https://github.com/telpirion/fantasy-maps-lib.git#egg=fantasy_maps_lib\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhvoZqO0mr6Q",
        "outputId": "c66c3025-c063-41bf-a125-32fb291eb68f"
      },
      "id": "dhvoZqO0mr6Q",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fantasy_maps_lib\n",
            "  Cloning https://github.com/telpirion/fantasy-maps-lib.git to /tmp/pip-install-q8w3ik0n/fantasy-maps-lib_fec2c500b1934deab1d55192b2760fac\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/telpirion/fantasy-maps-lib.git /tmp/pip-install-q8w3ik0n/fantasy-maps-lib_fec2c500b1934deab1d55192b2760fac\n",
            "  Resolved https://github.com/telpirion/fantasy-maps-lib.git to commit 202be49461b3f88f204bec3ea4254e7eb4d05a0c\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting black (from fantasy_maps_lib)\n",
            "  Downloading black-24.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m863.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.10/dist-packages (from fantasy_maps_lib) (1.63.0)\n",
            "Requirement already satisfied: google-cloud-firestore in /usr/local/lib/python3.10/dist-packages (from fantasy_maps_lib) (2.18.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from fantasy_maps_lib) (2.18.2)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.10/dist-packages (from fantasy_maps_lib) (0.4.0)\n",
            "Collecting jsonlines (from fantasy_maps_lib)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fantasy_maps_lib) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fantasy_maps_lib) (2.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from fantasy_maps_lib) (10.4.0)\n",
            "Requirement already satisfied: praw in /usr/local/lib/python3.10/dist-packages (from fantasy_maps_lib) (7.7.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from fantasy_maps_lib) (7.4.4)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from fantasy_maps_lib) (3.7.6)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->fantasy_maps_lib) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->fantasy_maps_lib)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black->fantasy_maps_lib) (24.1)\n",
            "Collecting pathspec>=0.9.0 (from black->fantasy_maps_lib)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->fantasy_maps_lib) (4.2.2)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->fantasy_maps_lib) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->fantasy_maps_lib) (4.12.2)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform->fantasy_maps_lib) (2.19.1)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->fantasy_maps_lib) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->fantasy_maps_lib) (1.24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->fantasy_maps_lib) (3.20.3)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->fantasy_maps_lib) (3.25.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->fantasy_maps_lib) (1.12.5)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->fantasy_maps_lib) (2.0.6)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->fantasy_maps_lib) (2.8.2)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->fantasy_maps_lib) (0.16)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->fantasy_maps_lib) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->fantasy_maps_lib) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->fantasy_maps_lib) (2.32.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->fantasy_maps_lib) (1.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug->fantasy_maps_lib) (1.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imgaug->fantasy_maps_lib) (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imgaug->fantasy_maps_lib) (3.7.1)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.10/dist-packages (from imgaug->fantasy_maps_lib) (0.23.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from imgaug->fantasy_maps_lib) (4.10.0.84)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug->fantasy_maps_lib) (2.34.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->fantasy_maps_lib) (24.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->fantasy_maps_lib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fantasy_maps_lib) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->fantasy_maps_lib) (2024.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from praw->fantasy_maps_lib) (2.4.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw->fantasy_maps_lib) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw->fantasy_maps_lib) (1.8.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->fantasy_maps_lib) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->fantasy_maps_lib) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->fantasy_maps_lib) (1.2.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (4.66.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (71.0.4)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (3.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform->fantasy_maps_lib) (1.64.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform->fantasy_maps_lib) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform->fantasy_maps_lib) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform->fantasy_maps_lib) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform->fantasy_maps_lib) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform->fantasy_maps_lib) (4.9)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform->fantasy_maps_lib) (0.13.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->fantasy_maps_lib) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform->fantasy_maps_lib) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform->fantasy_maps_lib) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->fantasy_maps_lib) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->fantasy_maps_lib) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->fantasy_maps_lib) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->fantasy_maps_lib) (2024.7.4)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug->fantasy_maps_lib) (3.3)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug->fantasy_maps_lib) (2024.8.24)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug->fantasy_maps_lib) (0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->fantasy_maps_lib) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->fantasy_maps_lib) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->fantasy_maps_lib) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->fantasy_maps_lib) (13.8.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->fantasy_maps_lib) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->fantasy_maps_lib) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy->fantasy_maps_lib) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->fantasy_maps_lib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->fantasy_maps_lib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->fantasy_maps_lib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->fantasy_maps_lib) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->fantasy_maps_lib) (3.1.4)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->fantasy_maps_lib) (1.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform->fantasy_maps_lib) (0.6.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->fantasy_maps_lib) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->fantasy_maps_lib) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->fantasy_maps_lib) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->fantasy_maps_lib) (0.1.2)\n",
            "Downloading black-24.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Building wheels for collected packages: fantasy_maps_lib\n",
            "  Building wheel for fantasy_maps_lib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fantasy_maps_lib: filename=fantasy_maps_lib-0.0.0-py3-none-any.whl size=16812 sha256=21a8b67fc39438b936f52ab9d064cd7ef5b96507f2cdfc9af8e259e43a33bdc3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rihuab96/wheels/95/85/a3/5ea513fa633d19614cc7ba22d8012afc8589ee6ebd9ddbf078\n",
            "Successfully built fantasy_maps_lib\n",
            "Installing collected packages: pathspec, mypy-extensions, jsonlines, black, fantasy_maps_lib\n",
            "Successfully installed black-24.8.0 fantasy_maps_lib-0.0.0 jsonlines-4.0.0 mypy-extensions-1.0.0 pathspec-0.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47b82e4a-6b1e-4efb-85f4-5eb2b64f8eaf",
      "metadata": {
        "id": "47b82e4a-6b1e-4efb-85f4-5eb2b64f8eaf"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "3. [Enable the Firestore API](https://console.cloud.google.com/flows/enableapi?apiid=firestore.googleapis.com).\n",
        "\n",
        "4. [Enable the Secret Manager API](https://console.cloud.google.com/flows/enableapi?apiid=secretmanager.googleapis.com).\n",
        "\n",
        "5. [Enable the Cloud Storage API](https://console.cloud.google.com/flows/enableapi?apiid=storage.googleapis.com).\n",
        "\n",
        "6. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "627dd1f8-9d0d-4c53-be6f-bc3a80d88cf2",
      "metadata": {
        "id": "627dd1f8-9d0d-4c53-be6f-bc3a80d88cf2"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If using Colab secrets, import them here\n",
        "from google.colab import userdata\n",
        "PROJECT_ID = userdata.get('PROJECT_ID')\n",
        "\n",
        "! gcloud config set project {PROJECT_ID}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJwbpxfnl9Qg",
        "outputId": "71287c65-4264-4e66-c874-fee2a4d6db1f"
      },
      "id": "kJwbpxfnl9Qg",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd84908-871b-4f8c-b586-3814514eac00",
      "metadata": {
        "id": "ddd84908-871b-4f8c-b586-3814514eac00",
        "outputId": "eb584c75-f5cc-4a3c-f3bc-ae95ff8a366a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "\u001b[1;33mWARNING:\u001b[0m You do not appear to have access to project [video-erschmid] or it does not exist.\n",
            "\n",
            "\n",
            "To take a quick anonymous survey, run:\n",
            "  $ gcloud survey\n",
            "\n"
          ]
        }
      ],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99eb88ba-b0fe-4989-99b7-33be554958a0",
      "metadata": {
        "id": "99eb88ba-b0fe-4989-99b7-33be554958a0"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6181b969-2184-4ea8-b42d-4f6377a91d5f",
      "metadata": {
        "id": "6181b969-2184-4ea8-b42d-4f6377a91d5f"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-west1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0455b922-0c10-4d2d-8861-3699c6ebd913",
      "metadata": {
        "id": "0455b922-0c10-4d2d-8861-3699c6ebd913"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46ba334b-e9b9-48fb-9ded-090bcf268cca",
      "metadata": {
        "id": "46ba334b-e9b9-48fb-9ded-090bcf268cca"
      },
      "source": [
        "**1. Vertex AI Workbench**\n",
        "* Do nothing as you are already authenticated."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1916caf9-48d1-4fe1-b42b-fd2fe2829200",
      "metadata": {
        "id": "1916caf9-48d1-4fe1-b42b-fd2fe2829200"
      },
      "source": [
        "**2. Local JupyterLab instance, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ac273d9-7f88-407e-9fe3-291bf2f0f5fb",
      "metadata": {
        "id": "6ac273d9-7f88-407e-9fe3-291bf2f0f5fb"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afd0c21b-fef6-44eb-8a8a-6a58140ff4f4",
      "metadata": {
        "id": "afd0c21b-fef6-44eb-8a8a-6a58140ff4f4"
      },
      "source": [
        "**3. Colab, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "57c6eb8f-f518-41a9-ae87-1558fe62eb8f",
      "metadata": {
        "id": "57c6eb8f-f518-41a9-ae87-1558fe62eb8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "1610e3af-fbef-4f1a-e9d8-3a39f8f2100f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-1f759c1655bd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/auth.py\u001b[0m in \u001b[0;36mauthenticate_user\u001b[0;34m(clear_output, project_id)\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_check_adc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CredentialType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUSER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_auth_ephem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m       _message.blocking_request(\n\u001b[0m\u001b[1;32m    261\u001b[0m           \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m           \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'auth_user_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c550d1b8-0e7c-4b85-9c01-8bb8906c2287",
      "metadata": {
        "id": "c550d1b8-0e7c-4b85-9c01-8bb8906c2287"
      },
      "source": [
        "**4. Service account or other**\n",
        "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "090dd331-0bd2-421b-bed5-8e8e4d7d8592",
      "metadata": {
        "id": "090dd331-0bd2-421b-bed5-8e8e4d7d8592"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9f2e97b-17d4-493d-a643-ef8670c95e1a",
      "metadata": {
        "id": "c9f2e97b-17d4-493d-a643-ef8670c95e1a"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ad6a6be-446c-440f-ae91-02066b13d34e",
      "metadata": {
        "id": "2ad6a6be-446c-440f-ae91-02066b13d34e"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77871323-0824-49c2-8c4c-fa0af8a67f4e",
      "metadata": {
        "id": "77871323-0824-49c2-8c4c-fa0af8a67f4e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efcd60f6-6872-48b1-8a64-a47f14bf3064",
      "metadata": {
        "id": "efcd60f6-6872-48b1-8a64-a47f14bf3064",
        "tags": []
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34174e56-8d65-4e5a-9046-d74625090cc5",
      "metadata": {
        "id": "34174e56-8d65-4e5a-9046-d74625090cc5"
      },
      "outputs": [],
      "source": [
        "from google.cloud import firestore\n",
        "from google.cloud import storage\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import hashlib\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import praw\n",
        "import re\n",
        "import requests\n",
        "import shutil\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8a0f2bb-849a-4ab1-a48c-adeca3906509",
      "metadata": {
        "id": "a8a0f2bb-849a-4ab1-a48c-adeca3906509"
      },
      "outputs": [],
      "source": [
        "import extract"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "854f52c5-8e0e-4a46-af45-86914c745f7f",
      "metadata": {
        "id": "854f52c5-8e0e-4a46-af45-86914c745f7f"
      },
      "source": [
        "### Get a Reddit API key\n",
        "\n",
        "You need a [Reddit API key](https://www.reddit.com/wiki/api/) to access Reddit programmatically. Copy your API key into the labeled fields of the dictionary in the following cell.\n",
        "\n",
        "**Note**: Once you have an API key, you must store it somewhere safe. It is recommended to store your API key as a JSON-formatted string in Cloud Secret Manager."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5683274-79c5-43fd-8065-9149626229e4",
      "metadata": {
        "id": "b5683274-79c5-43fd-8065-9149626229e4"
      },
      "outputs": [],
      "source": [
        "reddit_key_json = {\n",
        "    \"secret\": \"YOUR_SECRET\",\n",
        "    \"client_id\": \"YOUR_CLIENT_ID\",\n",
        "    \"user_agent\": \"YOUR_USER_AGENT\",\n",
        "    \"user_name\": \"YOUR_REDDIT_USER_NAME\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "998ab6f9-94f3-48aa-be1c-340a2c6da790",
      "metadata": {
        "id": "998ab6f9-94f3-48aa-be1c-340a2c6da790"
      },
      "source": [
        "## TODO: DELETE THIS SECTION BEFORE MERGING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32ab4c08-aab4-46dd-a73a-de5901c8f694",
      "metadata": {
        "id": "32ab4c08-aab4-46dd-a73a-de5901c8f694"
      },
      "outputs": [],
      "source": [
        "## TODO: Delete this cell before merging\n",
        "PROJECT_ID = \"video-erschmid\"\n",
        "REGION = \"us-central1\"\n",
        "BUCKET_NAME = \"video-erschmid\"\n",
        "reddit_key_json = {\n",
        "    \"secret\": \"_XDRI2jgcVAJ6xKIWmA46yz8CZw\",\n",
        "    \"client_id\": \"Z0g7xbmKNB9Mew\",\n",
        "    \"user_agent\": \"script:ScrapeForNLP:v1.0 (by u/Telpirion-78)\",\n",
        "    \"user_name\": \"Telpirion-78\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67219889-78bf-429a-b772-2f95a9904261",
      "metadata": {
        "id": "67219889-78bf-429a-b772-2f95a9904261"
      },
      "source": [
        "## Query data (posts) on Reddit\n",
        "\n",
        "Now that we have our API key ready for use, we can query Reddit for our data! In the next cell, we will read the top 100 \"hot\" posts from a subreddit.\n",
        "\n",
        "For our use-case, we want to check the posts to see whether: 1) they have an image associated with them; and 2) the title gives us some clues as to the contents (e.g. columns and rows) contained in the image.\n",
        "\n",
        "To make data visualization, we're going to store the data from Reddit in a `pandas` dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dc26f93-3bcb-4737-9b92-a798938ede19",
      "metadata": {
        "id": "9dc26f93-3bcb-4737-9b92-a798938ede19"
      },
      "outputs": [],
      "source": [
        "columns = ['Title', 'Post', 'ID', 'URL']\n",
        "\n",
        "subreddit_name = \"battlemaps\"\n",
        "posts = extract.get_reddit_posts(reddit_credentials=reddit_key_json,\n",
        "                                subreddit_name=subreddit_name, limit=100)\n",
        "reddit_df = extract.convert_posts_to_dataframe(posts, columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dbd67c7-d1e8-404f-89d9-f2540ea7ace9",
      "metadata": {
        "id": "4dbd67c7-d1e8-404f-89d9-f2540ea7ace9"
      },
      "source": [
        "Now that we have the top 100 \"hot\" posts from the subreddit, we're going to filter for only the posts that we want. Again, our criteria are: 1) must have an image; 2) the title must have the grid dimensions of the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d6f6764",
      "metadata": {
        "tags": [],
        "id": "0d6f6764",
        "outputId": "d67e9141-c103-48ae-f5e6-325261aede34"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Post</th>\n",
              "      <th>ID</th>\n",
              "      <th>URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>City ambush [32x44]</td>\n",
              "      <td></td>\n",
              "      <td>14ko28d</td>\n",
              "      <td>https://i.redd.it/pepgw7e1bm8b1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Old Watermill - Battle Map (30x45)</td>\n",
              "      <td></td>\n",
              "      <td>14kb71a</td>\n",
              "      <td>https://i.redd.it/0y3sfrkiqj8b1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[OC] The Inn Beyond Worlds - [50x50]</td>\n",
              "      <td></td>\n",
              "      <td>14kg2c0</td>\n",
              "      <td>https://i.redd.it/ks1wup6drk8b1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Hidden Cottage in the Forest [18x24]</td>\n",
              "      <td></td>\n",
              "      <td>14kn4cb</td>\n",
              "      <td>https://i.redd.it/5m885ua64m8b1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Temple Services Shop [17x11]</td>\n",
              "      <td></td>\n",
              "      <td>14kfl22</td>\n",
              "      <td>https://i.redd.it/myba44zhok8b1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>New Czepeku Sci-Fi map! Cyborg Cube Boss Room ...</td>\n",
              "      <td></td>\n",
              "      <td>14kee2p</td>\n",
              "      <td>https://i.redd.it/r8cxkhnxfk8b1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>City Streets [50x47]</td>\n",
              "      <td></td>\n",
              "      <td>14jk9uv</td>\n",
              "      <td>https://i.redd.it/rrmnm320rd8b1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Mountain hut (30x30)</td>\n",
              "      <td></td>\n",
              "      <td>14kdg4a</td>\n",
              "      <td>https://i.redd.it/bv245vur8k8b1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Ship \"The Unwanted Daughter\" Main Deck | [Anim...</td>\n",
              "      <td></td>\n",
              "      <td>14kavb1</td>\n",
              "      <td>https://i.redd.it/i8k5mjs3oj8b1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Arachne's Underground Prison [39x50]</td>\n",
              "      <td></td>\n",
              "      <td>14jowiy</td>\n",
              "      <td>https://i.redd.it/xg38dx8nme8b1.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Title Post       ID  \\\n",
              "2                                 City ambush [32x44]       14ko28d   \n",
              "3                  Old Watermill - Battle Map (30x45)       14kb71a   \n",
              "6                [OC] The Inn Beyond Worlds - [50x50]       14kg2c0   \n",
              "10               Hidden Cottage in the Forest [18x24]       14kn4cb   \n",
              "11                       Temple Services Shop [17x11]       14kfl22   \n",
              "12  New Czepeku Sci-Fi map! Cyborg Cube Boss Room ...       14kee2p   \n",
              "20                               City Streets [50x47]       14jk9uv   \n",
              "22                               Mountain hut (30x30)       14kdg4a   \n",
              "23  Ship \"The Unwanted Daughter\" Main Deck | [Anim...       14kavb1   \n",
              "27               Arachne's Underground Prison [39x50]       14jowiy   \n",
              "\n",
              "                                    URL  \n",
              "2   https://i.redd.it/pepgw7e1bm8b1.jpg  \n",
              "3   https://i.redd.it/0y3sfrkiqj8b1.jpg  \n",
              "6   https://i.redd.it/ks1wup6drk8b1.jpg  \n",
              "10  https://i.redd.it/5m885ua64m8b1.jpg  \n",
              "11  https://i.redd.it/myba44zhok8b1.jpg  \n",
              "12  https://i.redd.it/r8cxkhnxfk8b1.jpg  \n",
              "20  https://i.redd.it/rrmnm320rd8b1.jpg  \n",
              "22  https://i.redd.it/bv245vur8k8b1.jpg  \n",
              "23  https://i.redd.it/i8k5mjs3oj8b1.jpg  \n",
              "27  https://i.redd.it/xg38dx8nme8b1.jpg  "
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "jpg_df = reddit_df[(reddit_df[\"URL\"].str.contains(\"jpg\")) &\n",
        "                         (reddit_df[\"Title\"].str.contains(pat = \"\\d+x\\d\"))]\n",
        "\n",
        "jpg_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25212aba-89e7-4e7d-95f3-4ba5beecd476",
      "metadata": {
        "id": "25212aba-89e7-4e7d-95f3-4ba5beecd476"
      },
      "source": [
        "## Process the images and their metadata\n",
        "\n",
        "In this next step, we will process all of the Reddit posts with images posts:\n",
        "\n",
        "1. Download the image itself\n",
        "2. Parse each image's metadata\n",
        "3. (If needed) Split the image into smaller images\n",
        "4. Store the metadata in Firestore\n",
        "5. Save the images on Cloud Storage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7326eee-5514-4521-938c-3637ff3dcb3c",
      "metadata": {
        "id": "e7326eee-5514-4521-938c-3637ff3dcb3c"
      },
      "source": [
        "### Download the image\n",
        "\n",
        "First we're going to download the image locally. We'll need a meaningful filename to save the image.\n",
        "\n",
        "We also want to avoid downloading the same image more than once. We'll need to compare the images programmatically to verify that each image is unique.\n",
        "\n",
        "The easiest way to do this will be to reduce each image to a unique hash value and then ensure that we never have two copies of the same hash value. For the sake of simplicity, we'll use these hash values as the unique ID for each image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3844e20",
      "metadata": {
        "id": "f3844e20",
        "outputId": "956420b2-f4bb-4618-ec92-503b6f818adb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Post</th>\n",
              "      <th>ID</th>\n",
              "      <th>URL</th>\n",
              "      <th>Path</th>\n",
              "      <th>UID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>City ambush [32x44]</td>\n",
              "      <td></td>\n",
              "      <td>14ko28d</td>\n",
              "      <td>https://i.redd.it/pepgw7e1bm8b1.jpg</td>\n",
              "      <td>reddit_maps_data/city_ambush.32x44.jpg</td>\n",
              "      <td>cac2561ed36dc03bb6df5fc6c696454e56587a4a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Old Watermill - Battle Map (30x45)</td>\n",
              "      <td></td>\n",
              "      <td>14kb71a</td>\n",
              "      <td>https://i.redd.it/0y3sfrkiqj8b1.jpg</td>\n",
              "      <td>reddit_maps_data/old_watermill_battle_map.30x4...</td>\n",
              "      <td>7d328d43374fb2c33a15bd9221c3a51dfad8a229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>[OC] The Inn Beyond Worlds - [50x50]</td>\n",
              "      <td></td>\n",
              "      <td>14kg2c0</td>\n",
              "      <td>https://i.redd.it/ks1wup6drk8b1.jpg</td>\n",
              "      <td>reddit_maps_data/inn_worlds.50x50.jpg</td>\n",
              "      <td>77d64045f15b521ff987e13e1ed28af4bf2c1e0e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Hidden Cottage in the Forest [18x24]</td>\n",
              "      <td></td>\n",
              "      <td>14kn4cb</td>\n",
              "      <td>https://i.redd.it/5m885ua64m8b1.jpg</td>\n",
              "      <td>reddit_maps_data/hidden_cottage_forest.18x24.jpg</td>\n",
              "      <td>cb6aa134e623ad7f47071d4e399d66edc5b7086e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Temple Services Shop [17x11]</td>\n",
              "      <td></td>\n",
              "      <td>14kfl22</td>\n",
              "      <td>https://i.redd.it/myba44zhok8b1.jpg</td>\n",
              "      <td>reddit_maps_data/temple_services_shop.17x11.jpg</td>\n",
              "      <td>48f768d8f46ebb4f04defd5bb5c11ed377a916ca</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>New Czepeku Sci-Fi map! Cyborg Cube Boss Room ...</td>\n",
              "      <td></td>\n",
              "      <td>14kee2p</td>\n",
              "      <td>https://i.redd.it/r8cxkhnxfk8b1.jpg</td>\n",
              "      <td>reddit_maps_data/new_czepeku_sci_fi_map_cyborg...</td>\n",
              "      <td>6169a90d218c39feddd6f5df998a30306209397f</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>City Streets [50x47]</td>\n",
              "      <td></td>\n",
              "      <td>14jk9uv</td>\n",
              "      <td>https://i.redd.it/rrmnm320rd8b1.jpg</td>\n",
              "      <td>reddit_maps_data/city_streets.50x47.jpg</td>\n",
              "      <td>d023cd99c9d9ebcc19bf0058c2f7902c266db7a7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Mountain hut (30x30)</td>\n",
              "      <td></td>\n",
              "      <td>14kdg4a</td>\n",
              "      <td>https://i.redd.it/bv245vur8k8b1.jpg</td>\n",
              "      <td>reddit_maps_data/mountain_hut.30x30.jpg</td>\n",
              "      <td>0af68834a25314a844cb4ed8174b2beafd4813c9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Ship \"The Unwanted Daughter\" Main Deck | [Anim...</td>\n",
              "      <td></td>\n",
              "      <td>14kavb1</td>\n",
              "      <td>https://i.redd.it/i8k5mjs3oj8b1.jpg</td>\n",
              "      <td>reddit_maps_data/ship_unwanted_daughter_main_d...</td>\n",
              "      <td>1234bf6d8f9d1b1c9f2e03038e3f329a05f01585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Arachne's Underground Prison [39x50]</td>\n",
              "      <td></td>\n",
              "      <td>14jowiy</td>\n",
              "      <td>https://i.redd.it/xg38dx8nme8b1.jpg</td>\n",
              "      <td>reddit_maps_data/arachne_underground_prison.39...</td>\n",
              "      <td>ffe6a33fe84fd2848de1951226074f317cef9a49</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Title Post       ID  \\\n",
              "2                                 City ambush [32x44]       14ko28d   \n",
              "3                  Old Watermill - Battle Map (30x45)       14kb71a   \n",
              "6                [OC] The Inn Beyond Worlds - [50x50]       14kg2c0   \n",
              "10               Hidden Cottage in the Forest [18x24]       14kn4cb   \n",
              "11                       Temple Services Shop [17x11]       14kfl22   \n",
              "12  New Czepeku Sci-Fi map! Cyborg Cube Boss Room ...       14kee2p   \n",
              "20                               City Streets [50x47]       14jk9uv   \n",
              "22                               Mountain hut (30x30)       14kdg4a   \n",
              "23  Ship \"The Unwanted Daughter\" Main Deck | [Anim...       14kavb1   \n",
              "27               Arachne's Underground Prison [39x50]       14jowiy   \n",
              "\n",
              "                                    URL  \\\n",
              "2   https://i.redd.it/pepgw7e1bm8b1.jpg   \n",
              "3   https://i.redd.it/0y3sfrkiqj8b1.jpg   \n",
              "6   https://i.redd.it/ks1wup6drk8b1.jpg   \n",
              "10  https://i.redd.it/5m885ua64m8b1.jpg   \n",
              "11  https://i.redd.it/myba44zhok8b1.jpg   \n",
              "12  https://i.redd.it/r8cxkhnxfk8b1.jpg   \n",
              "20  https://i.redd.it/rrmnm320rd8b1.jpg   \n",
              "22  https://i.redd.it/bv245vur8k8b1.jpg   \n",
              "23  https://i.redd.it/i8k5mjs3oj8b1.jpg   \n",
              "27  https://i.redd.it/xg38dx8nme8b1.jpg   \n",
              "\n",
              "                                                 Path  \\\n",
              "2              reddit_maps_data/city_ambush.32x44.jpg   \n",
              "3   reddit_maps_data/old_watermill_battle_map.30x4...   \n",
              "6               reddit_maps_data/inn_worlds.50x50.jpg   \n",
              "10   reddit_maps_data/hidden_cottage_forest.18x24.jpg   \n",
              "11    reddit_maps_data/temple_services_shop.17x11.jpg   \n",
              "12  reddit_maps_data/new_czepeku_sci_fi_map_cyborg...   \n",
              "20            reddit_maps_data/city_streets.50x47.jpg   \n",
              "22            reddit_maps_data/mountain_hut.30x30.jpg   \n",
              "23  reddit_maps_data/ship_unwanted_daughter_main_d...   \n",
              "27  reddit_maps_data/arachne_underground_prison.39...   \n",
              "\n",
              "                                         UID  \n",
              "2   cac2561ed36dc03bb6df5fc6c696454e56587a4a  \n",
              "3   7d328d43374fb2c33a15bd9221c3a51dfad8a229  \n",
              "6   77d64045f15b521ff987e13e1ed28af4bf2c1e0e  \n",
              "10  cb6aa134e623ad7f47071d4e399d66edc5b7086e  \n",
              "11  48f768d8f46ebb4f04defd5bb5c11ed377a916ca  \n",
              "12  6169a90d218c39feddd6f5df998a30306209397f  \n",
              "20  d023cd99c9d9ebcc19bf0058c2f7902c266db7a7  \n",
              "22  0af68834a25314a844cb4ed8174b2beafd4813c9  \n",
              "23  1234bf6d8f9d1b1c9f2e03038e3f329a05f01585  \n",
              "27  ffe6a33fe84fd2848de1951226074f317cef9a49  "
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "local_reddit_data_dir = \"reddit_maps_data\"\n",
        "\n",
        "if not os.path.exists(local_reddit_data_dir):\n",
        "    os.mkdir(local_reddit_data_dir)\n",
        "\n",
        "paths = []\n",
        "hashes = []\n",
        "\n",
        "for _, row, in jpg_df.head(50).iterrows():\n",
        "    url = row[\"URL\"]\n",
        "    filename = extract.make_nice_filename(row[\"Title\"])\n",
        "    path = f\"{local_reddit_data_dir}/{filename}\"\n",
        "    is_downloaded = extract.download_image_local(url, path, hashes)\n",
        "\n",
        "    if is_downloaded:\n",
        "        paths.append(path)\n",
        "    else:\n",
        "        paths.append(\"\")\n",
        "\n",
        "jpg_df = jpg_df.assign(Path=paths, UID=hashes)\n",
        "jpg_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73d6641d-c090-47f1-9a2c-6da98f183299",
      "metadata": {
        "id": "73d6641d-c090-47f1-9a2c-6da98f183299"
      },
      "source": [
        "### Parse the image for metadata\n",
        "\n",
        "All we know from the Reddit API is that these posts have JPGs associated with them and that they contain a substring in the format \"NNxNN.\" However, we need more than just images and rough columns and rows for training a Vertex AI AutoML image object detection model. We would even need more data just to use these images in a VTT app.\n",
        "\n",
        "To get valid training data and VTT data, we need to make some inferences about the images based upon the data that we have (or can get). The data we have are the number of columns and rows stated in the post's title (granted, these are not always accurate). The data we can get is the image's width and height. From these four data points, and assuming that they are accurate and that all cells in the map are uniform, we can infer the width and height of cells in the map.\n",
        "\n",
        "Using the cell width and height, we can compute the rest of the data required for both an image object detection model and the data needed for a VTT app."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dc0a53b-311a-4837-9213-b4049da61e4c",
      "metadata": {
        "id": "6dc0a53b-311a-4837-9213-b4049da61e4c"
      },
      "source": [
        "#### Virtual tabletop (VTT) data\n",
        "\n",
        "The easiest for us to compute is the VTT data. This data provides us with the `cellWidth` and `cellHeight` data that will allow us to complete the ML training data.\n",
        "\n",
        "The JSON structure of VTT data is:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"imageHeight\": ##,\n",
        "    \"imageWidth\": ##,\n",
        "    \"cellHeight\": ##,\n",
        "    \"cellWidth\": ##,\n",
        "    \"cellOffsetY\": ##,\n",
        "    \"cellOffsetX\": ##\n",
        "}\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd38762b-ea37-4c1a-99b8-2b3022dd4f27",
      "metadata": {
        "id": "cd38762b-ea37-4c1a-99b8-2b3022dd4f27"
      },
      "source": [
        "#### Image object detection training data\n",
        "\n",
        "AutoML image object detection on Vertex AI requires a JSONL file with information for the training data. Each line in the JSONL file needs to contain: the Cloud Storage URI of the image; and the bounding boxes of the objects (cells) that we want to train the model to identify on the image.\n",
        "\n",
        "The structure of the JSON data in the JSONL file is:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"imageGcsUri\": \"URI\",\n",
        "    \"boundingBoxAnnotations\": {\n",
        "        \"displayName\": \"LABEL_NAME\",\n",
        "        \"xMin\": ##,\n",
        "        \"xMax\": ##,\n",
        "        \"yMin\": ##,\n",
        "        \"yMax\": ##,\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "For each bounding box, we need to provide a percentage value that expresses the vertices of the bounding box as a set of x and y pairs. Also, each bounding box needs to be given a label for that bounding box; all of our bounding boxes are \"cells\" so each one gets the label `cell`.\n",
        "\n",
        "**Note**: A training image in Vertex AI can only have at most 500 bounding boxes; many fantasy maps have many more than 500 cells. So that we can use the most of the training data, we will split too large images until smaller images, or \"shards\", and use the shards  for training.\n",
        "\n",
        "You can read more about how to format your training manifest JSONL file in [the documentation](https://cloud.google.com/vertex-ai/docs/datasets/prepare-image#object-detection)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83951a67-7bf5-403c-9e68-eebc5eedfefb",
      "metadata": {
        "id": "83951a67-7bf5-403c-9e68-eebc5eedfefb"
      },
      "source": [
        "With all of the helper functions in place, we can start processing each image to extract the VTT and image object detection data. We'll need a datastructure to store all of the newly computed data--luckily we can simply append this new data to each `Series` object in the `jpg_df` pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a569b0c0-59ef-4fe5-9262-d08a0150f6a6",
      "metadata": {
        "tags": [],
        "id": "a569b0c0-59ef-4fe5-9262-d08a0150f6a6",
        "outputId": "a6caba15-e014-4d8c-91c0-81c65ae135b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title                                    City ambush [32x44]\n",
            "Post                                                        \n",
            "ID                                                   14ko28d\n",
            "URL                      https://i.redd.it/pepgw7e1bm8b1.jpg\n",
            "Path                  reddit_maps_data/city_ambush.32x44.jpg\n",
            "UID                 cac2561ed36dc03bb6df5fc6c696454e56587a4a\n",
            "Width                                                 3072.0\n",
            "Height                                                4224.0\n",
            "Columns                                                 32.0\n",
            "Rows                                                    44.0\n",
            "VTT        {\"cellsOffsetX\": 0, \"cellsOffsetY\": 0, \"imageW...\n",
            "Name: 2, dtype: object\n",
            "5\n",
            "reddit_maps_data/city_ambush.32x44.jpg\n",
            "(0, 0, 2112, 2112, 22, 22)\n",
            "reddit_maps_data/city_ambush.32x44.jpg\n",
            "(0, 2112, 2112, 4224, 22, 22)\n",
            "reddit_maps_data/city_ambush.32x44.jpg\n",
            "(2112, 0, 3072, 2112, 10, 22)\n",
            "reddit_maps_data/city_ambush.32x44.jpg\n",
            "(2112, 2112, 3072, 4224, 10, 22)\n",
            "reddit_maps_data/city_ambush.32x44.jpg\n",
            "(0, 4224, 2112, 4224, 22, 0)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "cannot write empty image as JPEG",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_17560/620112110.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m         shard_df = extract.create_shard(x_min=shard[0], y_min=shard[1], x_max=shard[2],\n\u001b[1;32m     46\u001b[0m                                    \u001b[0my_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                                    img_path=local_path, parent_id=row[\"UID\"])\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshard_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/FantasyMaps/notebooks/final/extract.py\u001b[0m in \u001b[0;36mcreate_shard\u001b[0;34m(x_min, y_min, x_max, y_max, cols, rows, img_path, parent_id)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mconvert_image_to_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0mshard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         d = {\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2431\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2432\u001b[0;31m             \u001b[0msave_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2433\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2434\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mopen_fp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/JpegImagePlugin.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(im, fp, filename)\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cannot write empty image as JPEG\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot write empty image as JPEG"
          ]
        }
      ],
      "source": [
        "shards_df = pd.DataFrame()\n",
        "\n",
        "for i, row in jpg_df.iterrows():\n",
        "    local_path = row[\"Path\"]\n",
        "\n",
        "    # Get width & height for original image\n",
        "    w, h = extract.get_image_width_and_height(local_path)\n",
        "\n",
        "    jpg_df.at[i, \"Width\"] = w\n",
        "    jpg_df.at[i, \"Height\"] = h\n",
        "\n",
        "    # Get columns & rows for original image, based upon the name.\n",
        "    paths = local_path.split(\".\")\n",
        "    dims = paths[-2]\n",
        "    cols, rows = dims.split(\"x\")\n",
        "\n",
        "    cols = int(cols)\n",
        "    rows = int(rows)\n",
        "\n",
        "    jpg_df.at[i, \"Columns\"] = cols\n",
        "    jpg_df.at[i, \"Rows\"] = rows\n",
        "\n",
        "    # Compute the vtt data for the image\n",
        "    vtt = extract.compute_vtt_data(width=w, height=h, columns=cols, rows=rows)\n",
        "\n",
        "    # Note: pandas has issues storing a dict in a cell\n",
        "    jpg_df.at[i, \"VTT\"] = json.dumps(vtt)\n",
        "\n",
        "    # If image doesn't need to be sharded, simply compute and continue\n",
        "    if (cols * rows) <= 500:\n",
        "        bboxes = extract.compute_bboxes(series=row,\n",
        "                                cell_width=vtt[\"cellWidth\"],\n",
        "                                cell_height=vtt[\"cellHeight\"])\n",
        "        jpg_df.at[i, \"BBoxes\"] = json.dumps({ \"bboxes\": bboxes })\n",
        "        continue\n",
        "\n",
        "    # Compute the number of shards\n",
        "    shards = extract.compute_shard_coordinates(width=w, height=h, columns=cols, rows=rows,\n",
        "                                       cell_width=vtt[\"cellWidth\"], cell_height=vtt[\"cellHeight\"])\n",
        "    print(row)\n",
        "    print(len(shards))\n",
        "    for shard in shards:\n",
        "        print(local_path)\n",
        "        print(shard)\n",
        "        shard_df = extract.create_shard(x_min=shard[0], y_min=shard[1], x_max=shard[2],\n",
        "                                   y_max=shard[3], cols=shard[4], rows=shard[5],\n",
        "                                   img_path=local_path, parent_id=row[\"UID\"])\n",
        "\n",
        "        if shard_df is None:\n",
        "            continue\n",
        "\n",
        "        s_vtt = vtt\n",
        "        s_vtt[\"width\"] = int(shard_df.iloc[0][\"Width\"])\n",
        "        s_vtt[\"height\"] = int(shard_df.iloc[0][\"Height\"])\n",
        "        shard_df.at[0, \"VTT\"] = json.dumps(s_vtt)\n",
        "\n",
        "        bboxes = extract.compute_bboxes(dataframe=shard_df,\n",
        "                                cell_width=vtt[\"cellWidth\"],\n",
        "                                cell_height=vtt[\"cellHeight\"])\n",
        "\n",
        "        shard_df.at[0, \"BBoxes\"] = json.dumps({ \"bboxes\": bboxes })\n",
        "        shards_df = pd.concat([shards_df, shard_df])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b85519e0-e5d7-4d04-9d14-d8a79df43984",
      "metadata": {
        "id": "b85519e0-e5d7-4d04-9d14-d8a79df43984"
      },
      "source": [
        "Finally, now that we have all of the VTT and image object detection metadata computed for the original images and/or shards, we can join the two dataframes together into one. As part of this process, we also want to reindex the resulting dataframe so that it uses the UIDs we calculated instead of the automatically generated indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4d21b1c-95ec-40e4-89d9-011a06684e33",
      "metadata": {
        "id": "d4d21b1c-95ec-40e4-89d9-011a06684e33"
      },
      "outputs": [],
      "source": [
        "complete_df = pd.concat([jpg_df, shards_df])\n",
        "complete_df.set_index(\"UID\", inplace=True)\n",
        "complete_df.fillna(\"\", inplace=True)\n",
        "complete_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fb38256-b45c-4e32-a032-5c8bee3b2e0d",
      "metadata": {
        "id": "0fb38256-b45c-4e32-a032-5c8bee3b2e0d"
      },
      "source": [
        "## Store the images in Google Cloud Storage\n",
        "\n",
        "Now that we've created the image shards, we can begin uploading the images to Google Cloud Storage. We'll need to have a Storage bucket already created for this next cell of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "077f5105-81e0-4b77-aa8d-3c8bec170a39",
      "metadata": {
        "id": "077f5105-81e0-4b77-aa8d-3c8bec170a39"
      },
      "outputs": [],
      "source": [
        "for i, row in complete_df.iterrows():\n",
        "    gcs_uri = store_image_gcs(project_id=PROJECT_ID, series=row,\n",
        "                                bucket_name=BUCKET, prefix=\"FantasyMapsTest\")\n",
        "    complete_df.at[i, \"GCS URI\"] = gcs_uri"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb433af9-3674-4483-91fc-930ff6e71903",
      "metadata": {
        "id": "bb433af9-3674-4483-91fc-930ff6e71903"
      },
      "source": [
        "## Store the metadata in Firestore\n",
        "\n",
        "Next we're going to store all of this metadata and URI in Firestore. The benefit of using Firestore is that the fields with JSON-formatted strings-`VTT` and `BBoxes` will automatically be translated into the correct document structure in Firestore after they've been upserted."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35825de1-8209-48ed-9f09-472f97da92d3",
      "metadata": {
        "id": "35825de1-8209-48ed-9f09-472f97da92d3"
      },
      "source": [
        "Very, very last step: iterate over all the training data and store the metadata in the Firestore collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7968e57-2a80-45cf-bd38-495c273be909",
      "metadata": {
        "id": "c7968e57-2a80-45cf-bd38-495c273be909"
      },
      "outputs": [],
      "source": [
        "for uid, row in complete_df.iterrows():\n",
        "    store_metadata_fs(project_id=PROJECT_ID, series=row,\n",
        "                      collection_name=COLLECTION_NAME, uid=uid)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99950d94-57cc-47c7-85d4-093593f7c972",
      "metadata": {
        "id": "99950d94-57cc-47c7-85d4-093593f7c972"
      },
      "source": [
        "## Check the results of the metadata creation\n",
        "\n",
        "Now that (hopefully) all of the image metadata has been added to the Firestore collection, we can review the data to ensure that it is correct.\n",
        "\n",
        "To do this, we'll review the documents stored in the Firestore collection to verify that it has all the data we need--the VTT data, bounding boxes, and the GCS URI of the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c6764f-3f31-40bb-8361-3ce6bd17c9bb",
      "metadata": {
        "id": "23c6764f-3f31-40bb-8361-3ce6bd17c9bb"
      },
      "outputs": [],
      "source": [
        "client = firestore.Client(project=PROJECT_ID)\n",
        "collection = client.collection(COLLECTION_NAME)\n",
        "\n",
        "docs = collection.where(\"BBoxes\", \"!=\", \"\").select(field_paths=[\"gcsURI\", \"filename\", \"VTT\", \"Parent\"]).stream()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cee72d1-da57-4383-aab8-7b236302e556",
      "metadata": {
        "id": "4cee72d1-da57-4383-aab8-7b236302e556"
      },
      "source": [
        "With this Firestore query, we can verify the image metadata against the stored image in the Storage bucket. We'll first take the results of this query, compose it into a `pandas.DataFrame` object, and then print it out to the cell output. We can first take a look at the parent map (assuming that the map has been sharded) and then conclude whether the map and all its shards should be removed from the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e31b72c5-ad31-42df-8808-2f6239eb4618",
      "metadata": {
        "id": "e31b72c5-ad31-42df-8808-2f6239eb4618"
      },
      "outputs": [],
      "source": [
        "docs_list = ((d.to_dict(), d.id) for d in docs)\n",
        "docs_df = pd.DataFrame()\n",
        "for i, d in enumerate(docs_list):\n",
        "    d_dict = d[0]\n",
        "    vtt = d_dict[\"VTT\"]\n",
        "    d_dict[\"VTT\"] = json.dumps(vtt)\n",
        "    d_dict[\"UID\"] = d[1]\n",
        "    docs_df = pd.concat([docs_df, pd.DataFrame(data=d_dict, index=[0])], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df91b96-0a22-4a11-b3e0-80d05feff74f",
      "metadata": {
        "id": "8df91b96-0a22-4a11-b3e0-80d05feff74f"
      },
      "outputs": [],
      "source": [
        "docs_df.set_index(\"UID\", inplace=True)\n",
        "check_set_df = docs_df[[\"filename\", \"gcsURI\", \"Parent\"]]\n",
        "check_set_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93a96aef-5fdd-427f-8f0d-aeec42d99cb4",
      "metadata": {
        "id": "93a96aef-5fdd-427f-8f0d-aeec42d99cb4"
      },
      "source": [
        "Not everyone on Reddit follows the same conventions. Sometimes, there might be be a post where there are dimensions mentioned in the post (e.g. \"50x40\"), but the image doesn't actually have gridlines.\n",
        "\n",
        "We shouldn't allow these images into the training and test dataset for our model. Unfortunately, we have to review the images that we've collected on GCS and then verify that they do (or don't!) have gridlines visually.\n",
        "\n",
        "\n",
        "We'll start by printing out the entirety of our `DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81703224-47d9-4d86-95c7-928241e2461f",
      "metadata": {
        "id": "81703224-47d9-4d86-95c7-928241e2461f"
      },
      "outputs": [],
      "source": [
        "pd.set_option(\"display.max_rows\", 1000)\n",
        "check_set_df.sort_values(by=\"filename\", ascending=True, inplace=True)\n",
        "display(check_set_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90c45f1c-16f1-491c-975c-ccc660eacc08",
      "metadata": {
        "id": "90c45f1c-16f1-491c-975c-ccc660eacc08"
      },
      "source": [
        "This final step of data prepartion is to mark all of the unusable images in the Firestore collection. Luckily, we can use the Google Cloud Console to view the contents of our Storage bucket. We can even add new fields to the documents in our Firestore collection!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ba41c11-df6b-44de-a592-8ac1d7ece4a9",
      "metadata": {
        "id": "6ba41c11-df6b-44de-a592-8ac1d7ece4a9"
      },
      "source": [
        "![Storage user interface in the Cloud Console](https://github.com/telpirion/FantasyMaps/blob/main/notebooks/final/resources/StorageUI.png?raw=1)\n",
        "_Figure. The Google Cloud Storage user interface, showing images in a bucket._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db9d401f-1b87-486a-9def-96a8f36ed422",
      "metadata": {
        "id": "db9d401f-1b87-486a-9def-96a8f36ed422"
      },
      "source": [
        "![Firestore user interface in the Cloud Console](https://github.com/telpirion/FantasyMaps/blob/main/notebooks/final/resources/FirestoreUI.png?raw=1)\n",
        "_Figure. The Cloud Firestore user interface, showing a \"Usable\" field being added to a document._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "951d7636-b8fb-472b-9a05-043aa0a92c82",
      "metadata": {
        "id": "951d7636-b8fb-472b-9a05-043aa0a92c82"
      },
      "source": [
        "For this very last data preparation step, we will visually inspect all of the \"parent\" images in the Cloud Storage bucket. We will then create a list unusable images, where we store the image's UID. Finally, we will do a bulk update job to Firestore, setting a `Usable` field on the images to `False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc0f69b4-4f49-4b2b-8abc-ba14571f0ee9",
      "metadata": {
        "id": "fc0f69b4-4f49-4b2b-8abc-ba14571f0ee9"
      },
      "outputs": [],
      "source": [
        "unusable_parent_images = [\n",
        "    \"d3ee0039aeff5c33de778c5adbdd000f21c0b4cd\",\n",
        "    \"9a7e82433239b0087121f6fd31e133f5a94fa7dc\",\n",
        "    \"fc70f330cd5a3aaa3be94e0d603dab2876f9fca1\",\n",
        "    \"890c0d27318b0286aebf67c392fab286bdc4e7c5\",\n",
        "    \"2f4f496466d6e9fb8ad9791ccb81f7c13fd407db\",\n",
        "    \"4c0d3eb86ed599f496f7e30e18025021aebfe153\",\n",
        "    \"4bf88b8acc669331a65465e8c4b37fd8b9495e4b\",\n",
        "    \"b15fe8185c30b3e7800e42e280a3792998a0b55f\",\n",
        "    \"5cd7bbd5882fe8b9a270be1aa911c0ba858e818d\",\n",
        "    \"67925948371de53b58c09b32c97af60f72c58e0f\",\n",
        "    \"a816b6eb31b71cad5b50531d6e18ef46cb451cbd\",\n",
        "    \"f00be2ed7b19b8d0f44915a1886437193aa224aa\",\n",
        "    \"890c0d27318b0286aebf67c392fab286bdc4e7c5\",\n",
        "    # Others ....\n",
        "]\n",
        "\n",
        "usable_parent_images = [\n",
        "    \"7a903be0bad0fc00bfabbefd682b6eef23263b67\",\n",
        "    \"b1fe854b9c658cf9312319a447b594743513499b\",\n",
        "    \"3a52781685d7aed530a685739b58341fafd2e721\",\n",
        "    \"3d17d612a843af7a5ad1a2b2d5dcce29e67d367f\",\n",
        "    \"aa33a7ed5c3c87147fd25dafe6c0a1d3eb29dbe5\",\n",
        "    \"2ce9f80408137e36531581fb22ee3fe892f41f76\",\n",
        "    \"e086c1cf420e27448bc1a45147b5c43df4b3d8d0\",\n",
        "    \"5bc994d7d38c1fc4654c58666d674200f731986b\",\n",
        "    \"820c3bbfe4d14694ecb729ce3f45b4bda031f61a\",\n",
        "    \"2d323018c74db7e0432ff368283ea429f13bd36d\",\n",
        "    \"343833ab1d8cd17dae6b702830864500d1e66e19\",\n",
        "    \"c60b952d0c20a63dc263220ec5b49a54fd20d175\",\n",
        "    \"8ac01d3a84bc548a8b243d08c6e031206f293908\",\n",
        "    \"f00be2ed7b19b8d0f44915a1886437193aa224aa\",\n",
        "    \"950390c88b7bd9d6f886e5f01bae9460c0aa407b\",\n",
        "    \"3322c1b4795e4a9e4477feafd55685d647b4e29c\",\n",
        "    \"34362be27ada680a58e42d26758bf08c01d3460a\",\n",
        "    \"60d815ba2c1458c3a4039595a5ed723a7501a36e\",\n",
        "    \"928b339ba222a3933fce4523f8033fa3eb7ed62f\",\n",
        "    \"4931f0033f0ab217fd0fe2a2024d22a119782e2c\",\n",
        "    \"83846604933daff160cefc28c2a828bd93a84e1a\",\n",
        "    \"d2fe7281d8b1e043009033c957ca347847343e14\",\n",
        "    \"c1552a0046ef4ece5d146544043edb2deb97f7a1\",\n",
        "    # And more ...\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8587b9de-f507-436c-9314-75c1be0896bb",
      "metadata": {
        "id": "8587b9de-f507-436c-9314-75c1be0896bb"
      },
      "source": [
        "**Note**: If you're thinking that this manual process should be automatable--you're right! In another notebook, we will use a pre-trained version of our gridline-detecting model to accept or reject images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "092d79e7-1477-4e0c-a59f-63187ba48221",
      "metadata": {
        "id": "092d79e7-1477-4e0c-a59f-63187ba48221"
      },
      "outputs": [],
      "source": [
        "usable_set = set(usable_parent_images)\n",
        "unusable_set = set(unusable_parent_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d8034db-4dfe-48f6-a80d-22b7814a7f91",
      "metadata": {
        "id": "3d8034db-4dfe-48f6-a80d-22b7814a7f91"
      },
      "outputs": [],
      "source": [
        "firestore_client = firestore.Client(project=PROJECT_ID)\n",
        "bulkwriter = firestore_client.bulk_writer()\n",
        "collection = firestore_client.collection(COLLECTION_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae1e7a5b-df84-4d52-9517-90c959fcf72c",
      "metadata": {
        "id": "ae1e7a5b-df84-4d52-9517-90c959fcf72c"
      },
      "outputs": [],
      "source": [
        "# Iterate over all of the metadata entries & images that we want to delete\n",
        "unusable_shards = collection.where(\"Parent\", \"in\", list(unusable_set)[:10]).stream()\n",
        "for doc in unusable_shards:\n",
        "    bulkwriter.delete(doc.reference)\n",
        "\n",
        "bulkwriter.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63f3731c-2f6d-4c54-93d5-704fdd834d72",
      "metadata": {
        "id": "63f3731c-2f6d-4c54-93d5-704fdd834d72"
      },
      "outputs": [],
      "source": [
        "# Iterate over all of the good entries\n",
        "subset_start_index = 0\n",
        "while subset_start_index < len(usable_set):\n",
        "    subset = list(usable_set)[subset_start_index:subset_start_index + 10]\n",
        "    usable_shards = collection.where(\"Parent\", \"in\", subset).stream()\n",
        "\n",
        "    for doc in usable_shards:\n",
        "        bulkwriter.update(doc.reference, { \"Usable\": True})\n",
        "\n",
        "    subset_start_index = subset_start_index + 10\n",
        "\n",
        "bulkwriter.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b756e48-8a6b-4378-8665-502d10269d6c",
      "metadata": {
        "id": "1b756e48-8a6b-4378-8665-502d10269d6c"
      },
      "outputs": [],
      "source": [
        "bulkwriter.close()"
      ]
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "python3",
      "name": "common-cu110.m87",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cu110:m87"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}