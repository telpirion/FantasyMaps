{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a138013c-a157-41b1-bc69-7f25768aca03",
   "metadata": {
    "id": "67741dbe-06db-4f5c-baf9-b75416d69539",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551c8bcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Creating a Vertex Pipeline to extract training data\n",
    "\n",
    "This notebook (the second in a five-part series) creates a Vertex AI pipeline that scrapes images from an online source (e.g. Reddit) and stores the image metadata in Firestore. Here, you will build a pipeline that \n",
    "\n",
    "This notebook covers the following steps:\n",
    "\n",
    "1. Creating a pipeline component to collect images from Reddit\n",
    "1. Creating a pipeline component to store images in Cloud Storage\n",
    "1. Creating a pipeline component to store metadata about the images in Firestore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82dcd1e-611e-428d-9984-8c89fd181fec",
   "metadata": {},
   "source": [
    "### Set IAM permissions\n",
    "\n",
    "When you run a notebook on Vertex Workbench, the notebook runs in a Compute Engine context that has its own service account. You will need to give your service account IAM permissions to access Secret Manager before you can use it (in a pipeline).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71c9b41-cc68-4587-bac1-ef1c4799e88a",
   "metadata": {},
   "source": [
    "### Enable the Cloud resources\n",
    "\n",
    "For this notebook, you must have a Google Cloud project with the following resources:\n",
    "\n",
    "+ A Cloud Storage bucket\n",
    "+ The following APIs enabled:\n",
    "  - Cloud Firestore\n",
    "  - Vertex AI\n",
    "  - Storage\n",
    "  - Secret Manager\n",
    "  \n",
    "If you completed the [first](1_firestore.ipynb) notebook in this series, you should have these APIs already enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "841f4afa-2219-4274-81ae-5d1e37cd638b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  fantasymaps-334622\n"
     ]
    }
   ],
   "source": [
    "# Get your GCP project id from gcloud\n",
    "shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "PROJECT_ID=shell_output[0]\n",
    "print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0df4f24-5c51-45be-8aad-8fff58b6bd90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET = \"fantasy-maps\" # Google Cloud Storage bucket\n",
    "COLLECTION_NAME = \"FantasyMaps2\" # Firestore collection name\n",
    "LOCATION = \"us-west1\"\n",
    "GCS_PREFIX = \"FantasyMapsTest\"\n",
    "SUBREDDIT_NAME = \"battlemaps\"\n",
    "LIMIT=300\n",
    "MODEL_ID = \"4304645197347684352\"\n",
    "MODEL_NAME = f\"projects/{PROJECT_ID}/locations/{LOCATION}/models/{MODEL_ID}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f67b63f",
   "metadata": {},
   "source": [
    "### Install the required Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b04cdb7-fea6-46ec-a647-b334de316587",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rfd requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84639262-bb90-43b9-bdf2-a047277c29ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "google-cloud-secret-manager\n",
    "google-cloud-aiplatform\n",
    "google-cloud-pipeline-components>=1.0.30\n",
    "kfp\n",
    "praw\n",
    "pandas\n",
    "spacy\n",
    "pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bafcfe57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: google-cloud-secret-manager in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (2.16.1)\n",
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.20.0)\n",
      "Requirement already satisfied: google-cloud-pipeline-components>=1.0.30 in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (1.0.30)\n",
      "Requirement already satisfied: kfp in /home/jupyter/.local/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (1.8.12)\n",
      "Requirement already satisfied: praw in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (7.7.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (1.3.5)\n",
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (3.5.2)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (9.5.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-secret-manager->-r requirements.txt (line 1)) (3.20.3)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-secret-manager->-r requirements.txt (line 1)) (2.11.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.7/site-packages (from google-cloud-secret-manager->-r requirements.txt (line 1)) (0.12.4)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-secret-manager->-r requirements.txt (line 1)) (1.22.1)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform->-r requirements.txt (line 2)) (1.6.3)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform->-r requirements.txt (line 2)) (2.30.1)\n",
      "Requirement already satisfied: packaging<22.0.0dev,>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform->-r requirements.txt (line 2)) (21.3)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform->-r requirements.txt (line 2)) (2.8.0)\n",
      "Requirement already satisfied: google-cloud-notebooks>=0.4.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-pipeline-components>=1.0.30->-r requirements.txt (line 3)) (1.2.1)\n",
      "Requirement already satisfied: fire<1,>=0.3.1 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.4.0)\n",
      "Collecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
      "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "     |████████████████████████████████| 106 kB 4.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: typer<1.0,>=0.3.2 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.8.9)\n",
      "Requirement already satisfied: typing-extensions<4,>=3.7.4 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (3.10.0.2)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.9.1)\n",
      "Requirement already satisfied: strip-hints<1,>=0.1.8 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.1.10)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (1.2.13)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /opt/conda/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (1.12.11)\n",
      "Requirement already satisfied: jsonschema<4,>=3.0.1 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (3.2.0)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (2.0.0)\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (1.8.2)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (3.0.1)\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (18.20.0)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.14 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.1.14)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (1.7.1)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (5.4.1)\n",
      "Collecting google-auth<2,>=1.6.1\n",
      "  Using cached google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (8.0.3)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.13)\n",
      "Requirement already satisfied: update-checker>=0.18 in /home/jupyter/.local/lib/python3.7/site-packages (from praw->-r requirements.txt (line 5)) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /opt/conda/lib/python3.7/site-packages (from praw->-r requirements.txt (line 5)) (1.2.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in /home/jupyter/.local/lib/python3.7/site-packages (from praw->-r requirements.txt (line 5)) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 6)) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 6)) (1.21.6)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (59.4.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (2.4.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (3.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (2.26.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (4.62.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (6.2.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (1.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (2.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (8.1.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (2.0.7)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<2,>=0.9->kfp->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy->-r requirements.txt (line 7)) (3.6.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9,>=7.1.2->kfp->-r requirements.txt (line 4)) (4.8.2)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp->-r requirements.txt (line 4)) (1.13.3)\n",
      "Requirement already satisfied: termcolor in /home/jupyter/.local/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp->-r requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager->-r requirements.txt (line 1)) (1.57.0)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Using cached google_api_core-2.10.2-py3-none-any.whl (115 kB)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0\n",
      "  Downloading google_api_core-1.34.0-py3-none-any.whl (120 kB)\n",
      "     |████████████████████████████████| 120 kB 29.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager->-r requirements.txt (line 1)) (1.42.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-secret-manager->-r requirements.txt (line 1)) (1.51.1)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp->-r requirements.txt (line 4)) (0.19.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp->-r requirements.txt (line 4)) (0.1.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp->-r requirements.txt (line 4)) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp->-r requirements.txt (line 4)) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp->-r requirements.txt (line 4)) (4.2.4)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 2)) (2.3.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 2)) (2.4.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp->-r requirements.txt (line 4)) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp->-r requirements.txt (line 4)) (21.2.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp->-r requirements.txt (line 4)) (2021.10.8)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp->-r requirements.txt (line 4)) (1.26.7)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform->-r requirements.txt (line 2)) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 7)) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 7)) (3.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp->-r requirements.txt (line 4)) (0.37.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.8->spacy->-r requirements.txt (line 7)) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.8->spacy->-r requirements.txt (line 7)) (0.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy->-r requirements.txt (line 7)) (2.0.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 2)) (1.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp->-r requirements.txt (line 4)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp->-r requirements.txt (line 4)) (3.1.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 2)) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 2)) (2.21)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Installing collected packages: google-auth, google-api-core, google-cloud-storage\n",
      "  Attempting uninstall: google-auth\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "    Found existing installation: google-auth 2.17.3\n",
      "    Uninstalling google-auth-2.17.3:\n",
      "      Successfully uninstalled google-auth-2.17.3\n",
      "  Attempting uninstall: google-api-core\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "    Found existing installation: google-api-core 2.11.0\n",
      "    Uninstalling google-api-core-2.11.0:\n",
      "      Successfully uninstalled google-api-core-2.11.0\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "    Found existing installation: google-cloud-storage 2.8.0\n",
      "    Uninstalling google-cloud-storage-2.8.0:\n",
      "      Successfully uninstalled google-cloud-storage-2.8.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fantasy-maps 0.1.0 requires google-cloud-aiplatform==1.3.0, but you have google-cloud-aiplatform 1.20.0 which is incompatible.\u001b[0m\n",
      "Successfully installed google-api-core-2.2.2 google-auth-2.3.3 google-cloud-storage-1.44.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7087667-c013-4166-b44f-b703ac011f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "     |████████████████████████████████| 12.8 MB 4.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /opt/conda/lib/python3.7/site-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.2.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.62.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/jupyter/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions<4.5.0,>=3.7.4.1 in /home/jupyter/.local/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.10.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.21.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.26.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (59.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.8.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343a3b6-2918-45c7-bf13-8a38de0cfc7b",
   "metadata": {},
   "source": [
    "## Store your Reddit API key in Cloud Secret Manager\n",
    "\n",
    "Although you can [create a new secret in Cloud Secret Manager programmatically](https://cloud.google.com/secret-manager/docs/creating-and-accessing-secrets#create), in this notebook you must create it using the Cloud Console.\n",
    "\n",
    "To create a new secret in the Cloud Console, do the following:\n",
    "\n",
    "  1. Open the [Cloud Console](https://console.cloud.google.com/security/secret-manager).\n",
    "  1. Click **Create secret**.\n",
    "  1. In the **Create secret** page, do the following:\n",
    "     \n",
    "     + Give your secret a memorable name. This notebook uses the Reddit API, so the name of the secret\n",
    "       is `reddit-api-key`.\n",
    "     + Upload the credentials file. In this example, the `client_id`, `secret`, and `user_agent` credentials\n",
    "       provided by Reddit are stored as JSON in a single file.\n",
    "  \n",
    "  1. Click **Create secret** at the bottom of the page.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd122b8-04be-4007-9156-c00562ba2317",
   "metadata": {},
   "source": [
    "## Get Reddit API key from Secret Manager\n",
    "\n",
    "The important bit about an API key is that it should remain _secret_. You don't want to have it embedded in a notebook where anyone can see it!\n",
    "\n",
    "The next step is to make sure that you can access your Reddit API key programmatically from the notebook. We'll use the API key stored in Secret Manager to make calls to Reddit, both in the notebook and later from a Vertex AI pipeline.\n",
    "\n",
    "This notebook assumes that your Reddit API key is stored as a JSON-formatted string, with the following fields:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"secret\": \"YOUR_SECRET\",\n",
    "    \"client_id\": \"YOUR_CLIENT_ID\",\n",
    "    \"user_agent\": \"YOUR_USER_AGENT\",\n",
    "    \"user_name\": \"YOUR_REDDIT_USER_NAME\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87601b6f-16a2-4de1-b941-96358e7fd63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reddit_credentials(project_id):\n",
    "    \"\"\"Gets the Reddit API key out of Secrets Manager\n",
    "    \n",
    "    Arguments:\n",
    "        project_id (str): the current project ID\n",
    "    \n",
    "    Returns:\n",
    "        JSON object (dict)\n",
    "    \"\"\"\n",
    "    from google.cloud import secretmanager\n",
    "    import json\n",
    "\n",
    "    client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "    secret_resource_name = f\"projects/{project_id}/secrets/reddit-api-key/versions/1\"\n",
    "    response = client.access_secret_version(request={\"name\": secret_resource_name})\n",
    "\n",
    "    payload = response.payload.data.decode(\"UTF-8\")\n",
    "    reddit_key_json = json.loads(payload)\n",
    "\n",
    "    return reddit_key_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc1ddf-3a46-42b5-ad02-28481c44813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_key_json = get_reddit_credentials(PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9383ecef",
   "metadata": {},
   "source": [
    "## Create a custom Reddit pipelines component\n",
    "\n",
    "The pipeline and all it components need to be compiled into a runnable format. We use the Kubeflow Pipelines (`kfp`) SDK to create this uploadable pipelines file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99aef58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component)\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f2f591-92a3-4c46-82d4-082c6961d080",
   "metadata": {},
   "source": [
    "Now we can define the pipeline. For this component, we are going to store the `pandas.DataFrame` that we compose from the Redit posts as a CSV file on Cloud Storage. We'll pass the URI of this Storage file onto the next piece of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fdc68927",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Stage 1. Identify the images on Reddit that we could scrape.\n",
    "\n",
    "This part of the pipeline calls the Reddit API, reads `limit` number of posts on the subreddit,\n",
    "and then stores metadata about the posts in a CSV file on Storage.\n",
    "\"\"\"\n",
    "@component(packages_to_install=[\"praw\",\n",
    "                                \"google-cloud-secret-manager\",\n",
    "                                \"google-cloud-storage\",\n",
    "                                \"numpy\",\n",
    "                                \"pandas\",\n",
    "                                \"spacy\"])\n",
    "def reddit(\n",
    "    secret_name: str,\n",
    "    subreddit_name: str,\n",
    "    gcs_bucket_name: str,\n",
    "    gcs_prefix_name: str,\n",
    "    project_id: str,\n",
    "    limit: int,\n",
    ") -> str:\n",
    "    from datetime import datetime\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import praw\n",
    "    import re\n",
    "    \n",
    "    from google.cloud import storage\n",
    "\n",
    "    def get_reddit_credentials(project_id):\n",
    "        \"\"\"Gets the Reddit API key out of Secrets Manager\n",
    "    \n",
    "        Arguments:\n",
    "            project_id (str): the current project ID\n",
    "\n",
    "        Returns:\n",
    "            JSON object (dict)\n",
    "        \"\"\"\n",
    "        from google.cloud import secretmanager\n",
    "        import json\n",
    "\n",
    "        client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "        secret_resource_name = f\"projects/{project_id}/secrets/{secret_name}/versions/1\"\n",
    "        response = client.access_secret_version(request={\"name\": secret_resource_name})\n",
    "        payload = response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "        return json.loads(payload)\n",
    "    \n",
    "    def get_reddit_posts(reddit_credentials, subreddit_name, limit):\n",
    "        \"\"\"Gets posts from a subreddit.\n",
    "\n",
    "        Arguments:\n",
    "            reddit_credentials (dict): a dictionary with client_id, secret, and user_agent\n",
    "            subreddit_name (str): the name of the subreddit to scrape posts from\n",
    "            limit (int): the maximum number of posts to grab\n",
    "\n",
    "        Returns:\n",
    "            List of Reddit API objects\n",
    "        \"\"\"\n",
    "        import praw\n",
    "\n",
    "        reddit = praw.Reddit(client_id=reddit_credentials[\"client_id\"], \n",
    "                     client_secret=reddit_credentials[\"secret\"],\n",
    "                     user_agent=reddit_credentials[\"user_agent\"])\n",
    "\n",
    "        return reddit.subreddit(subreddit_name).hot(limit=limit)\n",
    "\n",
    "    def convert_posts_to_dataframe(posts, columns):\n",
    "        \"\"\"Converts a sequence of Reddit API post objects into a pandas.DataFrame.\n",
    "        \n",
    "        Arguments:\n",
    "            posts (list(praw.Post)): the posts from Reddit\n",
    "            columns (list(str)): the column headings for the Dataframe\n",
    "        \n",
    "        Returns:\n",
    "            A pandas.Dataframe\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "\n",
    "        filtered_posts = [[s.title, s.selftext, s.id, s.url] for s in posts]\n",
    "        filtered_posts = np.array(filtered_posts)\n",
    "        reddit_posts_df = pd.DataFrame(filtered_posts,\n",
    "                                   columns=columns)\n",
    "\n",
    "        return reddit_posts_df\n",
    "    \n",
    "    COLUMNS = ['Title', 'Post', 'ID', 'URL']\n",
    "    \n",
    "    # Get the data from Reddit\n",
    "    credentials = get_reddit_credentials(project_id=project_id)\n",
    "    posts = get_reddit_posts(reddit_credentials=credentials, subreddit_name=subreddit_name,\n",
    "                             limit=limit)\n",
    "    \n",
    "    reddit_posts_df = convert_posts_to_dataframe(posts=posts, columns=COLUMNS)\n",
    "    \n",
    "    # Remove all of the posts that don't meet our criteria\n",
    "    import re\n",
    "    jpg_df = reddit_posts_df[(reddit_posts_df[\"URL\"].str.contains(\"jpg\")) &\n",
    "                             (reddit_posts_df[\"Title\"].str.contains(pat = \"\\d+x\\d\"))]\n",
    "    \n",
    "    # Save the dataframe as CSV in Storage\n",
    "    csv_str = jpg_df.to_csv()\n",
    "    \n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(gcs_bucket_name)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    csv_file_uri = f\"{gcs_prefix_name}/_reddit-scraped-{subreddit_name}-{timestamp}.csv\"\n",
    "    \n",
    "    file_blob = bucket.blob(csv_file_uri)\n",
    "    file_blob.upload_from_string(csv_str)\n",
    "    \n",
    "    return csv_file_uri\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0794401a-c153-4407-a04f-030e70c4b3db",
   "metadata": {},
   "source": [
    "## Create the Cloud Storage component\n",
    "\n",
    "In this next component, we need to store any unique map images that we have picked up from the scraping. However, we need to validate that these images are useful training data before we process them and store their metadata in Firestore.\n",
    "\n",
    "To validate the images, we will do the following:\n",
    "\n",
    "1. Ensure that we don't already have the image in Firestore\n",
    "2. Use a pre-trained, earlier version of our model to infer the existence of gridlines on the image\n",
    "\n",
    "We'll do the first step in validation in the `storage` component. The second step (using an existing model to validate the usefulness of the images) will require using batch predictions on Vertex AI; we'll create another component to handle that part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb58d5ca-67dd-4abe-8aba-e6cee61e16d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Stage 2. Save images from Reddit in Cloud Storage.\n",
    "\n",
    "This part of the pipeline reads the CSV from the previous step, downloads\n",
    "the image from Reddit locally, compares a hash value of the image against the\n",
    "document IDs in Firestore, and then stores any image with new hash values (IDs)\n",
    "in Cloud Storage.\n",
    "\n",
    "One interstitial step in this process is to create a new human-friendly filename\n",
    "for the image. The pipeline uses spaCy to asses the post title for tokens to\n",
    "be used as filenames.\n",
    "\"\"\"\n",
    "@component(packages_to_install=[\"spacy\",\n",
    "                                \"google-cloud-firestore\",\n",
    "                                \"google-cloud-storage\",\n",
    "                                \"pandas\",\n",
    "                                \"jsonlines\"])\n",
    "def storage(\n",
    "    project_id: str,\n",
    "    location_name: str,\n",
    "    gcs_bucket_name: str,\n",
    "    gcs_prefix_name: str,\n",
    "    collection_name: str,\n",
    "    csv_input_file: str,\n",
    ") -> NamedTuple(\n",
    "    \"outputs\",\n",
    "    [\n",
    "        (\"batch_predict_file_uri\", str),\n",
    "        (\"posts_csv_file\", str),\n",
    "    ]\n",
    "):\n",
    "    \n",
    "    from google.cloud import firestore\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    import base64\n",
    "    from datetime import datetime\n",
    "    from io import BytesIO\n",
    "    import jsonlines\n",
    "    import pandas as pd\n",
    "    \n",
    "    import spacy\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    spacy.prefer_gpu()\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    def make_nice_filename(name):\n",
    "        \"\"\"Converts Reddit post title into a meaningful(ish) filename.\n",
    "\n",
    "        Arguments:\n",
    "            name (str): title of the post\n",
    "\n",
    "        Returns:\n",
    "            String. Format is `<adj.>-<nouns>.<cols>x<rows>.jpg`\n",
    "        \"\"\"\n",
    "        import re\n",
    "\n",
    "        dims = re.findall(\"\\d+x\\d+\", name)\n",
    "        if len(dims) is 0:\n",
    "            return \"\"\n",
    "\n",
    "        dims = dims[0].split(\"x\")\n",
    "        if len(dims) is not 2:\n",
    "            return \"\"\n",
    "\n",
    "        tokens = get_tokens(name)\n",
    "        new_name = name.lower()[:30]\n",
    "\n",
    "        if len(tokens) > 0:\n",
    "            tokens = tokens[:6] # Arbitrarily keep new names to six words or less\n",
    "            new_name = \"_\".join(tokens)\n",
    "\n",
    "        return f\"{new_name}.{dims[0]}x{dims[1]}.jpg\"\n",
    "\n",
    "    def get_tokens(title):\n",
    "        \"\"\"Analyzes a post for nouns, proper nouns, and adjectives.\n",
    "\n",
    "        Arguments:\n",
    "            title (str): title of the post\n",
    "\n",
    "        Returns:\n",
    "            List of string. Words to use in a filename.    \n",
    "        \"\"\"\n",
    "        import spacy\n",
    "\n",
    "        POS = [\"PROPN\", \"NOUN\", \"ADJ\"]\n",
    "        words = []\n",
    "\n",
    "        tokens = nlp(title)\n",
    "        for t in tokens:\n",
    "            pos = t.pos_\n",
    "\n",
    "            if pos in POS:\n",
    "                words.append(t.text.lower())\n",
    "\n",
    "        return words\n",
    "    def convert_image_to_hash(content):\n",
    "        \"\"\"Convert image data to hash value (str).\n",
    "\n",
    "        Arguments:\n",
    "            content (byte array): the image\n",
    "\n",
    "        Return:\n",
    "            The image hash value as a string.\n",
    "        \"\"\"\n",
    "        import hashlib\n",
    "\n",
    "        sha1 = hashlib.sha1()\n",
    "        jpg_hash = sha1.update(content)\n",
    "        jpg_hash = sha1.hexdigest()\n",
    "\n",
    "        return jpg_hash\n",
    "    \n",
    "    def download_image(url):\n",
    "        \"\"\"Download an image from the internet to local file system.\n",
    "\n",
    "        Arguments:\n",
    "            url (str): the image to download\n",
    "\n",
    "        Returns:\n",
    "            Bool. Indicates whether downloading the image was successful.\n",
    "        \"\"\"\n",
    "        import requests\n",
    "\n",
    "        r = requests.get(url, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            r.raw.decode_content = True\n",
    "            \n",
    "            hsh = convert_image_to_hash(r.content)\n",
    "            return (r.content, hsh)\n",
    "    \n",
    "    # Begin pipeline\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(gcs_bucket_name)\n",
    "\n",
    "    firestore_client = firestore.Client(project=project_id)\n",
    "    collection_ref = firestore_client.collection(collection_name)\n",
    "\n",
    "    blob = bucket.blob(csv_input_file)\n",
    "    csv_bytes = blob.download_as_string()\n",
    "    csv_buffer = BytesIO(csv_bytes)\n",
    "\n",
    "    jpg_df = pd.read_csv(csv_buffer)\n",
    "    batch_prediction_inputs = []\n",
    "    \n",
    "    for i, row, in jpg_df.iterrows():\n",
    "        url = row[\"URL\"]\n",
    "        title = row[\"Title\"]\n",
    "        \n",
    "        content, hsh = download_image(url)\n",
    "\n",
    "        # Check whether we already have this image\n",
    "        doc_ref = collection_ref.document(hsh)\n",
    "        doc_ref = doc_ref.get()\n",
    "        if doc_ref.exists:\n",
    "            continue\n",
    "        \n",
    "        file_name = make_nice_filename(title)\n",
    "                \n",
    "        img_gcs_uri = f\"gs://{gcs_bucket_name}/{gcs_prefix_name}/{file_name}\"\n",
    "        blob_name = f\"{gcs_prefix_name}/{file_name}\"\n",
    "\n",
    "        file_blob = bucket.blob(blob_name)\n",
    "        image_buffer = BytesIO(content)\n",
    "\n",
    "        # Get image grid metadata\n",
    "        file_blob.upload_from_file(image_buffer)\n",
    "        jpg_df.at[i, \"URI\"] = img_gcs_uri\n",
    "        jpg_df.at[i, \"UID\"] = hsh\n",
    "        jpg_df.at[i, \"Filename\"] = file_name\n",
    "        \n",
    "        batch_prediction_inputs.append({\n",
    "            \"content\": img_gcs_uri,\n",
    "            \"mimeType\": \"image/jpeg\",\n",
    "        })\n",
    "\n",
    "    # Save the dataframe as CSV in Storage (again)\n",
    "    csv_str = jpg_df.to_csv()\n",
    "    file_blob = bucket.blob(csv_input_file)\n",
    "    file_blob.upload_from_string(csv_str)\n",
    "    \n",
    "    # Create batch prediction input file\n",
    "    bpi = BytesIO()\n",
    "    writer = jsonlines.Writer(bpi)\n",
    "    writer.write_all(batch_prediction_inputs)\n",
    "    writer.close()\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    bpi_gcs_path = f\"{gcs_prefix_name}/_batch_prediction_input_{timestamp}.jsonl\"\n",
    "    batch_prediction_input_file = f\"gs://{gcs_bucket_name}/{bpi_gcs_path}\"\n",
    "    \n",
    "    bpi_str = str(bpi.getvalue(), encoding=\"UTF8\")\n",
    "    bpi_blob = bucket.blob(bpi_gcs_path)\n",
    "    bpi_blob.upload_from_string(bpi_str)\n",
    "    \n",
    "    return (batch_prediction_input_file, csv_input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f1585-d2dc-4a79-b0fe-86ea4cfd980d",
   "metadata": {},
   "source": [
    "## Create a custom batch prediction component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "89a6d7b0-2825-4d81-a6df-091e2f582357",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Stage 3. Use an earlier version of the model to identify good data candidates.\n",
    "\n",
    "This part of the pipeline uses batch prediction to measure the usefulness of\n",
    "the images from Reddit for refining the model. The batch prediction operation\n",
    "returns confidence values for gridlines that are used in subsequent training runs. \n",
    "\"\"\"\n",
    "@component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def custom_batch_prediction(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model_resource_name: str,\n",
    "    job_display_name: str,\n",
    "    gcs_input_file: str,\n",
    "    gcs_output_dir: str\n",
    ") -> str:\n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    aiplatform.init(project=project, location=location)\n",
    "    \n",
    "    model=aiplatform.Model(model_resource_name)\n",
    "    \n",
    "    batch_prediction_job = model.batch_predict(\n",
    "        job_display_name=job_display_name,\n",
    "        gcs_source=gcs_input_file,\n",
    "        gcs_destination_prefix=gcs_output_dir,\n",
    "        sync=True,\n",
    "    )\n",
    "\n",
    "    batch_prediction_job.wait()\n",
    "    \n",
    "    return batch_prediction_job.output_info.gcs_output_directory\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52768187-c334-451e-819d-e0d7fd1f5161",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the Firestore component\n",
    "\n",
    "Before creating this component, it helps to understand the schema for documents in the Firestore collection. The identifier of the document is a truncated hash value derived from the data in the image.\n",
    "\n",
    "Each document has the following fields and data types (expressed as JSON):\n",
    "\n",
    "```json\n",
    "{\n",
    "   \"BBoxes\": [{\n",
    "        \"xMin\": 0.0,\n",
    "        \"xMax\": 0.0,\n",
    "        \"yMin\": 0.0,\n",
    "        \"yMax\": 0.0,\n",
    "        \"displayName\": \"string\"\n",
    "   }],\n",
    "   \"VTT\": {\n",
    "        \"cellsOffsetX\": 0,\n",
    "        \"cellsOffsetY\": 0,\n",
    "        \"imageWidth\": 0,\n",
    "        \"imageHeight\": 0,\n",
    "        \"cellWidth\": 0,\n",
    "        \"cellHeight\": 0\n",
    "    },\n",
    "    \"URI\": \"string\",\n",
    "    \"URL\": \"string\"\n",
    "    \"Title\": \"string\",\n",
    "    \"Post\": \"string\"\n",
    "    \"Filename\": \"string\",\n",
    "    \"Width\": 0.0,\n",
    "    \"Height\": 0.0\n",
    "    \"Columns\": 0,\n",
    "    \"Rows\": 0,\n",
    "    \"NeedsSharding\": false\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "50ff240e-a81e-4504-8803-4ecbf2f8962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Stage 4. Store the metadata of useful images into Firestore.\n",
    "\n",
    "This pipeline component reads the results of the batch prediction, filters on\n",
    "a set confidence threshold for the predictions, and then stores the metadata\n",
    "of the \"good\" maps into Firestore.\n",
    "\"\"\"\n",
    "@component(packages_to_install=[\"Pillow\",\n",
    "                                \"google-cloud-firestore\",\n",
    "                                \"google-cloud-storage\",\n",
    "                                \"numpy\",\n",
    "                                \"pandas\",\n",
    "                                \"jsonlines\"])\n",
    "def firestore(\n",
    "    subreddit_name: str,\n",
    "    collection_name: str,\n",
    "    gcs_bucket_name: str,\n",
    "    gcs_prefix_name: str,\n",
    "    csv_input_file: str,\n",
    "    batch_prediction_uri: str,\n",
    "    project_id: str,\n",
    "    threshold: float,\n",
    "    percentage: float\n",
    ") -> NamedTuple(\n",
    "    \"outputs\",\n",
    "    [\n",
    "        (\"usable\", int),\n",
    "        (\"unusable\", int),\n",
    "        (\"stored\", int),\n",
    "    ]\n",
    "):\n",
    "    \n",
    "    from datetime import datetime\n",
    "    import hashlib\n",
    "    from io import BytesIO\n",
    "    import json\n",
    "    import jsonlines\n",
    "    import math\n",
    "    import pandas as pd\n",
    "    from PIL import Image\n",
    "    import re\n",
    "    import requests\n",
    "    import shutil\n",
    "\n",
    "    from google.cloud import firestore\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def process_batch_predict_output(bp_jsonl, threshold, percentage):\n",
    "        \"\"\"Parses a set of batch predictions for highest confidence inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            bp_jsonl (str): the batch prediction results, as a string\n",
    "            threshold (float): the lowest confidence value to accept from 0.0 to 1.0\n",
    "            percentage (float): the top percentage (by quality) of predictions to check\n",
    "            \n",
    "        Returns:\n",
    "            list of GCS URIs\n",
    "        \"\"\"\n",
    "        predictions = bp_jsonl.decode(\"utf-8\").split(\"\\n\")\n",
    "        reader = jsonlines.Reader(predictions)\n",
    "\n",
    "        images_uris = {\n",
    "            \"usable\": [],\n",
    "            \"unusable\": [],\n",
    "        }\n",
    "\n",
    "        for obj in reader.iter(type=dict, skip_invalid=True):\n",
    "            confidences = obj[\"prediction\"][\"confidences\"]\n",
    "            image_gcs_uri = obj[\"instance\"][\"content\"]\n",
    "            # NOTE: we check the predictions above the percentage of\n",
    "            # to ensure they are above the acceptable threshold. If\n",
    "            # the image is above the acceptable threshold, we keep\n",
    "            # the image for training data.\n",
    "            top_n_images = int(len(confidences) * percentage)\n",
    "            marginal_result = confidences[top_n_images]\n",
    "            if marginal_result > threshold:\n",
    "                images_uris[\"usable\"].append(image_gcs_uri)\n",
    "            else:\n",
    "                images_uris[\"unusable\"].append(image_gcs_uri)\n",
    "        \n",
    "        return images_uris\n",
    "        \n",
    "    def get_image_width_and_height(img_bytes):\n",
    "        \"\"\"Open the image and get the image's height and width in pixels.\n",
    "\n",
    "        Arguments:\n",
    "            img_bytes (str):\n",
    "\n",
    "        Returns:\n",
    "            Tuple of width, height\n",
    "        \"\"\"\n",
    "        w = h = 0\n",
    "        \n",
    "        f = BytesIO(img_bytes)\n",
    "        with Image.open(f) as img:\n",
    "            w, h = img.size\n",
    "\n",
    "        return (int(w), int(h))\n",
    "\n",
    "    def compute_vtt_data(width, height, columns, rows):\n",
    "        \"\"\"Calculate the VTT values for the image.\n",
    "\n",
    "        Arguments:\n",
    "            width (int):\n",
    "            height (int):\n",
    "            columns (int):\n",
    "            rows (int): \n",
    "        Returns:\n",
    "            Dict.\n",
    "        \"\"\"\n",
    "\n",
    "        return {\n",
    "            \"cellsOffsetX\": 0, # Assumes no offset\n",
    "            \"cellsOffsetY\": 0, # Assumes no offset\n",
    "            \"imageWidth\": int(width),\n",
    "            \"imageHeight\": int(height),\n",
    "            \"cellWidth\": int(width / columns),\n",
    "            \"cellHeight\": int(height / rows)\n",
    "        }\n",
    "\n",
    "    def convert_image_to_hash(content):\n",
    "        \"\"\"Convert image data to hash value (str).\n",
    "\n",
    "        Arguments:\n",
    "            content (byte array): the image\n",
    "\n",
    "        Return:\n",
    "            The image hash value as a string.\n",
    "        \"\"\"\n",
    "        sha1 = hashlib.sha1()\n",
    "        jpg_hash = sha1.update(content)\n",
    "        jpg_hash = sha1.hexdigest()\n",
    "\n",
    "        return jpg_hash    \n",
    "    \n",
    "    def compute_bboxes(*, width=0, height=0, columns=0, rows=0, cell_width=0, cell_height=0):\n",
    "        \"\"\"Determines bounding boxes for image object detection.\n",
    "\n",
    "        Arguments:\n",
    "            width (int): width of the image\n",
    "            height (int): height of the image\n",
    "            columns (int): number of columns in the grid\n",
    "            rows (int): number of rows in the grid\n",
    "            cell_width (int):\n",
    "            cell_height (int):\n",
    "\n",
    "        Returns:\n",
    "            List of dict.\n",
    "        \"\"\"\n",
    "        bboxes = []\n",
    "        BORDER = 1 # 1px border around the outside of the cell\n",
    "        LABEL = \"cell\"\n",
    "\n",
    "        curr_x = cell_width\n",
    "        while curr_x < width:\n",
    "            curr_y = cell_height\n",
    "            while curr_y < height:\n",
    "                x_min = (curr_x - BORDER) / width\n",
    "                y_min = (curr_y - BORDER) / height\n",
    "                x_max = (curr_x + cell_width + BORDER) / width\n",
    "                y_max = (curr_y + cell_height + BORDER) / height\n",
    "                bboxes.append({\n",
    "                    \"xMin\": x_min,\n",
    "                    \"xMax\": x_max,\n",
    "                    \"yMin\": y_min,\n",
    "                    \"yMax\": y_max,\n",
    "                    \"displayName\": LABEL\n",
    "                })\n",
    "                curr_y = curr_y + cell_height\n",
    "            curr_x = curr_x + cell_width\n",
    "            \n",
    "        return bboxes\n",
    "    \n",
    "    def store_metadata_fs(*, project_id, series, collection_name, uid):\n",
    "        \"\"\"Upserts image metadata into a Firestore collection.\n",
    "\n",
    "        Arguments:\n",
    "            project_id (str): the Google Cloud project to store these in\n",
    "            series (pd.Series): a Pandas series with the image's metadata\n",
    "            collection_name (str): the Firestore collection to store the data in\n",
    "        \"\"\"\n",
    "        client = firestore.Client(project=project_id)\n",
    "\n",
    "        series_dict = series.to_dict()\n",
    "\n",
    "        # clean up the data a little bit before upserting\n",
    "        vtt = series[\"VTT\"]\n",
    "        if vtt is not \"\":\n",
    "            vtt = json.loads(vtt)\n",
    "            series_dict[\"VTT\"] = vtt\n",
    "\n",
    "        bboxes = series[\"BBoxes\"]\n",
    "        if bboxes is not \"\":\n",
    "            bboxes = json.loads(bboxes)[\"bboxes\"]\n",
    "            series_dict[\"BBoxes\"] = bboxes\n",
    "\n",
    "        # upsert the dict directly into Firestore!\n",
    "        client.collection(collection_name).document(uid).set(series_dict)\n",
    "    \n",
    "    # BEGIN MAIN\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(gcs_bucket_name)\n",
    "    \n",
    "    firestore_client = firestore.Client(project=project_id)\n",
    "    collection_ref = firestore_client.collection(collection_name)\n",
    "\n",
    "    # Determine how many of the scraped data is usable for training a model\n",
    "    bp_path = batch_prediction_uri.replace(f\"gs://{gcs_bucket_name}/\", \"\")\n",
    "    \n",
    "    blobs = bucket.list_blobs(prefix=bp_path)\n",
    "    for b in blobs:\n",
    "        if b.name.find(\".jsonl\") > -1:\n",
    "            bp_jsonl_blob = b\n",
    "            break\n",
    "            \n",
    "    bp_jsonl = bp_jsonl_blob.download_as_string()\n",
    "    \n",
    "    image_uris = process_batch_predict_output(bp_jsonl, threshold, percentage)\n",
    "    \n",
    "    # Open up the complete list of scraped images as a DataFrame\n",
    "    blob = bucket.blob(csv_input_file)\n",
    "    csv_bytes = blob.download_as_string()\n",
    "    csv_buffer = BytesIO(csv_bytes)\n",
    "\n",
    "    jpg_df = pd.read_csv(csv_buffer)\n",
    "    \n",
    "    # Iterate over JPG URIs, download them in batches, convert to sha values\n",
    "    for i, row in jpg_df.iterrows():\n",
    "        jpg_uri = row[\"URI\"]\n",
    "        filename = row[\"Filename\"]\n",
    "        \n",
    "        if jpg_uri in image_uris[\"unusable\"]:\n",
    "            continue\n",
    "\n",
    "        jpg_uri = jpg_uri.replace(f\"gs://{gcs_bucket_name}/\", \"\")\n",
    "        jpg_blob = bucket.blob(jpg_uri)\n",
    "        jpg_bytes = jpg_blob.download_as_bytes()\n",
    "        \n",
    "        w, h = get_image_width_and_height(jpg_bytes)\n",
    "    \n",
    "        jpg_df.at[i, \"Width\"] = w\n",
    "        jpg_df.at[i, \"Height\"] = h\n",
    "\n",
    "        # Get columns & rows for original image, based upon the name.\n",
    "        paths = filename.split(\".\")\n",
    "        dims = paths[-2]\n",
    "        cols, rows = dims.split(\"x\")\n",
    "    \n",
    "        cols = int(cols)\n",
    "        rows = int(rows)\n",
    "\n",
    "        jpg_df.at[i, \"Columns\"] = cols\n",
    "        jpg_df.at[i, \"Rows\"] = rows\n",
    "        \n",
    "        # Compute the vtt data for the image\n",
    "        vtt = compute_vtt_data(width=w, height=h, columns=cols, rows=rows)\n",
    "    \n",
    "        # Note: pandas has issues storing a dict in a cell\n",
    "        jpg_df.at[i, \"VTT\"] = json.dumps(vtt)  \n",
    "        bboxes = compute_bboxes(width=w,\n",
    "                                height=h,\n",
    "                                columns=cols,\n",
    "                                rows=rows,\n",
    "                                cell_width=vtt[\"cellWidth\"],\n",
    "                                cell_height=vtt[\"cellHeight\"])\n",
    "        if len(bboxes) is 0:\n",
    "            print(f\"Error: {filename}\") \n",
    "                  \n",
    "        jpg_df.at[i, \"BBoxes\"] = json.dumps({ \"bboxes\": bboxes })\n",
    "        \n",
    "        if (cols * rows) > 500:\n",
    "            jpg_df.at[i, \"NeedsSharding\"] = True\n",
    "        else:\n",
    "            jpg_df.at[i, \"NeedsSharding\"] = False\n",
    "            \n",
    "    complete_df.set_index(\"UID\", inplace=True)\n",
    "    #complete_df.fillna(\"\", inplace=True)\n",
    "    complete_df.head(10)\n",
    "    \n",
    "    for uid, row in complete_df.iterrows():\n",
    "        store_metadata_fs(project_id=project_id, series=row,\n",
    "                          collection_name=collection_name, uid=uid)\n",
    "        \n",
    "    return (len(image_uris[\"usable\"]), len(image_uris[\"unusable\"]), len(complete_df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0f108e-5159-4878-aca3-103afd1bcbcc",
   "metadata": {},
   "source": [
    "## Create image shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "00e88f5c-dc0d-4d26-bd30-abd32d8e1c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Stage 5. Create smaller training images from stored images\n",
    "\n",
    "This pipeline component filters the Firestore collection to identify images that are\n",
    "too large (e.g. have more than 500 grid cells). For those images, the component \n",
    "creates smaller images (shards) from cropped versions of the original image.\n",
    "\"\"\"\n",
    "@component(packages_to_install=[\"Pillow\",\n",
    "                                \"google-cloud-firestore\",\n",
    "                                \"google-cloud-storage\",\n",
    "                                \"numpy\",\n",
    "                                \"pandas\",\n",
    "                                \"jsonlines\"])\n",
    "def create_shards(\n",
    "    collection_name: str,\n",
    "    gcs_bucket_name: str,\n",
    "    gcs_prefix_name: str,\n",
    "    project_id: str\n",
    ") -> NamedTuple(\n",
    "    \"outputs\",\n",
    "    [\n",
    "        (\"shards_created\", int),\n",
    "    ]\n",
    "):\n",
    "    import math\n",
    "    import pandas as pd\n",
    "    from PIL import Image\n",
    "    \n",
    "    def create_shard_path(filename, x_min, y_min, cols, rows):\n",
    "        \"\"\"Convert an image path string to new string.\n",
    "\n",
    "        Assumes the filename is of the format:\n",
    "            <name>.<cols>x<rows>.jpg\n",
    "\n",
    "        Arguments:\n",
    "            filename (str):\n",
    "            x_min (int):\n",
    "            y_min (int):\n",
    "            cols (int):\n",
    "            rows (int):\n",
    "\n",
    "        Returns:\n",
    "            String. New image path.\n",
    "        \"\"\"\n",
    "        paths = filename.split(\".\")\n",
    "        paths[-2] = f\"{math.floor(x_min)}_{math.floor(y_min)}.{cols}x{rows}\"\n",
    "        s_path = \".\".join(paths)\n",
    "        return s_path\n",
    "    \n",
    "    def create_shard(x_min, y_min, x_max, y_max, cols, rows, filename, content, parent_id):\n",
    "        \"\"\"Crops and saves an image.\n",
    "\n",
    "        Arguments:\n",
    "            x_min (int): the left-most point to crop, relative to the parent image\n",
    "            y_min (int): the top-most point to crop, relative to the parent image\n",
    "            x_max (int): the right-most point, relative to the parent image\n",
    "            y_max (int): the bottom-most poinst, relative to the parent image\n",
    "            cols (cols): the grid columns in this shard\n",
    "            rows (rows): the grid rows in this shard\n",
    "            filename (str): the parent image's local path\n",
    "            content (bytes): the original image\n",
    "            parent_id (str): the parent image's UID\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with local path, UID, width, height, columns, and rows\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "\n",
    "            f = BytesIO(content)\n",
    "            with Image.open(f) as img:\n",
    "                shard = img.crop((int(x_min), int(y_min), int(x_max), int(y_max)))\n",
    "\n",
    "                # Get new filepath name\n",
    "                s_path = create_shard_path(filename, x_min, y_min, cols, rows)\n",
    "\n",
    "                # Get new UID\n",
    "                hash_ = convert_image_to_hash(shard.tobytes())\n",
    "\n",
    "                shard.save(s_path)\n",
    "\n",
    "                d = {\n",
    "                    \"Width\": int(x_max - x_min),\n",
    "                    \"Height\": int(y_max - y_min),\n",
    "                    \"Columns\": cols,\n",
    "                    \"Rows\": rows,\n",
    "                    \"UID\": hash_,\n",
    "                    \"Path\": s_path,\n",
    "                    \"IsShard\": True,\n",
    "                    \"Parent\": parent_id\n",
    "                }\n",
    "\n",
    "        except SystemError as e:\n",
    "            print(f\"Error: {img_path}, bounds: {x_max},{y_max}\")\n",
    "            return None\n",
    "\n",
    "        return pd.DataFrame(data=d, index=[0])\n",
    "\n",
    "    def compute_shard_coordinates(width, height, cell_width,\n",
    "                                  cell_height, columns, rows):\n",
    "        \"\"\"Converts image data into 1,or more shards.\n",
    "\n",
    "        Arguments:\n",
    "            width (int):\n",
    "            height (int):\n",
    "            cell_width (int):\n",
    "            cell_height (int):\n",
    "            columns (int):\n",
    "            rows (int):\n",
    "\n",
    "        Returns:\n",
    "            List of tuples of (xMin, yMin, xMax, yMax, columns, rows)\n",
    "        \"\"\"\n",
    "        total_cells = columns * rows\n",
    "        if total_cells <= 500:\n",
    "            return\n",
    "\n",
    "        # Assume that a perfectly square map that approaches 500 cells is 22 cols by 22 rows.\n",
    "        # Cut an image into as many 22x22 shards as possible\n",
    "        SQRT = 22\n",
    "\n",
    "        h_shards = math.floor(columns / SQRT)\n",
    "        h_rem = columns % SQRT\n",
    "        v_shards = math.floor(rows / SQRT)\n",
    "        v_rem = rows % SQRT\n",
    "        shard_columns = shard_rows = SQRT\n",
    "        shards = []\n",
    "    \n",
    "        # Edge case 1: we have a narrow width (portrait-oriented) map\n",
    "        if h_shards == 0:\n",
    "            h_shards = 1\n",
    "            h_rem = 0\n",
    "            shard_columns = columns\n",
    "\n",
    "            # Edge case 2: we have a short height (landscape-oriented) map\n",
    "            if v_shards == 0:\n",
    "                v_shards = 1\n",
    "                v_rem = 0\n",
    "                shard_rows = rows\n",
    "\n",
    "            curr_min_x = 0\n",
    "            curr_min_y = 0\n",
    "            for _ in range(h_shards):\n",
    "                max_x = (cell_width * shard_columns) + curr_min_x\n",
    "                if max_x > width:\n",
    "                    max_x = width\n",
    "                for _ in range(v_shards):\n",
    "                    max_y = (cell_height * shard_rows) + curr_min_y\n",
    "                    if max_y > height:\n",
    "                        max_y = height\n",
    "\n",
    "                    shards.append((curr_min_x, curr_min_y, max_x, max_y, shard_columns, shard_rows))\n",
    "                    curr_min_y = max_y\n",
    "\n",
    "                curr_min_y = 0\n",
    "                curr_min_x = max_x\n",
    "    \n",
    "        # Get the right-side remainder\n",
    "        curr_min_x = width - (h_rem * cell_width)\n",
    "        curr_min_y = 0\n",
    "        for _ in range(v_shards):\n",
    "            max_y = (cell_height * shard_rows) + curr_min_y\n",
    "            if max_y > height:\n",
    "                max_y = height\n",
    "            shards.append((curr_min_x, curr_min_y, width, max_y, h_rem, shard_rows))\n",
    "            curr_min_y = max_y\n",
    "\n",
    "        # Get the bottom-side remainder\n",
    "        curr_min_y = height - (v_rem * cell_height)\n",
    "        curr_min_x = 0\n",
    "        for _ in range(h_shards):\n",
    "            max_x = (cell_width * shard_columns) + curr_min_x\n",
    "            if max_x > width:\n",
    "                max_x = width\n",
    "            shards.append((curr_min_x, curr_min_y, max_x, height, shard_columns, v_rem))\n",
    "            curr_min_x = max_x\n",
    "\n",
    "        return shards\n",
    "    \n",
    "    # BEGIN MAIN\n",
    "    shards = compute_shard_coordinates(width=w, height=h, columns=cols, rows=rows,\n",
    "                                       cell_width=vtt[\"cellWidth\"], cell_height=vtt[\"cellHeight\"])\n",
    "            \n",
    "    for shard in shards:\n",
    "        shard_df = create_shard(x_min=shard[0], y_min=shard[1], x_max=shard[2],\n",
    "                                   y_max=shard[3], cols=shard[4], rows=shard[5],\n",
    "                                   filename=filename, content=jpg_bytes, parent_id=row[\"UID\"])\n",
    "\n",
    "        if shard_df is None:\n",
    "            continue\n",
    "\n",
    "        s_vtt = vtt\n",
    "        s_vtt[\"width\"] = int(shard_df.iloc[0][\"Width\"])\n",
    "        s_vtt[\"height\"] = int(shard_df.iloc[0][\"Height\"])\n",
    "        shard_df.at[0, \"VTT\"] = json.dumps(s_vtt)\n",
    "\n",
    "        bboxes = compute_bboxes(dataframe=shard_df,\n",
    "                                cell_width=vtt[\"cellWidth\"],\n",
    "                                cell_height=vtt[\"cellHeight\"])\n",
    "\n",
    "        shard_df.at[0, \"BBoxes\"] = json.dumps({ \"bboxes\": bboxes })\n",
    "        shards_df = pd.concat([shards_df, shard_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb86e8e",
   "metadata": {},
   "source": [
    "## Build a simple pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a33687d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"reddit-scraper-pipeline\",\n",
    "    description=\"Gets data from a subreddit\",\n",
    "    pipeline_root=f\"gs://{BUCKET}/pipeline_root\",\n",
    ")\n",
    "def reddit_pipeline(\n",
    "    collection_name: str = COLLECTION_NAME,\n",
    "    secret_name: str = \"reddit-api-key\",\n",
    "    subreddit_name: str = SUBREDDIT_NAME,\n",
    "    gcs_bucket: str = BUCKET,\n",
    "    gcs_prefix: str = GCS_PREFIX,\n",
    "    gcs_bp_output: str = \"gs://fantasy-maps/ScrapedData\",\n",
    "    project_id: str = PROJECT_ID,\n",
    "    location: str = LOCATION,\n",
    "    limit: int = LIMIT,\n",
    "    threshold: float = 0.3, # confidence value of 0.30 or better\n",
    "    percentage: float = 0.1, # top 10%\n",
    "    model_name: str = MODEL_NAME\n",
    "):\n",
    "    \n",
    "    # Get the images from Reddit\n",
    "    reddit_op = reddit(\n",
    "        secret_name=secret_name,\n",
    "        subreddit_name=subreddit_name,\n",
    "        gcs_bucket_name=gcs_bucket,\n",
    "        gcs_prefix_name=gcs_prefix,\n",
    "        project_id=project_id,\n",
    "        limit=limit,\n",
    "    )\n",
    "    \n",
    "    reddit_csv_file = reddit_op.output\n",
    "    \n",
    "    # Store the new images on Cloud Storage\n",
    "    storage_op = storage(\n",
    "        project_id=project_id,\n",
    "        location_name=location,\n",
    "        gcs_bucket_name=gcs_bucket,\n",
    "        gcs_prefix_name=gcs_prefix,\n",
    "        collection_name=collection_name,\n",
    "        csv_input_file=reddit_csv_file\n",
    "    )\n",
    "    \n",
    "    batch_prediction_op= custom_batch_prediction(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        model_resource_name=model_name,\n",
    "        job_display_name=\"test-custom-bp\",\n",
    "        gcs_input_file=storage_op.outputs[\"batch_predict_file_uri\"],\n",
    "        gcs_output_dir=gcs_bp_output\n",
    "    )\n",
    "    \n",
    "    # Store training data in Firestore!\n",
    "    firestore(\n",
    "        subreddit_name = subreddit_name,\n",
    "        collection_name = collection_name,\n",
    "        gcs_bucket_name = gcs_bucket,\n",
    "        gcs_prefix_name = gcs_prefix,\n",
    "        csv_input_file = storage_op.outputs[\"posts_csv_file\"],\n",
    "        batch_prediction_uri = batch_prediction_op.output,\n",
    "        project_id = project_id,\n",
    "        threshold = threshold,\n",
    "        percentage = percentage\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1f32db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=reddit_pipeline, package_path=\"artifacts/reddit_scraper_pipeline_job.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ff782324",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=LOCATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40e59c5-f05a-4959-919c-1b3f3d2a2e7d",
   "metadata": {},
   "source": [
    "When we run the pipeline, we don't want it to cache the pipeline, since caching the pipeline will likely result in producing the exact same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf97982c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/reddit-scraper-pipeline-20230727034213?project=fantasymaps-334622\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    job_spec_path=\"artifacts/reddit_scraper_pipeline_job.json\",\n",
    "    enable_caching=False # Change to False when needing to generate new values per job run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579351b7-e456-43d2-8758-67cec8fa6fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m87"
  },
  "kernelspec": {
   "display_name": "XGBoost 1.7 (Local)",
   "language": "python",
   "name": "xgboost-1-7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
