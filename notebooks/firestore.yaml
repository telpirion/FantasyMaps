name: Firestore
inputs:
- {name: subreddit_name, type: String}
- {name: collection_name, type: String}
- {name: gcs_bucket_name, type: String}
- {name: gcs_prefix_name, type: String}
- {name: csv_input_file, type: String}
- {name: project_id, type: String}
outputs:
- {name: batch_predict_file_uri, type: String}
- {name: bp_inputs_count, type: Integer}
implementation:
  container:
    image: python:3.7
    command:
    - sh
    - -c
    - |2

      if ! [ -x "$(command -v pip)" ]; then
          python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip
      fi

      PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'Pillow' 'google-cloud-firestore' 'google-cloud-storage' 'numpy' 'pandas' 'kfp==1.8.12' && "$0" "$@"
    - sh
    - -ec
    - |
      program_path=$(mktemp -d)
      printf "%s" "$0" > "$program_path/ephemeral_component.py"
      python3 -m kfp.v2.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
    - "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing\
      \ import *\n\ndef firestore(\n    subreddit_name: str,\n    collection_name:\
      \ str,\n    gcs_bucket_name: str,\n    gcs_prefix_name: str,\n    csv_input_file:\
      \ str,\n    project_id: str,\n) -> NamedTuple(\n    \"Outputs\",\n    [\n  \
      \      (\"batch_predict_file_uri\", str),\n        (\"bp_inputs_count\", int),\n\
      \    ]\n):\n\n    from datetime import datetime\n    import hashlib\n    from\
      \ io import BytesIO\n    import json\n    import pandas as pd\n    from PIL\
      \ import Image\n    import re\n    import requests\n    import shutil\n\n  \
      \  from google.cloud import firestore\n    from google.cloud import storage\n\
      \n    storage_client = storage.Client(project=project_id)\n    bucket = storage_client.bucket(gcs_bucket_name)\n\
      \n    firestore_client = firestore.Client(project=project_id)\n    collection_ref\
      \ = firestore_client.collection(collection_name)\n\n    blob = bucket.blob(csv_input_file)\n\
      \    csv_bytes = blob.download_as_string()\n    csv_buffer = BytesIO(csv_bytes)\n\
      \n    jpg_df = pd.read_csv(csv_buffer)\n\n    hashes = [None] * len(jpg_df.index)\n\
      \    jpg_df.insert(1, \"HashId\", hashes, True)\n    jpg_df.insert(6, \"GcsURI\"\
      , hashes, True)\n\n    # Concatenate string of batch prediction inputs\n   \
      \ bp_inputs = \"\"\n    bp_inputs_count = 0\n\n    def make_nice_filename(name,\
      \ *, rows=None, cols=None):\n        regex = \"[\\s|\\(|\\\"|\\)]\"\n      \
      \  new_name = re.sub(regex, \"_\", name)\n        new_name = new_name.lower()[:30]\n\
      \        new_name = new_name.replace(\"__\", \"_\")\n\n        if rows is not\
      \ None and cols is not None:\n            new_name += f\".{cols}x{rows}\"\n\
      \        return f\"{new_name}.jpg\"\n\n\n    def create_vtt_json(content, title):\n\
      \        img = Image.open(BytesIO(content))\n        w, h = img.size\n\n   \
      \     dims = re.findall(\"\\d+x\\d+\", title)\n        if len(dims) is 0:\n\
      \            return None\n\n        dims = dims[0].split(\"x\")\n\n        if\
      \ len(dims) is not 2:\n            return None\n\n        cols = int(dims[0])\n\
      \        rows = int(dims[1])\n\n        cell_w = w / rows\n        cell_h =\
      \ h / cols\n        if cell_w != cell_h:\n            return None\n\n      \
      \  return {\n            \"cols\": cols,\n            \"rows\": rows,\n    \
      \        \"imageWidth\": w,\n            \"imageHeight\": h,\n            \"\
      cellOffsetX\": 0,\n            'cellOffsetY': 0, \n            'cellWidth':\
      \ cell_w, \n            'cellHeight': cell_h, \n        }\n\n    def compute_bboxes(vtt_data):\n\
      \        bboxes = []\n\n        cols = vtt_data[\"cols\"]\n        rows = vtt_data[\"\
      rows\"]\n\n        for x in range(1, cols):\n            for y in range(1, rows):\n\
      \               x_min_tmp = vtt_data[\"cellOffsetX\"] + (vtt_data[\"cellWidth\"\
      ] * x) - 2\n               x_max_tmp = x_min_tmp + vtt_data[\"cellWidth\"] +\
      \ 4\n               y_min_tmp = vtt_data[\"cellOffsetY\"] + (vtt_data[\"cellHeight\"\
      ] * y) - 2\n               y_max_tmp = y_min_tmp + vtt_data[\"cellHeight\"]\
      \ + 4\n\n               x_min_train = x_min_tmp / vtt_data[\"imageWidth\"]\n\
      \               x_max_train = x_max_tmp / vtt_data[\"imageWidth\"]\n       \
      \        y_min_train = y_min_tmp / vtt_data[\"imageHeight\"]\n             \
      \  y_max_train = y_max_tmp / vtt_data[\"imageHeight\"]\n\n               bboxes.append({\n\
      \                   \"xMin\": x_min_train,\n                   \"yMin\": y_min_train,\n\
      \                   \"xMax\": x_max_train,\n                   \"yMax\": y_max_train,\n\
      \                   \"displayName\": \"cell\"\n               })\n\n       \
      \ return bboxes\n\n    # Iterate over JPG URIs, download them in batches, convert\
      \ to sha values\n    for i, r in jpg_df.iterrows():\n        jpg_url = r[\"\
      URL\"]\n        title = r[\"Title\"]\n\n        req = requests.get(jpg_url,\
      \ stream=True)\n        if req.status_code == 200:\n            req.raw.decode_content\
      \ = True\n            sha1 = hashlib.sha1()\n            jpg_hash = sha1.update(req.content)\n\
      \            jpg_hash = sha1.hexdigest()\n\n            jpg_df[\"HashId\"][i]\
      \ = jpg_hash\n            #print(f\"Index {i}, hash {jpg_hash}\")\n        \
      \    hashes.append(jpg_hash)\n\n            # Try to fetch each document from\
      \ Firestore. If it does not exist,\n            # overwrite and download the\
      \ image.\n            doc_ref = collection_ref.document(jpg_hash)\n        \
      \    doc = doc_ref.get()\n            if not doc.exists:\n\n               \
      \ img_data = create_vtt_json(req.content, title)\n\n                if img_data\
      \ is not None:\n                    file_name = make_nice_filename(title,\n\
      \                                                   rows=img_data[\"rows\"],\n\
      \                                                   cols=img_data[\"cols\"])\n\
      \                else:\n                    file_name = make_nice_filename(title)\n\
      \n                img_gcs_uri = f\"gs://{gcs_bucket_name}/{gcs_prefix_name}/{file_name}\"\
      \n                blob_name = f\"{gcs_prefix_name}/{file_name}\"\n\n       \
      \         file_blob = bucket.blob(blob_name)\n                image_buffer =\
      \ BytesIO(req.content)\n\n                # Get image grid metadata\n      \
      \          #img_data = create_vtt_json(req.content, title)\n               \
      \ print(img_data)\n\n                file_blob.upload_from_file(BytesIO(req.content))\n\
      \n                data = {\n                    u\"filename\": file_name,\n\
      \                    u\"gcsURI\": img_gcs_uri,\n                    u\"source\"\
      : gcs_prefix_name,\n                    u\"userId\": \"None\",\n           \
      \         u\"sourceUrl\": jpg_url,\n                }\n\n                if\
      \ img_data is not None:\n                    bboxes = compute_bboxes(img_data)\n\
      \                    data[\"vtt\"] = img_data\n                    data[\"computedBBoxes\"\
      ] = bboxes\n\n                    doc_ref.set(data)\n                    print(f\"\
      Set data: {data}\")\n                    bp_inputs += json.dumps({ \"content\"\
      : img_gcs_uri, \"mimeType\": \"image/jpeg\"})\n                    bp_inputs\
      \ += \"\\n\"\n                    bp_inputs_count = bp_inputs_count + 1\n\n\
      \    # No fresh JPGs in this scraping; return empty string\n    if bp_inputs\
      \ is \"\":\n        print(\"no inputs\")\n        return (\"\", 0)\n\n    print(f\"\
      First ten: {jpg_df.head(10)}\")\n\n    # Save the batch_predict file\n    timestamp\
      \ = datetime.now().strftime(\"%Y%m%d%H%M%S\") \n    batch_predict_file_uri =\
      \ f\"gs://{gcs_bucket_name}/{gcs_prefix_name}/bp_input_{timestamp}.jsonl\"\n\
      \n    bp_blob_name = f\"{gcs_prefix_name}/bp_input_{timestamp}.jsonl\"\n   \
      \ bp_blob = bucket.blob(bp_blob_name)\n\n    bp_blob.upload_from_string(bp_inputs)\n\
      \n    return (batch_predict_file_uri, bp_inputs_count)  \n\n"
    args:
    - --executor_input
    - {executorInput: null}
    - --function_to_execute
    - firestore
