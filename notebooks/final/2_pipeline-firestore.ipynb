{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce211e25-1262-49d7-9dbc-6748718e81c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title ###### Licensed to the Apache Software Foundation (ASF), Version 2.0 (the \"License\")\n",
    "\n",
    "# Licensed to the Apache Software Foundation (ASF) under one\n",
    "# or more contributor license agreements. See the NOTICE file\n",
    "# distributed with this work for additional information\n",
    "# regarding copyright ownership. The ASF licenses this file\n",
    "# to you under the Apache License, Version 2.0 (the\n",
    "# \"License\"); you may not use this file except in compliance\n",
    "# with the License. You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing,\n",
    "# software distributed under the License is distributed on an\n",
    "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "# KIND, either express or implied. See the License for the\n",
    "# specific language governing permissions and limitations\n",
    "# under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551c8bcb",
   "metadata": {},
   "source": [
    "# Creating a Vertex Pipeline to extract training data\n",
    "\n",
    "This notebook (the second in a five-part series) creates a Vertex AI pipeline that scrapes images from an online source (e.g. Reddit) and stores the image metadata in Firestore. Here, you will build a pipeline that \n",
    "\n",
    "This notebook covers the following steps:\n",
    "\n",
    "1. Creating a pipeline component to collect images from Reddit\n",
    "1. Creating a pipeline component to store images in Cloud Storage\n",
    "1. Creating a pipeline component to store metadata about the images in Firestore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82dcd1e-611e-428d-9984-8c89fd181fec",
   "metadata": {},
   "source": [
    "### Set IAM permissions\n",
    "\n",
    "When you run a notebook on Vertex Workbench, the notebook runs in a Compute Engine context that has its own service account. You will need to give your service account IAM permissions to access Secret Manager before you can use it (in a pipeline).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71c9b41-cc68-4587-bac1-ef1c4799e88a",
   "metadata": {},
   "source": [
    "### Enable the Cloud resources\n",
    "\n",
    "For this notebook, you must have a Google Cloud project with the following resources:\n",
    "\n",
    "+ A Cloud Storage bucket\n",
    "+ The following APIs enabled:\n",
    "  - Cloud Firestore\n",
    "  - Vertex AI\n",
    "  - Storage\n",
    "  - Secret Manager\n",
    "  \n",
    "If you completed the [first](1_firestore.ipynb) notebook in this series, you should have these APIs already enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "841f4afa-2219-4274-81ae-5d1e37cd638b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  fantasymaps-334622\n"
     ]
    }
   ],
   "source": [
    "# Get your GCP project id from gcloud\n",
    "shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "PROJECT_ID=shell_output[0]\n",
    "print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0df4f24-5c51-45be-8aad-8fff58b6bd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = \"fantasy-maps\" # Google Cloud Storage bucket\n",
    "COLLECTION_NAME = \"FantasyMapsTest\" # Firestore collection name\n",
    "LOCATION = \"us-central1\"\n",
    "GCS_PREFIX = \"ScrapedData\"\n",
    "SUBREDDIT_NAME = \"battlemaps\"\n",
    "LIMIT=300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f67b63f",
   "metadata": {},
   "source": [
    "### Install the required Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b04cdb7-fea6-46ec-a647-b334de316587",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rfd requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84639262-bb90-43b9-bdf2-a047277c29ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "google-cloud-secret-manager\n",
    "google-cloud-aiplatform\n",
    "google-cloud-pipeline-components\n",
    "kfp\n",
    "praw\n",
    "pandas\n",
    "spacy\n",
    "pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bafcfe57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: google-cloud-secret-manager in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (2.10.0)\n",
      "Requirement already satisfied: google-cloud-aiplatform in /home/jupyter/.local/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: google-cloud-pipeline-components in /home/jupyter/.local/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (0.1.5)\n",
      "Requirement already satisfied: kfp in /home/jupyter/.local/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (1.8.12)\n",
      "Requirement already satisfied: praw in /home/jupyter/.local/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (7.5.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (1.3.4)\n",
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (3.4.3)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (8.4.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-secret-manager->-r requirements.txt (line 1)) (0.12.3)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-secret-manager->-r requirements.txt (line 1)) (1.31.5)\n",
      "Requirement already satisfied: proto-plus>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-secret-manager->-r requirements.txt (line 1)) (1.19.8)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform->-r requirements.txt (line 2)) (21.3)\n",
      "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform->-r requirements.txt (line 2)) (1.43.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform->-r requirements.txt (line 2)) (2.30.1)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (5.4.1)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.9.1)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions<4,>=3.7.4 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (3.10.0.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (1.35.0)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.8.9)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.14 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.1.14)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (1.7.1)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (1.8.2)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (1.12.8)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (3.0.1)\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (18.20.0)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (1.2.13)\n",
      "Requirement already satisfied: jsonschema<4,>=3.0.1 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (3.2.0)\n",
      "Requirement already satisfied: fire<1,>=0.3.1 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: strip-hints<1,>=0.1.8 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.1.10)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (8.0.3)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.13)\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (3.19.1)\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.3.2 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /opt/conda/lib/python3.7/site-packages (from praw->-r requirements.txt (line 5)) (1.2.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in /home/jupyter/.local/lib/python3.7/site-packages (from praw->-r requirements.txt (line 5)) (2.3.0)\n",
      "Requirement already satisfied: update-checker>=0.18 in /home/jupyter/.local/lib/python3.7/site-packages (from praw->-r requirements.txt (line 5)) (0.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 6)) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 6)) (1.19.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (1.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (2.4.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (4.62.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (3.0.10)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (0.10.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (2.0.7)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (59.4.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (8.1.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (2.26.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (3.0.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<2,>=0.9->kfp->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy->-r requirements.txt (line 7)) (3.6.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9,>=7.1.2->kfp->-r requirements.txt (line 4)) (4.8.2)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp->-r requirements.txt (line 4)) (1.13.3)\n",
      "Requirement already satisfied: termcolor in /home/jupyter/.local/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp->-r requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-secret-manager->-r requirements.txt (line 1)) (1.53.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-secret-manager->-r requirements.txt (line 1)) (1.42.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp->-r requirements.txt (line 4)) (0.19.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp->-r requirements.txt (line 4)) (0.1.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp->-r requirements.txt (line 4)) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp->-r requirements.txt (line 4)) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp->-r requirements.txt (line 4)) (0.2.7)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 2)) (1.7.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 2)) (2.1.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp->-r requirements.txt (line 4)) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp->-r requirements.txt (line 4)) (0.18.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp->-r requirements.txt (line 4)) (2021.10.8)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp->-r requirements.txt (line 4)) (1.26.7)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-aiplatform->-r requirements.txt (line 2)) (2.4.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy->-r requirements.txt (line 7)) (6.2.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 7)) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 7)) (3.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp->-r requirements.txt (line 4)) (0.37.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy->-r requirements.txt (line 7)) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy->-r requirements.txt (line 7)) (0.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy->-r requirements.txt (line 7)) (2.0.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 2)) (1.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp->-r requirements.txt (line 4)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp->-r requirements.txt (line 4)) (3.1.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 2)) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 2)) (2.21)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7087667-c013-4166-b44f-b703ac011f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "     |████████████████████████████████| 12.8 MB 5.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.8.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/jupyter/.local/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (59.4.0)\n",
      "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /home/jupyter/.local/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.10.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.62.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.19.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.7)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.8.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9383ecef",
   "metadata": {},
   "source": [
    "## Create a custom Reddit pipelines component\n",
    "\n",
    "The pipeline and all it components need to be compiled into a runnable format. We use the Kubeflow Pipelines (`kfp`) SDK to create this uploadable pipelines file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99aef58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component)\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f2f591-92a3-4c46-82d4-082c6961d080",
   "metadata": {},
   "source": [
    "Now we can define the pipeline. For this component, we are going to store the `pandas.DataFrame` that we compose from the Redit posts as a CSV file on Cloud Storage. We'll pass the URI of this Storage file onto the next piece of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdc68927",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"praw\",\n",
    "                                \"google-cloud-secret-manager\",\n",
    "                                \"google-cloud-storage\",\n",
    "                                \"numpy\",\n",
    "                                \"pandas\",\n",
    "                                \"spacy\"])\n",
    "def reddit(\n",
    "    secret_name: str,\n",
    "    subreddit_name: str,\n",
    "    gcs_bucket_name: str,\n",
    "    gcs_prefix_name: str,\n",
    "    project_id: str,\n",
    "    limit: int,\n",
    ") -> str:\n",
    "    from datetime import datetime\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import praw\n",
    "    import re\n",
    "    \n",
    "    from google.cloud import storage\n",
    "\n",
    "    def get_reddit_credentials(project_id):\n",
    "        \"\"\"Gets the Reddit API key out of Secrets Manager\n",
    "    \n",
    "        Arguments:\n",
    "            project_id (str): the current project ID\n",
    "\n",
    "        Returns:\n",
    "            JSON object (dict)\n",
    "        \"\"\"\n",
    "        from google.cloud import secretmanager\n",
    "        import json\n",
    "\n",
    "        client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "        secret_resource_name = f\"projects/{project_id}/secrets/{secret_name}/versions/1\"\n",
    "        response = client.access_secret_version(request={\"name\": secret_resource_name})\n",
    "        payload = response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "        return json.loads(payload)\n",
    "    \n",
    "    def get_reddit_posts(reddit_credentials, subreddit_name, limit):\n",
    "        \"\"\"Gets posts from a subreddit.\n",
    "\n",
    "        Arguments:\n",
    "            reddit_credentials (dict): a dictionary with client_id, secret, and user_agent\n",
    "            subreddit_name (str): the name of the subreddit to scrape posts from\n",
    "            limit (int): the maximum number of posts to grab\n",
    "\n",
    "        Returns:\n",
    "            List of Reddit API objects\n",
    "        \"\"\"\n",
    "        import praw\n",
    "\n",
    "        reddit = praw.Reddit(client_id=reddit_credentials[\"client_id\"], \n",
    "                     client_secret=reddit_credentials[\"secret\"],\n",
    "                     user_agent=reddit_credentials[\"user_agent\"])\n",
    "\n",
    "        return reddit.subreddit(subreddit_name).hot(limit=limit)\n",
    "\n",
    "    def convert_posts_to_dataframe(posts, columns):\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "\n",
    "        filtered_posts = [[s.title, s.selftext, s.id, s.url] for s in posts]\n",
    "        filtered_posts = np.array(filtered_posts)\n",
    "        reddit_posts_df = pd.DataFrame(filtered_posts,\n",
    "                                   columns=columns)\n",
    "\n",
    "        return reddit_posts_df\n",
    "    \n",
    "    COLUMNS = ['Title', 'Post', 'ID', 'URL']\n",
    "    \n",
    "    # Get the data from Reddit\n",
    "    credentials = get_reddit_credentials(project_id=project_id)\n",
    "    posts = get_reddit_posts(reddit_credentials=credentials, subreddit_name=subreddit_name,\n",
    "                             limit=limit)\n",
    "    \n",
    "    reddit_posts_df = convert_posts_to_dataframe(posts=posts, columns=COLUMNS)\n",
    "    \n",
    "    # Remove all of the posts that don't meet our criteria\n",
    "    import re\n",
    "    jpg_df = reddit_posts_df[(reddit_posts_df[\"URL\"].str.contains(\"jpg\")) &\n",
    "                             (reddit_posts_df[\"Title\"].str.contains(pat = \"\\d+x\\d\"))]\n",
    "    \n",
    "    # Save the dataframe as CSV in Storage\n",
    "    csv_str = jpg_df.to_csv()\n",
    "    \n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(gcs_bucket_name)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    csv_file_uri = f\"{gcs_prefix_name}/reddit-scraped-{subreddit_name}-{timestamp}.csv\"\n",
    "    \n",
    "    file_blob = bucket.blob(csv_file_uri)\n",
    "    file_blob.upload_from_string(csv_str)\n",
    "    \n",
    "    return csv_file_uri\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0794401a-c153-4407-a04f-030e70c4b3db",
   "metadata": {},
   "source": [
    "## Create the Cloud Storage component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb58d5ca-67dd-4abe-8aba-e6cee61e16d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52768187-c334-451e-819d-e0d7fd1f5161",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create the Firestore component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "50ff240e-a81e-4504-8803-4ecbf2f8962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "@component(packages_to_install=[\"Pillow\",\n",
    "                                \"google-cloud-firestore\",\n",
    "                                \"google-cloud-storage\",\n",
    "                                \"numpy\",\n",
    "                                \"pandas\"])\n",
    "def firestore(\n",
    "    subreddit_name: str,\n",
    "    collection_name: str,\n",
    "    gcs_bucket_name: str,\n",
    "    gcs_prefix_name: str,\n",
    "    csv_input_file: str,\n",
    "    project_id: str,\n",
    ") -> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"batch_predict_file_uri\", str),\n",
    "        (\"bp_inputs_count\", int),\n",
    "    ]\n",
    "):\n",
    "    \n",
    "    from datetime import datetime\n",
    "    import hashlib\n",
    "    from io import BytesIO\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from PIL import Image\n",
    "    import re\n",
    "    import requests\n",
    "    import shutil\n",
    "\n",
    "    from google.cloud import firestore\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def make_nice_filename(name, *, rows=None, cols=None):\n",
    "        regex = \"[\\s|\\(|\\\"|\\)]\"\n",
    "        new_name = re.sub(regex, \"_\", name)\n",
    "        new_name = new_name.lower()[:30]\n",
    "        new_name = new_name.replace(\"__\", \"_\")\n",
    "        \n",
    "        if rows is not None and cols is not None:\n",
    "            new_name += f\".{cols}x{rows}\"\n",
    "        return f\"{new_name}.jpg\"\n",
    "\n",
    "\n",
    "    def create_vtt_json(content, title):\n",
    "        img = Image.open(BytesIO(content))\n",
    "        w, h = img.size\n",
    "\n",
    "        dims = re.findall(\"\\d+x\\d+\", title)\n",
    "        if len(dims) is 0:\n",
    "            return None\n",
    "\n",
    "        dims = dims[0].split(\"x\")\n",
    "\n",
    "        if len(dims) is not 2:\n",
    "            return None\n",
    "\n",
    "        cols = int(dims[0])\n",
    "        rows = int(dims[1])\n",
    "\n",
    "        cell_w = w / rows\n",
    "        cell_h = h / cols\n",
    "        if cell_w != cell_h:\n",
    "            return None\n",
    "\n",
    "        return {\n",
    "            \"cols\": cols,\n",
    "            \"rows\": rows,\n",
    "            \"imageWidth\": w,\n",
    "            \"imageHeight\": h,\n",
    "            \"cellOffsetX\": 0,\n",
    "            'cellOffsetY': 0, \n",
    "            'cellWidth': cell_w, \n",
    "            'cellHeight': cell_h, \n",
    "        }\n",
    "\n",
    "    def compute_bboxes(vtt_data):\n",
    "        bboxes = []\n",
    "\n",
    "        cols = vtt_data[\"cols\"]\n",
    "        rows = vtt_data[\"rows\"]\n",
    "\n",
    "        for x in range(1, cols):\n",
    "            for y in range(1, rows):\n",
    "               x_min_tmp = vtt_data[\"cellOffsetX\"] + (vtt_data[\"cellWidth\"] * x) - 2\n",
    "               x_max_tmp = x_min_tmp + vtt_data[\"cellWidth\"] + 4\n",
    "               y_min_tmp = vtt_data[\"cellOffsetY\"] + (vtt_data[\"cellHeight\"] * y) - 2\n",
    "               y_max_tmp = y_min_tmp + vtt_data[\"cellHeight\"] + 4\n",
    "\n",
    "               x_min_train = x_min_tmp / vtt_data[\"imageWidth\"]\n",
    "               x_max_train = x_max_tmp / vtt_data[\"imageWidth\"]\n",
    "               y_min_train = y_min_tmp / vtt_data[\"imageHeight\"]\n",
    "               y_max_train = y_max_tmp / vtt_data[\"imageHeight\"]\n",
    "\n",
    "               bboxes.append({\n",
    "                   \"xMin\": x_min_train,\n",
    "                   \"yMin\": y_min_train,\n",
    "                   \"xMax\": x_max_train,\n",
    "                   \"yMax\": y_max_train,\n",
    "                   \"displayName\": \"cell\"\n",
    "               })\n",
    "\n",
    "        return bboxes\n",
    "\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(gcs_bucket_name)\n",
    "\n",
    "    firestore_client = firestore.Client(project=project_id)\n",
    "    collection_ref = firestore_client.collection(collection_name)\n",
    "\n",
    "    blob = bucket.blob(csv_input_file)\n",
    "    csv_bytes = blob.download_as_string()\n",
    "    csv_buffer = BytesIO(csv_bytes)\n",
    "\n",
    "    jpg_df = pd.read_csv(csv_buffer)\n",
    "\n",
    "    hashes = [None] * len(jpg_df.index)\n",
    "    jpg_df.insert(1, \"HashId\", hashes, True)\n",
    "    jpg_df.insert(6, \"GcsURI\", hashes, True)\n",
    "\n",
    "    # Concatenate string of batch prediction inputs\n",
    "    bp_inputs = \"\"\n",
    "    bp_inputs_count = 0\n",
    "    \n",
    "    # Iterate over JPG URIs, download them in batches, convert to sha values\n",
    "    for i, r in jpg_df.iterrows():\n",
    "        jpg_url = r[\"URL\"]\n",
    "        title = r[\"Title\"]\n",
    "\n",
    "        req = requests.get(jpg_url, stream=True)\n",
    "        if req.status_code == 200:\n",
    "            req.raw.decode_content = True\n",
    "            sha1 = hashlib.sha1()\n",
    "            jpg_hash = sha1.update(req.content)\n",
    "            jpg_hash = sha1.hexdigest()\n",
    "\n",
    "            jpg_df[\"HashId\"][i] = jpg_hash\n",
    "            #print(f\"Index {i}, hash {jpg_hash}\")\n",
    "            hashes.append(jpg_hash)\n",
    "\n",
    "            # Try to fetch each document from Firestore. If it does not exist,\n",
    "            # overwrite and download the image.\n",
    "            doc_ref = collection_ref.document(jpg_hash)\n",
    "            doc = doc_ref.get()\n",
    "            if not doc.exists:\n",
    "\n",
    "                img_data = create_vtt_json(req.content, title)\n",
    "                \n",
    "                if img_data is not None:\n",
    "                    file_name = make_nice_filename(title,\n",
    "                                                   rows=img_data[\"rows\"],\n",
    "                                                   cols=img_data[\"cols\"])\n",
    "                else:\n",
    "                    file_name = make_nice_filename(title)\n",
    "                \n",
    "                img_gcs_uri = f\"gs://{gcs_bucket_name}/{gcs_prefix_name}/{file_name}\"\n",
    "                blob_name = f\"{gcs_prefix_name}/{file_name}\"\n",
    "\n",
    "                file_blob = bucket.blob(blob_name)\n",
    "                image_buffer = BytesIO(req.content)\n",
    "\n",
    "                # Get image grid metadata\n",
    "                #img_data = create_vtt_json(req.content, title)\n",
    "                print(img_data)\n",
    "\n",
    "                file_blob.upload_from_file(BytesIO(req.content))\n",
    "\n",
    "                data = {\n",
    "                    u\"filename\": file_name,\n",
    "                    u\"gcsURI\": img_gcs_uri,\n",
    "                    u\"source\": gcs_prefix_name,\n",
    "                    u\"userId\": \"None\",\n",
    "                    u\"sourceUrl\": jpg_url,\n",
    "                }\n",
    "\n",
    "                if img_data is not None:\n",
    "                    bboxes = compute_bboxes(img_data)\n",
    "                    data[\"vtt\"] = img_data\n",
    "                    data[\"computedBBoxes\"] = bboxes\n",
    "\n",
    "                    doc_ref.set(data)\n",
    "                    print(f\"Set data: {data}\")\n",
    "                    bp_inputs += json.dumps({ \"content\": img_gcs_uri, \"mimeType\": \"image/jpeg\"})\n",
    "                    bp_inputs += \"\\n\"\n",
    "                    bp_inputs_count = bp_inputs_count + 1\n",
    "\n",
    "    # No fresh JPGs in this scraping; return empty string\n",
    "    if bp_inputs is \"\":\n",
    "        print(\"no inputs\")\n",
    "        return (\"\", 0)\n",
    "\n",
    "    print(f\"First ten: {jpg_df.head(10)}\")\n",
    "\n",
    "    # Save the batch_predict file\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\") \n",
    "    batch_predict_file_uri = f\"gs://{gcs_bucket_name}/{gcs_prefix_name}/bp_input_{timestamp}.jsonl\"\n",
    "\n",
    "    bp_blob_name = f\"{gcs_prefix_name}/bp_input_{timestamp}.jsonl\"\n",
    "    bp_blob = bucket.blob(bp_blob_name)\n",
    "\n",
    "    bp_blob.upload_from_string(bp_inputs)\n",
    "\n",
    "    return (batch_predict_file_uri, bp_inputs_count)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb86e8e",
   "metadata": {},
   "source": [
    "## Build a simple pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a33687d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"reddit-scraper-pipeline\",\n",
    "    description=\"Gets data from a subreddit\",\n",
    "    pipeline_root=f\"gs://{BUCKET}/pipeline_root\",\n",
    ")\n",
    "def reddit_pipeline(\n",
    "    collection_name: str = COLLECTION_NAME,\n",
    "    secret_name: str = \"reddit-api-key\",\n",
    "    subreddit_name: str = SUBREDDIT_NAME,\n",
    "    gcs_bucket: str = BUCKET,\n",
    "    gcs_prefix: str = GCS_PREFIX,\n",
    "    project_id: str = PROJECT_ID,\n",
    "    location: str = LOCATION,\n",
    "    limit: int = LIMIT,\n",
    "):\n",
    "    \n",
    "    # Get the images from Reddit\n",
    "    reddit_op_1 = reddit(\n",
    "        secret_name=secret_name,\n",
    "        subreddit_name=subreddit_name,\n",
    "        gcs_bucket_name=gcs_bucket,\n",
    "        gcs_prefix_name=gcs_prefix,\n",
    "        project_id=project_id,\n",
    "        limit=limit,\n",
    "    )\n",
    "    \n",
    "    reddit_csv_file_1 = reddit_op_1.output\n",
    "    \n",
    "    # Firestore operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f32db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=reddit_pipeline, package_path=\"artifacts/reddit_scraper_pipeline_job.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff782324",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=LOCATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40e59c5-f05a-4959-919c-1b3f3d2a2e7d",
   "metadata": {},
   "source": [
    "When we run the pipeline, we don't want it to cache the pipeline, since caching the pipeline will likely result in producing the exact same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf97982c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/reddit-scraper-pipeline-20221215000136?project=fantasymaps-334622\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    job_spec_path=\"artifacts/reddit_scraper_pipeline_job.json\",\n",
    "    enable_caching=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f49503-4709-44fa-b1b6-4eba3e6ae735",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
