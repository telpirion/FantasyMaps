{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/telpirion/FantasyMaps/blob/main/notebooks/final/1_firestore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "id": "view-in-github"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67741dbe-06db-4f5c-baf9-b75416d69539",
      "metadata": {
        "tags": [],
        "id": "67741dbe-06db-4f5c-baf9-b75416d69539"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8489ffb-e3a6-4cb6-883a-547afd8eac82",
      "metadata": {
        "id": "c8489ffb-e3a6-4cb6-883a-547afd8eac82"
      },
      "source": [
        "# Storing training data in Firestore\n",
        "\n",
        "{TODO: Update the links below.}\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/notebook_template.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/notebook_template.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fffca17d-1158-4ce0-ad90-f8b869358df5",
      "metadata": {
        "id": "fffca17d-1158-4ce0-ad90-f8b869358df5"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates how to collect data from social media (e.g. Reddit), preprocess that data, and then store that data in a Firestore collection. In this scenario, you collect fictional, hand-drawn maps that are used for virtual role-playing games. (Later, you convert these maps into training data for a Vertex AI notebook.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c569a85-e173-4c11-9988-7712fb0ac26f",
      "metadata": {
        "tags": [],
        "id": "2c569a85-e173-4c11-9988-7712fb0ac26f"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you learn how to create a dataset in Cloud Storage and Firestore from a third-party API.\n",
        "\n",
        "This tutorial uses the following Google Cloud resources:\n",
        "\n",
        "+ Cloud Storage bucket\n",
        "+ Firestore collection\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "1. Collecting images from Reddit\n",
        "1. Storing the images in Cloud Storage\n",
        "1. Inferring training data from the images\n",
        "1. Storing that training data in a Firestore collection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "597098cf-1b89-4466-92d1-6c9434594677",
      "metadata": {
        "id": "597098cf-1b89-4466-92d1-6c9434594677"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "In this tutorial, you collect data from subreddit posts on Reddit. The individual posts are processed and their metadata stored in Firestore. Any image data the posts contain are extracted and stored in a Storage bucket."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2d14d80-853b-40cf-883a-9d762c082a08",
      "metadata": {
        "id": "d2d14d80-853b-40cf-883a-9d762c082a08"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Firestore\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Firestore pricing](https://cloud.google.com/firestore/pricing),\n",
        "and [Cloud Storage pricing](https://cloud.google.com/storage/pricing),\n",
        "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0aad14a-3e5d-4a5f-a56e-9a91a8c4192a",
      "metadata": {
        "id": "c0aad14a-3e5d-4a5f-a56e-9a91a8c4192a"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Install the following packages required to execute this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "237e0e69-b281-4683-bc2b-570e0ffeb713",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "237e0e69-b281-4683-bc2b-570e0ffeb713",
        "outputId": "712e8a3e-b8cc-488b-d60c-d101d96d235d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726690216539,
          "user_tz": 420,
          "elapsed": 92,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements.txt\n",
        "google-cloud-firestore\n",
        "google-cloud-secret-manager\n",
        "google-cloud-storage\n",
        "praw\n",
        "pandas\n",
        "numpy\n",
        "spacy\n",
        "pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "5e29f548-6bce-4b71-9a6c-244fe2ebd28f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e29f548-6bce-4b71-9a6c-244fe2ebd28f",
        "outputId": "003e963a-f17b-4d0e-b013-f6b2aa2b31c0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726690252817,
          "user_tz": 420,
          "elapsed": 31028,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.2/326.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.6/184.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.1.4, but you have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install --upgrade -qr requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "235a373b-c3ca-41e5-9a48-e7ac13c2ff5f",
      "metadata": {
        "id": "235a373b-c3ca-41e5-9a48-e7ac13c2ff5f"
      },
      "source": [
        "We will also use a simple natural language parsing library to analyze posts. For this use case, we'll use the open source library [spaCy](https://spacy.io). spaCy requires that a language model be downloaded before it can be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3ec6dea5-e36d-495f-b765-3cf6949136b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ec6dea5-e36d-495f-b765-3cf6949136b9",
        "outputId": "12431226-0d4d-423b-8e58-c856c845fddc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726690278351,
          "user_tz": 420,
          "elapsed": 11308,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.18)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.5.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have prepared a library of functions to help you process images."
      ],
      "metadata": {
        "id": "F809nXQ-Bbkk"
      },
      "id": "F809nXQ-Bbkk"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip uninstall -y fantasy_maps_lib\n",
        "! pip install \"git+https://github.com/telpirion/fantasy-maps-lib.git#egg=fantasy_maps_lib\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhvoZqO0mr6Q",
        "outputId": "1d260b5f-d217-454e-ff64-ba8a1b763433",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726690297124,
          "user_tz": 420,
          "elapsed": 14419,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "dhvoZqO0mr6Q",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping fantasy_maps_lib as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting fantasy_maps_lib\n",
            "  Cloning https://github.com/telpirion/fantasy-maps-lib.git to /tmp/pip-install-1207w0to/fantasy-maps-lib_992463cea1e94686922e9fa17da42a92\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/telpirion/fantasy-maps-lib.git /tmp/pip-install-1207w0to/fantasy-maps-lib_992463cea1e94686922e9fa17da42a92\n",
            "  Resolved https://github.com/telpirion/fantasy-maps-lib.git to commit 71c9abc6dec9f44a001ef9c89fac7f9336358275\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting black (from fantasy_maps_lib)\n",
            "  Downloading black-24.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.10/dist-packages (from fantasy_maps_lib) (1.65.0)\n",
            "Requirement already satisfied: google-cloud-firestore in /usr/local/lib/python3.10/dist-packages (from fantasy_maps_lib) (2.18.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (from fantasy_maps_lib) (2.18.2)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.10/dist-packages (from fantasy_maps_lib) (0.4.0)\n",
            "Collecting jsonlines (from fantasy_maps_lib)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fantasy_maps_lib) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fantasy_maps_lib) (2.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from fantasy_maps_lib) (10.4.0)\n",
            "Requirement already satisfied: praw in /usr/local/lib/python3.10/dist-packages (from fantasy_maps_lib) (7.7.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from fantasy_maps_lib) (7.4.4)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (from fantasy_maps_lib) (3.7.6)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->fantasy_maps_lib) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black->fantasy_maps_lib)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black->fantasy_maps_lib) (24.1)\n",
            "Collecting pathspec>=0.9.0 (from black->fantasy_maps_lib)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->fantasy_maps_lib) (4.3.2)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->fantasy_maps_lib) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black->fantasy_maps_lib) (4.12.2)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform->fantasy_maps_lib) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->fantasy_maps_lib) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->fantasy_maps_lib) (1.24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->fantasy_maps_lib) (3.20.3)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->fantasy_maps_lib) (3.25.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->fantasy_maps_lib) (1.12.5)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->fantasy_maps_lib) (2.0.6)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->fantasy_maps_lib) (1.10.18)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform->fantasy_maps_lib) (0.16)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->fantasy_maps_lib) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->fantasy_maps_lib) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->fantasy_maps_lib) (2.32.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage->fantasy_maps_lib) (1.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug->fantasy_maps_lib) (1.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from imgaug->fantasy_maps_lib) (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from imgaug->fantasy_maps_lib) (3.7.1)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.10/dist-packages (from imgaug->fantasy_maps_lib) (0.23.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from imgaug->fantasy_maps_lib) (4.10.0.84)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from imgaug->fantasy_maps_lib) (2.34.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->fantasy_maps_lib) (24.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->fantasy_maps_lib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fantasy_maps_lib) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->fantasy_maps_lib) (2024.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from praw->fantasy_maps_lib) (2.4.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw->fantasy_maps_lib) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw->fantasy_maps_lib) (1.8.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->fantasy_maps_lib) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->fantasy_maps_lib) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->fantasy_maps_lib) (1.2.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (4.66.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (69.5.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy->fantasy_maps_lib) (3.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform->fantasy_maps_lib) (1.65.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform->fantasy_maps_lib) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform->fantasy_maps_lib) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform->fantasy_maps_lib) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform->fantasy_maps_lib) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform->fantasy_maps_lib) (4.9)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform->fantasy_maps_lib) (0.13.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy->fantasy_maps_lib) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->fantasy_maps_lib) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->fantasy_maps_lib) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->fantasy_maps_lib) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage->fantasy_maps_lib) (2024.8.30)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug->fantasy_maps_lib) (3.3)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug->fantasy_maps_lib) (2024.8.30)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.14.2->imgaug->fantasy_maps_lib) (0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->fantasy_maps_lib) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy->fantasy_maps_lib) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->fantasy_maps_lib) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy->fantasy_maps_lib) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->fantasy_maps_lib) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy->fantasy_maps_lib) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy->fantasy_maps_lib) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->fantasy_maps_lib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->fantasy_maps_lib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->fantasy_maps_lib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->fantasy_maps_lib) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->imgaug->fantasy_maps_lib) (3.1.4)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->fantasy_maps_lib) (1.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform->fantasy_maps_lib) (0.6.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->fantasy_maps_lib) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->fantasy_maps_lib) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->fantasy_maps_lib) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->fantasy_maps_lib) (0.1.2)\n",
            "Downloading black-24.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Building wheels for collected packages: fantasy_maps_lib\n",
            "  Building wheel for fantasy_maps_lib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fantasy_maps_lib: filename=fantasy_maps_lib-0.0.0-py3-none-any.whl size=17158 sha256=35b686da5d7c14e15d13d49efc260682c715da1f8032edd22b51d55005c98d99\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5pf8ynaf/wheels/95/85/a3/5ea513fa633d19614cc7ba22d8012afc8589ee6ebd9ddbf078\n",
            "Successfully built fantasy_maps_lib\n",
            "Installing collected packages: pathspec, mypy-extensions, jsonlines, black, fantasy_maps_lib\n",
            "Successfully installed black-24.8.0 fantasy_maps_lib-0.0.0 jsonlines-4.0.0 mypy-extensions-1.0.0 pathspec-0.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47b82e4a-6b1e-4efb-85f4-5eb2b64f8eaf",
      "metadata": {
        "id": "47b82e4a-6b1e-4efb-85f4-5eb2b64f8eaf"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "3. [Enable the Firestore API](https://console.cloud.google.com/flows/enableapi?apiid=firestore.googleapis.com).\n",
        "\n",
        "4. [Enable the Secret Manager API](https://console.cloud.google.com/flows/enableapi?apiid=secretmanager.googleapis.com).\n",
        "\n",
        "5. [Enable the Cloud Storage API](https://console.cloud.google.com/flows/enableapi?apiid=storage.googleapis.com).\n",
        "\n",
        "6. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "627dd1f8-9d0d-4c53-be6f-bc3a80d88cf2",
      "metadata": {
        "id": "627dd1f8-9d0d-4c53-be6f-bc3a80d88cf2"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If using Colab secrets, import them here\n",
        "from google.colab import userdata\n",
        "PROJECT_ID = userdata.get('PROJECT_ID')\n",
        "\n",
        "! gcloud config set project {PROJECT_ID}"
      ],
      "metadata": {
        "id": "kJwbpxfnl9Qg"
      },
      "id": "kJwbpxfnl9Qg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If running in Vertex Workbench or Colab Enterprise, set your project here\n",
        "PROJECT_ID = !gcloud config get-value project\n",
        "PROJECT_ID = PROJECT_ID[0]\n",
        "print(PROJECT_ID)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6HjEkHRE9mk",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726690314267,
          "user_tz": 420,
          "elapsed": 819,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "d1a53a41-a0e2-4695-8528-36a4ed4d94d7"
      },
      "id": "N6HjEkHRE9mk",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fantasymaps-334622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd84908-871b-4f8c-b586-3814514eac00",
      "metadata": {
        "id": "ddd84908-871b-4f8c-b586-3814514eac00"
      },
      "outputs": [],
      "source": [
        "# If running in some other Jupyter notebook installation, provide your\n",
        "# Google Cloud project name\n",
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99eb88ba-b0fe-4989-99b7-33be554958a0",
      "metadata": {
        "id": "99eb88ba-b0fe-4989-99b7-33be554958a0"
      },
      "source": [
        "#### Region\n",
        "\n",
        "You can also change the `REGION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "6181b969-2184-4ea8-b42d-4f6377a91d5f",
      "metadata": {
        "id": "6181b969-2184-4ea8-b42d-4f6377a91d5f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726690357985,
          "user_tz": 420,
          "elapsed": 89,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "REGION = \"us-west1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0455b922-0c10-4d2d-8861-3699c6ebd913",
      "metadata": {
        "id": "0455b922-0c10-4d2d-8861-3699c6ebd913"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46ba334b-e9b9-48fb-9ded-090bcf268cca",
      "metadata": {
        "id": "46ba334b-e9b9-48fb-9ded-090bcf268cca"
      },
      "source": [
        "**1. Vertex AI Workbench**\n",
        "* Do nothing as you are already authenticated."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1916caf9-48d1-4fe1-b42b-fd2fe2829200",
      "metadata": {
        "id": "1916caf9-48d1-4fe1-b42b-fd2fe2829200"
      },
      "source": [
        "**2. Local JupyterLab instance, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ac273d9-7f88-407e-9fe3-291bf2f0f5fb",
      "metadata": {
        "id": "6ac273d9-7f88-407e-9fe3-291bf2f0f5fb"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afd0c21b-fef6-44eb-8a8a-6a58140ff4f4",
      "metadata": {
        "id": "afd0c21b-fef6-44eb-8a8a-6a58140ff4f4"
      },
      "source": [
        "**3. Colab, uncomment and run:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57c6eb8f-f518-41a9-ae87-1558fe62eb8f",
      "metadata": {
        "id": "57c6eb8f-f518-41a9-ae87-1558fe62eb8f"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c550d1b8-0e7c-4b85-9c01-8bb8906c2287",
      "metadata": {
        "id": "c550d1b8-0e7c-4b85-9c01-8bb8906c2287"
      },
      "source": [
        "**4. Service account or other**\n",
        "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "090dd331-0bd2-421b-bed5-8e8e4d7d8592",
      "metadata": {
        "id": "090dd331-0bd2-421b-bed5-8e8e4d7d8592"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "c9f2e97b-17d4-493d-a643-ef8670c95e1a",
      "metadata": {
        "id": "c9f2e97b-17d4-493d-a643-ef8670c95e1a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726690374663,
          "user_tz": 420,
          "elapsed": 93,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = f\"gs://fantasy-maps-{PROJECT_ID}-unique\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ad6a6be-446c-440f-ae91-02066b13d34e",
      "metadata": {
        "id": "2ad6a6be-446c-440f-ae91-02066b13d34e"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77871323-0824-49c2-8c4c-fa0af8a67f4e",
      "metadata": {
        "id": "77871323-0824-49c2-8c4c-fa0af8a67f4e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efcd60f6-6872-48b1-8a64-a47f14bf3064",
      "metadata": {
        "id": "efcd60f6-6872-48b1-8a64-a47f14bf3064",
        "tags": []
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "34174e56-8d65-4e5a-9046-d74625090cc5",
      "metadata": {
        "id": "34174e56-8d65-4e5a-9046-d74625090cc5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726690383354,
          "user_tz": 420,
          "elapsed": 4166,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "from google.cloud import firestore\n",
        "from google.cloud import secretmanager\n",
        "from google.cloud import storage\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import hashlib\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pprint\n",
        "import praw\n",
        "import re\n",
        "import requests\n",
        "import shutil\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "a8a0f2bb-849a-4ab1-a48c-adeca3906509",
      "metadata": {
        "id": "a8a0f2bb-849a-4ab1-a48c-adeca3906509",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726690384813,
          "user_tz": 420,
          "elapsed": 101,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "from fantasy_maps.reddit import posts\n",
        "from fantasy_maps.image import extract, shards\n",
        "from fantasy_maps.image.image_metadata import ImageMetadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "854f52c5-8e0e-4a46-af45-86914c745f7f",
      "metadata": {
        "id": "854f52c5-8e0e-4a46-af45-86914c745f7f"
      },
      "source": [
        "### Get a Reddit API key\n",
        "\n",
        "You need a [Reddit API key](https://www.reddit.com/wiki/api/) to access Reddit programmatically. Copy your API key into the labeled fields of the dictionary in the following cell.\n",
        "\n",
        "**Note**: Once you have an API key, you must store it somewhere safe. It is recommended to store your API key as a JSON-formatted string in Cloud Secret Manager."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SECRET_RESOURCE_NAME = f\"projects/{PROJECT_ID}/secrets/reddit-api-key/versions/1\" # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "gmsje-JHslz8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726690388179,
          "user_tz": 420,
          "elapsed": 88,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "gmsje-JHslz8",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "secret_client = secretmanager.SecretManagerServiceClient()\n",
        "secret = secret_client.access_secret_version(name=SECRET_RESOURCE_NAME)\n",
        "reddit_key_json = json.loads(secret.payload.data)\n",
        "print(reddit_key_json)"
      ],
      "metadata": {
        "id": "gWjJGFtRrvw6",
        "outputId": "fa800a54-b5f0-40a0-856d-1d2c84c21d9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726690390710,
          "user_tz": 420,
          "elapsed": 926,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "gWjJGFtRrvw6",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'secret': '_XDRI2jgcVAJ6xKIWmA46yz8CZw', 'client_id': 'Z0g7xbmKNB9Mew', 'user_agent': 'script:ScrapeForNLP:v1.0 (by u/Telpirion-78)', 'user_name': 'Telpirion-78'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67219889-78bf-429a-b772-2f95a9904261",
      "metadata": {
        "id": "67219889-78bf-429a-b772-2f95a9904261"
      },
      "source": [
        "## Query data (posts) on Reddit\n",
        "\n",
        "Now that we have our API key ready for use, we can query Reddit for our data! In the next cell, we will read the top 100 \"hot\" posts from a subreddit.\n",
        "\n",
        "For our use-case, we want to check the posts to see whether: 1) they have an image associated with them; and 2) the title gives us some clues as to the contents (e.g. columns and rows) contained in the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "9dc26f93-3bcb-4737-9b92-a798938ede19",
      "metadata": {
        "id": "9dc26f93-3bcb-4737-9b92-a798938ede19",
        "outputId": "3b6bbf95-a814-485b-e75d-b482f58cb0aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726690402182,
          "user_tz": 420,
          "elapsed": 2487,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "columns = ['Title', 'Post', 'ID', 'URL']\n",
        "\n",
        "subreddit_name = \"battlemaps\"\n",
        "reddit_posts = posts.get_reddit_posts(reddit_credentials=reddit_key_json,\n",
        "                                subreddit_name=subreddit_name, limit=100)\n",
        "reddit_dicts = posts.convert_posts_to_dicts(reddit_posts, columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dbd67c7-d1e8-404f-89d9-f2540ea7ace9",
      "metadata": {
        "id": "4dbd67c7-d1e8-404f-89d9-f2540ea7ace9"
      },
      "source": [
        "Now that we have the top 100 \"hot\" posts from the subreddit, we're going to filter for only the posts that we want. Again, our criteria are: 1) must have an image; 2) the title must have the grid dimensions of the image.\n",
        "\n",
        "We'll use Pandas to visualize the resulting data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_images = [\n",
        "  ImageMetadata(\n",
        "      url=d['URL'],\n",
        "      title=d['Title'],\n",
        "      rid=d['ID']) for d in reddit_dicts if (\n",
        "    'jpeg' in d['URL'] and\n",
        "    re.search(r'\\d+x\\d+', d['Title'])\n",
        "  )\n",
        "]\n",
        "print(len(reddit_images))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5hVfhOyH9Nm",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726690406736,
          "user_tz": 420,
          "elapsed": 98,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "36600bad-3916-4ec6-8ad7-6dd23c88d03c"
      },
      "id": "Q5hVfhOyH9Nm",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25212aba-89e7-4e7d-95f3-4ba5beecd476",
      "metadata": {
        "id": "25212aba-89e7-4e7d-95f3-4ba5beecd476"
      },
      "source": [
        "## Process the images and their metadata\n",
        "\n",
        "In this next step, we will process all of the Reddit posts with images posts:\n",
        "\n",
        "1. Download the image itself\n",
        "2. Parse each image's metadata\n",
        "3. (If needed) Split the image into smaller images\n",
        "4. Store the metadata in Firestore\n",
        "5. Save the images on Cloud Storage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7326eee-5514-4521-938c-3637ff3dcb3c",
      "metadata": {
        "id": "e7326eee-5514-4521-938c-3637ff3dcb3c"
      },
      "source": [
        "### Download the image\n",
        "\n",
        "We're going to download the image locally. We'll need a meaningful filename to save the image. We also need a directory system to save the image to.\n",
        "\n",
        "We also want to avoid downloading the same image more than once. We'll need to compare the images programmatically to verify that each image is unique.\n",
        "\n",
        "The easiest way to do this will be to reduce each image to a unique hash value and then ensure that we never have two copies of the same hash value. For the sake of simplicity, we'll use these hash values as the unique ID for each image."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If using Vertex Workbench or a local Jupyter notebook, run this cell.\n",
        "root_dir = os.getcwd()"
      ],
      "metadata": {
        "id": "E9Tbz7hwTtaC"
      },
      "id": "E9Tbz7hwTtaC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If using Colab (not Colab Enterprise), mount Google Drive\n",
        "root_dir = '/content/drive'\n",
        "from google.colab import drive\n",
        "drive.mount(root_dir)"
      ],
      "metadata": {
        "id": "YdPYHDL3_QMV"
      },
      "id": "YdPYHDL3_QMV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If using Colab Enterprise, use gcsfuse\n",
        "root_dir = '/content/drive'\n",
        "\n",
        "!echo \"deb https://packages.cloud.google.com/apt gcsfuse-`lsb_release -c -s` main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "!apt -qq update && apt -qq install gcsfuse\n",
        "!gcsfuse --implicit-dirs $BUCKET_URI $root_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg300dsBujw6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726689779489,
          "user_tz": 420,
          "elapsed": 11828,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "0b3d946c-d253-4240-b0c0-4829de9e84bc"
      },
      "id": "Zg300dsBujw6",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deb https://packages.cloud.google.com/apt gcsfuse-jammy main\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "100  1021  100  1021    0     0  12527      0 --:--:-- --:--:-- --:--:-- 12604\n",
            "OK\n",
            "75 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mhttps://packages.cloud.google.com/apt/dists/gcsfuse-jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\u001b[0m\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "The following NEW packages will be installed:\n",
            "  gcsfuse\n",
            "0 upgraded, 1 newly installed, 0 to remove and 75 not upgraded.\n",
            "Need to get 8,321 kB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Selecting previously unselected package gcsfuse.\n",
            "(Reading database ... 123694 files and directories currently installed.)\n",
            "Preparing to unpack .../gcsfuse_2.4.1_amd64.deb ...\n",
            "Unpacking gcsfuse (2.4.1) ...\n",
            "Setting up gcsfuse (2.4.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUCKET_NAME = BUCKET_URI.replace('gs://', '')\n",
        "root_dir = '/gcs/content/'\n",
        "\n",
        "if not os.path.exists(root_dir):\n",
        "  os.makedirs(root_dir, exist_ok=True)\n",
        "\n",
        "!gcsfuse --implicit-dirs {BUCKET_NAME} {root_dir}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_KbkBA2vvq7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726690568252,
          "user_tz": 420,
          "elapsed": 262,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "10dcd4cf-7cc8-4c47-a375-28c7630d69e4"
      },
      "id": "g_KbkBA2vvq7",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"timestamp\":{\"seconds\":1726690568,\"nanos\":144119077},\"severity\":\"INFO\",\"message\":\"Start gcsfuse/2.4.1 (Go version go1.23.0) for app \\\"\\\" using mount point: /gcs/content/\\n\"}\n",
            "{\"timestamp\":{\"seconds\":1726690568,\"nanos\":144163329},\"severity\":\"INFO\",\"message\":\"GCSFuse config\",\"config\":{\"debug\":{},\"file-cache\":{\"download-chunk-size-mb\":50,\"max-parallel-downloads\":16,\"max-size-mb\":-1,\"parallel-downloads-per-file\":16,\"write-buffer-size\":4194304},\"file-system\":{\"dir-mode\":\"755\",\"file-mode\":\"644\",\"gid\":-1,\"ignore-interrupts\":true,\"uid\":-1},\"gcs-auth\":{\"reuse-token-from-url\":true},\"gcs-connection\":{\"client-protocol\":\"http1\",\"grpc-conn-pool-size\":1,\"limit-bytes-per-sec\":-1,\"limit-ops-per-sec\":-1,\"max-idle-conns-per-host\":100,\"sequential-read-size-mb\":200},\"gcs-retries\":{\"max-retry-sleep\":30000000000,\"multiplier\":2},\"implicit-dirs\":true,\"list\":{},\"logging\":{\"log-rotate\":{\"backup-file-count\":10,\"compress\":true,\"max-file-size-mb\":512},\"severity\":\"INFO\"},\"metadata-cache\":{\"deprecated-stat-cache-capacity\":20460,\"deprecated-stat-cache-ttl\":60000000000,\"deprecated-type-cache-ttl\":60000000000,\"experimental-metadata-prefetch-on-mount\":\"disabled\",\"stat-cache-max-size-mb\":32,\"ttl-secs\":60,\"type-cache-max-size-mb\":4},\"metrics\":{},\"monitoring\":{},\"write\":{}}}\n",
            "{\"timestamp\":{\"seconds\":1726690568,\"nanos\":238451687},\"severity\":\"INFO\",\"message\":\"File system has been successfully mounted.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "f3844e20",
      "metadata": {
        "id": "f3844e20",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726690675087,
          "user_tz": 420,
          "elapsed": 42259,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "local_reddit_data_dir = f'{root_dir}reddit_maps_data'\n",
        "\n",
        "if os.path.exists(local_reddit_data_dir):\n",
        "  shutil.rmtree(local_reddit_data_dir)\n",
        "\n",
        "os.mkdir(local_reddit_data_dir)\n",
        "\n",
        "hashes = []\n",
        "\n",
        "for post in reddit_images:\n",
        "    url = post.url\n",
        "    filename = posts.make_nice_filename(post.title)\n",
        "\n",
        "    if filename == '':\n",
        "        continue\n",
        "\n",
        "    path = f'{local_reddit_data_dir}/{filename}'\n",
        "    uid = extract.download_image_local(url=url, path=path)\n",
        "    if uid in hashes:\n",
        "      continue\n",
        "\n",
        "    hashes.append(uid)\n",
        "\n",
        "    post.path = path\n",
        "    post.uid = uid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pprint.pprint(reddit_images[0].to_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bCMuF8i2wX0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1726690721834,
          "user_tz": 420,
          "elapsed": 97,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "8e440c11-1236-4697-ff06-29d1a656a045"
      },
      "id": "7bCMuF8i2wX0",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bboxes': [],\n",
            " 'cell_height': 0,\n",
            " 'cell_offset_x': 0,\n",
            " 'cell_offset_y': 0,\n",
            " 'cell_width': 0,\n",
            " 'columns': 1,\n",
            " 'height': 0,\n",
            " 'path': '/gcs/content/reddit_maps_data/plane_players_portal_brand_new_czepeku.31x60.jpg',\n",
            " 'rid': '1fjqahc',\n",
            " 'rows': 1,\n",
            " 'title': 'Which plane will your players visit through the portal? - Brand New '\n",
            "          'Czepeku Scene/Battlemap Combo! [31x60]',\n",
            " 'uid': 'c60cc1bafc2e1e94e34dbdd05f8c67203cefd011',\n",
            " 'url': 'https://i.redd.it/885viw3mxjpd1.jpeg',\n",
            " 'width': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73d6641d-c090-47f1-9a2c-6da98f183299",
      "metadata": {
        "id": "73d6641d-c090-47f1-9a2c-6da98f183299"
      },
      "source": [
        "### Parse the image for metadata\n",
        "\n",
        "All we know from the Reddit API is that these posts have JPGs associated with them and that they contain a substring in the format \"NNxNN.\" However, we need more than just images and rough columns and rows for training a Vertex AI AutoML image object detection model. We would even need more data just to use these images in a VTT app.\n",
        "\n",
        "To get valid training data and VTT data, we need to make some inferences about the images based upon the data that we have (or can get). The data we have are the number of columns and rows stated in the post's title (granted, these are not always accurate). The data we can get is the image's width and height. From these four data points, and assuming that they are accurate and that all cells in the map are uniform, we can infer the width and height of cells in the map.\n",
        "\n",
        "Using the cell width and height, we can compute the rest of the data required for both an image object detection model and the data needed for a VTT app."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dc0a53b-311a-4837-9213-b4049da61e4c",
      "metadata": {
        "id": "6dc0a53b-311a-4837-9213-b4049da61e4c"
      },
      "source": [
        "#### Virtual tabletop (VTT) data\n",
        "\n",
        "The easiest for us to compute is the VTT data. This data provides us with the `cellWidth` and `cellHeight` data that will allow us to complete the ML training data.\n",
        "\n",
        "The JSON structure of VTT data is:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"imageHeight\": ##,\n",
        "    \"imageWidth\": ##,\n",
        "    \"cellHeight\": ##,\n",
        "    \"cellWidth\": ##,\n",
        "    \"cellOffsetY\": ##,\n",
        "    \"cellOffsetX\": ##\n",
        "}\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd38762b-ea37-4c1a-99b8-2b3022dd4f27",
      "metadata": {
        "id": "cd38762b-ea37-4c1a-99b8-2b3022dd4f27"
      },
      "source": [
        "#### Image object detection training data\n",
        "\n",
        "AutoML image object detection on Vertex AI requires a JSONL file with information for the training data. Each line in the JSONL file needs to contain: the Cloud Storage URI of the image; and the bounding boxes of the objects (cells) that we want to train the model to identify on the image.\n",
        "\n",
        "**Tip**: The format we use for training data is called the [Common Objects in Context (COCO)](https://cocodataset.org/#format-data), with some notable exceptions. The canonical example expresses the maximum values as `width` and `height`, where as we will annotate our data as `x` and `y` values expressed as a percentage of the total height and widthy of the image.\n",
        "\n",
        "The structure of the JSON data in the JSONL file is:\n",
        "\n",
        "```json\n",
        "{\n",
        "    \"imageGcsUri\": \"URI\",\n",
        "    \"boundingBoxAnnotations\": {\n",
        "        \"displayName\": \"LABEL_NAME\",\n",
        "        \"xMin\": ##,\n",
        "        \"xMax\": ##,\n",
        "        \"yMin\": ##,\n",
        "        \"yMax\": ##,\n",
        "    }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each bounding box, we need to provide a percentage value that expresses the vertices of the bounding box as a set of x and y pairs. Also, each bounding box needs to be given a label for that bounding box; all of our bounding boxes are \"cells\" so each one gets the label `cell`.\n",
        "\n",
        "**Note**: A training image in Vertex AI can only have at most 500 bounding boxes; many fantasy maps have many more than 500 cells. So that we can use the most of the training data, we will split too large images until smaller images, or \"shards\", and use the shards  for training. We'll create the shards a bit later.\n",
        "\n",
        "You can read more about how to format your training manifest JSONL file in [the documentation](https://cloud.google.com/vertex-ai/docs/datasets/prepare-image#object-detection)."
      ],
      "metadata": {
        "id": "0QnWvAZrkDLE"
      },
      "id": "0QnWvAZrkDLE"
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "test_path = '/gcs/content/reddit_maps_data/plane_players_portal_brand_new_czepeku.31x60.jpg'\n",
        "with open(test_path, 'rb') as f:\n",
        "    img_bytes = f.read()\n",
        "    img_size = sys.getsizeof(img_bytes)\n",
        "    print(img_size)\n",
        "    img = Image.frombytes(mode='YCbCr', size=img_size, data=img_bytes)\n",
        "    img.show()\n",
        "\n",
        "#img = Image.open(test_path)\n",
        "#img.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "fiZv5I-P46F7",
        "executionInfo": {
          "status": "error",
          "timestamp": 1726693125563,
          "user_tz": 420,
          "elapsed": 200,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "8c236b02-5d36-4dd4-82ba-57e235ee9119"
      },
      "id": "fiZv5I-P46F7",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1547651\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Size must be a tuple",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-81e0920f755e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimg_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsizeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'YCbCr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombytes\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2957\u001b[0m         \"\"\"\n\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2959\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2960\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36m_check_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2883\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAFFINE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2884\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2885\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2886\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPERSPECTIVE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2887\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Size must be a tuple"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "a569b0c0-59ef-4fe5-9262-d08a0150f6a6",
      "metadata": {
        "tags": [],
        "id": "a569b0c0-59ef-4fe5-9262-d08a0150f6a6",
        "outputId": "4d88a952-14d3-4802-d8ac-54454f3e9bf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "executionInfo": {
          "status": "error",
          "timestamp": 1726690771251,
          "user_tz": 420,
          "elapsed": 89,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnidentifiedImageError",
          "evalue": "cannot identify image file '/gcs/content/reddit_maps_data/plane_players_portal_brand_new_czepeku.31x60.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-d425142f883d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Get width & height for original image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_width_and_height\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fantasy_maps/image/extract.py\u001b[0m in \u001b[0;36mget_image_width_and_height\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \"\"\"\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3282\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3283\u001b[0;31m         \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"L\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"I\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"F\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3285\u001b[0m         \u001b[0mndmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file '/gcs/content/reddit_maps_data/plane_players_portal_brand_new_czepeku.31x60.jpg'"
          ]
        }
      ],
      "source": [
        "for post in reddit_images:\n",
        "    if not os.path.exists(post.path):\n",
        "      continue\n",
        "\n",
        "    local_path = post.path\n",
        "\n",
        "    # Get width & height for original image\n",
        "    w, h = extract.get_image_width_and_height(local_path)\n",
        "\n",
        "    post.width = w\n",
        "    post.height = h\n",
        "\n",
        "    # Get columns & rows for original image, based upon the name.\n",
        "    paths = local_path.split(\".\")\n",
        "    dims = paths[-2]\n",
        "    cols, rows = dims.split(\"x\")\n",
        "\n",
        "    cols = int(cols)\n",
        "    rows = int(rows)\n",
        "\n",
        "    post.columns = cols\n",
        "    post.rows = rows\n",
        "\n",
        "    # If image doesn't need to be sharded, simply compute and continue\n",
        "    if (cols * rows) <= 500:\n",
        "        bboxes = extract.compute_bboxes(img_metadata=post)\n",
        "        post.bboxes = bboxes\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SHARDS=3\n",
        "SHARD_COLS=15\n",
        "SHARD_ROWS=15\n",
        "    # Compute the number of shards\n",
        "    img_metadata = ImageMetadata(path=local_path,\n",
        "                                 width=w,\n",
        "                                 height=h,\n",
        "                                 columns=cols,\n",
        "                                 rows=rows,\n",
        "                                 cell_width=vtt[\"cellWidth\"],\n",
        "                                 cell_height=vtt[\"cellHeight\"])\n",
        "    shards = shards.compute_shard_coordinates(img_metadata=img_metadata,\n",
        "                                              num_shards=NUM_SHARDS,\n",
        "                                              shard_cols=SHARD_COLS,\n",
        "                                              shard_rows=SHARD_ROWS)\n",
        "    print(row)\n",
        "    print(len(shards))\n",
        "    for shard in shards:\n",
        "        print(local_path)\n",
        "        print(shard)\n",
        "        shard_df = extract.create_shard(x_min=shard[0], y_min=shard[1], x_max=shard[2],\n",
        "                                   y_max=shard[3], cols=shard[4], rows=shard[5],\n",
        "                                   img_path=local_path, parent_id=row[\"UID\"])\n",
        "\n",
        "        if shard_df is None:\n",
        "            continue\n",
        "\n",
        "        s_vtt = vtt\n",
        "        s_vtt[\"width\"] = int(shard_df.iloc[0][\"Width\"])\n",
        "        s_vtt[\"height\"] = int(shard_df.iloc[0][\"Height\"])\n",
        "        shard_df.at[0, \"VTT\"] = json.dumps(s_vtt)\n",
        "\n",
        "        bboxes = extract.compute_bboxes(dataframe=shard_df,\n",
        "                                cell_width=vtt[\"cellWidth\"],\n",
        "                                cell_height=vtt[\"cellHeight\"])\n",
        "\n",
        "        shard_df.at[0, \"BBoxes\"] = json.dumps({ \"bboxes\": bboxes })\n",
        "        shards_df = pd.concat([shards_df, shard_df])"
      ],
      "metadata": {
        "id": "PGDvS7qjkbAz"
      },
      "id": "PGDvS7qjkbAz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b85519e0-e5d7-4d04-9d14-d8a79df43984",
      "metadata": {
        "id": "b85519e0-e5d7-4d04-9d14-d8a79df43984"
      },
      "source": [
        "Finally, now that we have all of the VTT and image object detection metadata computed for the original images and/or shards, we can join the two dataframes together into one. As part of this process, we also want to reindex the resulting dataframe so that it uses the UIDs we calculated instead of the automatically generated indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4d21b1c-95ec-40e4-89d9-011a06684e33",
      "metadata": {
        "id": "d4d21b1c-95ec-40e4-89d9-011a06684e33"
      },
      "outputs": [],
      "source": [
        "complete_df = pd.concat([jpg_df, shards_df])\n",
        "complete_df.set_index(\"UID\", inplace=True)\n",
        "complete_df.fillna(\"\", inplace=True)\n",
        "complete_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fb38256-b45c-4e32-a032-5c8bee3b2e0d",
      "metadata": {
        "id": "0fb38256-b45c-4e32-a032-5c8bee3b2e0d"
      },
      "source": [
        "## Store the images in Google Cloud Storage\n",
        "\n",
        "Now that we've created the image shards, we can begin uploading the images to Google Cloud Storage. We'll need to have a Storage bucket already created for this next cell of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "077f5105-81e0-4b77-aa8d-3c8bec170a39",
      "metadata": {
        "id": "077f5105-81e0-4b77-aa8d-3c8bec170a39"
      },
      "outputs": [],
      "source": [
        "for i, row in complete_df.iterrows():\n",
        "    gcs_uri = store_image_gcs(project_id=PROJECT_ID, series=row,\n",
        "                                bucket_name=BUCKET, prefix=\"FantasyMapsTest\")\n",
        "    complete_df.at[i, \"GCS URI\"] = gcs_uri"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb433af9-3674-4483-91fc-930ff6e71903",
      "metadata": {
        "id": "bb433af9-3674-4483-91fc-930ff6e71903"
      },
      "source": [
        "## Store the metadata in Firestore\n",
        "\n",
        "Next we're going to store all of this metadata and URI in Firestore. The benefit of using Firestore is that the fields with JSON-formatted strings-`VTT` and `BBoxes` will automatically be translated into the correct document structure in Firestore after they've been upserted."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35825de1-8209-48ed-9f09-472f97da92d3",
      "metadata": {
        "id": "35825de1-8209-48ed-9f09-472f97da92d3"
      },
      "source": [
        "Very, very last step: iterate over all the training data and store the metadata in the Firestore collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7968e57-2a80-45cf-bd38-495c273be909",
      "metadata": {
        "id": "c7968e57-2a80-45cf-bd38-495c273be909"
      },
      "outputs": [],
      "source": [
        "for uid, row in complete_df.iterrows():\n",
        "    store_metadata_fs(project_id=PROJECT_ID, series=row,\n",
        "                      collection_name=COLLECTION_NAME, uid=uid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99950d94-57cc-47c7-85d4-093593f7c972",
      "metadata": {
        "id": "99950d94-57cc-47c7-85d4-093593f7c972"
      },
      "source": [
        "## Check the results of the metadata creation\n",
        "\n",
        "Now that (hopefully) all of the image metadata has been added to the Firestore collection, we can review the data to ensure that it is correct.\n",
        "\n",
        "To do this, we'll review the documents stored in the Firestore collection to verify that it has all the data we need--the VTT data, bounding boxes, and the GCS URI of the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c6764f-3f31-40bb-8361-3ce6bd17c9bb",
      "metadata": {
        "id": "23c6764f-3f31-40bb-8361-3ce6bd17c9bb"
      },
      "outputs": [],
      "source": [
        "client = firestore.Client(project=PROJECT_ID)\n",
        "collection = client.collection(COLLECTION_NAME)\n",
        "\n",
        "docs = collection.where(\"BBoxes\", \"!=\", \"\").select(field_paths=[\"gcsURI\", \"filename\", \"VTT\", \"Parent\"]).stream()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cee72d1-da57-4383-aab8-7b236302e556",
      "metadata": {
        "id": "4cee72d1-da57-4383-aab8-7b236302e556"
      },
      "source": [
        "With this Firestore query, we can verify the image metadata against the stored image in the Storage bucket. We'll first take the results of this query, compose it into a `pandas.DataFrame` object, and then print it out to the cell output. We can first take a look at the parent map (assuming that the map has been sharded) and then conclude whether the map and all its shards should be removed from the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e31b72c5-ad31-42df-8808-2f6239eb4618",
      "metadata": {
        "id": "e31b72c5-ad31-42df-8808-2f6239eb4618"
      },
      "outputs": [],
      "source": [
        "docs_list = ((d.to_dict(), d.id) for d in docs)\n",
        "docs_df = pd.DataFrame()\n",
        "for i, d in enumerate(docs_list):\n",
        "    d_dict = d[0]\n",
        "    vtt = d_dict[\"VTT\"]\n",
        "    d_dict[\"VTT\"] = json.dumps(vtt)\n",
        "    d_dict[\"UID\"] = d[1]\n",
        "    docs_df = pd.concat([docs_df, pd.DataFrame(data=d_dict, index=[0])], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df91b96-0a22-4a11-b3e0-80d05feff74f",
      "metadata": {
        "id": "8df91b96-0a22-4a11-b3e0-80d05feff74f"
      },
      "outputs": [],
      "source": [
        "docs_df.set_index(\"UID\", inplace=True)\n",
        "check_set_df = docs_df[[\"filename\", \"gcsURI\", \"Parent\"]]\n",
        "check_set_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93a96aef-5fdd-427f-8f0d-aeec42d99cb4",
      "metadata": {
        "id": "93a96aef-5fdd-427f-8f0d-aeec42d99cb4"
      },
      "source": [
        "Not everyone on Reddit follows the same conventions. Sometimes, there might be be a post where there are dimensions mentioned in the post (e.g. \"50x40\"), but the image doesn't actually have gridlines.\n",
        "\n",
        "We shouldn't allow these images into the training and test dataset for our model. Unfortunately, we have to review the images that we've collected on GCS and then verify that they do (or don't!) have gridlines visually.\n",
        "\n",
        "\n",
        "We'll start by printing out the entirety of our `DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81703224-47d9-4d86-95c7-928241e2461f",
      "metadata": {
        "id": "81703224-47d9-4d86-95c7-928241e2461f"
      },
      "outputs": [],
      "source": [
        "pd.set_option(\"display.max_rows\", 1000)\n",
        "check_set_df.sort_values(by=\"filename\", ascending=True, inplace=True)\n",
        "display(check_set_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90c45f1c-16f1-491c-975c-ccc660eacc08",
      "metadata": {
        "id": "90c45f1c-16f1-491c-975c-ccc660eacc08"
      },
      "source": [
        "This final step of data prepartion is to mark all of the unusable images in the Firestore collection. Luckily, we can use the Google Cloud Console to view the contents of our Storage bucket. We can even add new fields to the documents in our Firestore collection!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ba41c11-df6b-44de-a592-8ac1d7ece4a9",
      "metadata": {
        "id": "6ba41c11-df6b-44de-a592-8ac1d7ece4a9"
      },
      "source": [
        "![Storage user interface in the Cloud Console](https://github.com/telpirion/FantasyMaps/blob/main/notebooks/final/resources/StorageUI.png?raw=1)\n",
        "_Figure. The Google Cloud Storage user interface, showing images in a bucket._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db9d401f-1b87-486a-9def-96a8f36ed422",
      "metadata": {
        "id": "db9d401f-1b87-486a-9def-96a8f36ed422"
      },
      "source": [
        "![Firestore user interface in the Cloud Console](https://github.com/telpirion/FantasyMaps/blob/main/notebooks/final/resources/FirestoreUI.png?raw=1)\n",
        "_Figure. The Cloud Firestore user interface, showing a \"Usable\" field being added to a document._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "951d7636-b8fb-472b-9a05-043aa0a92c82",
      "metadata": {
        "id": "951d7636-b8fb-472b-9a05-043aa0a92c82"
      },
      "source": [
        "For this very last data preparation step, we will visually inspect all of the \"parent\" images in the Cloud Storage bucket. We will then create a list unusable images, where we store the image's UID. Finally, we will do a bulk update job to Firestore, setting a `Usable` field on the images to `False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc0f69b4-4f49-4b2b-8abc-ba14571f0ee9",
      "metadata": {
        "id": "fc0f69b4-4f49-4b2b-8abc-ba14571f0ee9"
      },
      "outputs": [],
      "source": [
        "unusable_parent_images = [\n",
        "    \"d3ee0039aeff5c33de778c5adbdd000f21c0b4cd\",\n",
        "    \"9a7e82433239b0087121f6fd31e133f5a94fa7dc\",\n",
        "    \"fc70f330cd5a3aaa3be94e0d603dab2876f9fca1\",\n",
        "    \"890c0d27318b0286aebf67c392fab286bdc4e7c5\",\n",
        "    \"2f4f496466d6e9fb8ad9791ccb81f7c13fd407db\",\n",
        "    \"4c0d3eb86ed599f496f7e30e18025021aebfe153\",\n",
        "    \"4bf88b8acc669331a65465e8c4b37fd8b9495e4b\",\n",
        "    \"b15fe8185c30b3e7800e42e280a3792998a0b55f\",\n",
        "    \"5cd7bbd5882fe8b9a270be1aa911c0ba858e818d\",\n",
        "    \"67925948371de53b58c09b32c97af60f72c58e0f\",\n",
        "    \"a816b6eb31b71cad5b50531d6e18ef46cb451cbd\",\n",
        "    \"f00be2ed7b19b8d0f44915a1886437193aa224aa\",\n",
        "    \"890c0d27318b0286aebf67c392fab286bdc4e7c5\",\n",
        "    # Others ....\n",
        "]\n",
        "\n",
        "usable_parent_images = [\n",
        "    \"7a903be0bad0fc00bfabbefd682b6eef23263b67\",\n",
        "    \"b1fe854b9c658cf9312319a447b594743513499b\",\n",
        "    \"3a52781685d7aed530a685739b58341fafd2e721\",\n",
        "    \"3d17d612a843af7a5ad1a2b2d5dcce29e67d367f\",\n",
        "    \"aa33a7ed5c3c87147fd25dafe6c0a1d3eb29dbe5\",\n",
        "    \"2ce9f80408137e36531581fb22ee3fe892f41f76\",\n",
        "    \"e086c1cf420e27448bc1a45147b5c43df4b3d8d0\",\n",
        "    \"5bc994d7d38c1fc4654c58666d674200f731986b\",\n",
        "    \"820c3bbfe4d14694ecb729ce3f45b4bda031f61a\",\n",
        "    \"2d323018c74db7e0432ff368283ea429f13bd36d\",\n",
        "    \"343833ab1d8cd17dae6b702830864500d1e66e19\",\n",
        "    \"c60b952d0c20a63dc263220ec5b49a54fd20d175\",\n",
        "    \"8ac01d3a84bc548a8b243d08c6e031206f293908\",\n",
        "    \"f00be2ed7b19b8d0f44915a1886437193aa224aa\",\n",
        "    \"950390c88b7bd9d6f886e5f01bae9460c0aa407b\",\n",
        "    \"3322c1b4795e4a9e4477feafd55685d647b4e29c\",\n",
        "    \"34362be27ada680a58e42d26758bf08c01d3460a\",\n",
        "    \"60d815ba2c1458c3a4039595a5ed723a7501a36e\",\n",
        "    \"928b339ba222a3933fce4523f8033fa3eb7ed62f\",\n",
        "    \"4931f0033f0ab217fd0fe2a2024d22a119782e2c\",\n",
        "    \"83846604933daff160cefc28c2a828bd93a84e1a\",\n",
        "    \"d2fe7281d8b1e043009033c957ca347847343e14\",\n",
        "    \"c1552a0046ef4ece5d146544043edb2deb97f7a1\",\n",
        "    # And more ...\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8587b9de-f507-436c-9314-75c1be0896bb",
      "metadata": {
        "id": "8587b9de-f507-436c-9314-75c1be0896bb"
      },
      "source": [
        "**Note**: If you're thinking that this manual process should be automatable--you're right! In another notebook, we will use a pre-trained version of our gridline-detecting model to accept or reject images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "092d79e7-1477-4e0c-a59f-63187ba48221",
      "metadata": {
        "id": "092d79e7-1477-4e0c-a59f-63187ba48221"
      },
      "outputs": [],
      "source": [
        "usable_set = set(usable_parent_images)\n",
        "unusable_set = set(unusable_parent_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d8034db-4dfe-48f6-a80d-22b7814a7f91",
      "metadata": {
        "id": "3d8034db-4dfe-48f6-a80d-22b7814a7f91"
      },
      "outputs": [],
      "source": [
        "firestore_client = firestore.Client(project=PROJECT_ID)\n",
        "bulkwriter = firestore_client.bulk_writer()\n",
        "collection = firestore_client.collection(COLLECTION_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae1e7a5b-df84-4d52-9517-90c959fcf72c",
      "metadata": {
        "id": "ae1e7a5b-df84-4d52-9517-90c959fcf72c"
      },
      "outputs": [],
      "source": [
        "# Iterate over all of the metadata entries & images that we want to delete\n",
        "unusable_shards = collection.where(\"Parent\", \"in\", list(unusable_set)[:10]).stream()\n",
        "for doc in unusable_shards:\n",
        "    bulkwriter.delete(doc.reference)\n",
        "\n",
        "bulkwriter.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63f3731c-2f6d-4c54-93d5-704fdd834d72",
      "metadata": {
        "id": "63f3731c-2f6d-4c54-93d5-704fdd834d72"
      },
      "outputs": [],
      "source": [
        "# Iterate over all of the good entries\n",
        "subset_start_index = 0\n",
        "while subset_start_index < len(usable_set):\n",
        "    subset = list(usable_set)[subset_start_index:subset_start_index + 10]\n",
        "    usable_shards = collection.where(\"Parent\", \"in\", subset).stream()\n",
        "\n",
        "    for doc in usable_shards:\n",
        "        bulkwriter.update(doc.reference, { \"Usable\": True})\n",
        "\n",
        "    subset_start_index = subset_start_index + 10\n",
        "\n",
        "bulkwriter.flush()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b756e48-8a6b-4378-8665-502d10269d6c",
      "metadata": {
        "id": "1b756e48-8a6b-4378-8665-502d10269d6c"
      },
      "outputs": [],
      "source": [
        "bulkwriter.close()"
      ]
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "python3",
      "name": "common-cu110.m87",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cu110:m87"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "colab": {
      "provenance": [],
      "name": "1_firestore.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}